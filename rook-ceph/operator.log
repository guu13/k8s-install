2023-02-16 08:53:36.450985 I | rookcmd: starting Rook v1.9.9 with arguments '/usr/local/bin/rook ceph operator'
2023-02-16 08:53:36.451071 I | rookcmd: flag values: --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-level=INFO, --operator-image=, --service-account=
2023-02-16 08:53:36.451075 I | cephcmd: starting Rook-Ceph operator
2023-02-16 08:53:36.584087 I | cephcmd: base ceph version inside the rook operator image is "ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)"
2023-02-16 08:53:36.591339 I | op-k8sutil: ROOK_CURRENT_NAMESPACE_ONLY="true" (env var)
2023-02-16 08:53:36.591366 I | operator: watching the current namespace "rook-ceph" for a Ceph CRs
2023-02-16 08:53:36.591439 I | operator: setting up schemes
2023-02-16 08:53:36.592957 I | operator: setting up the controller-runtime manager
2023-02-16 08:53:37.346713 I | operator: Fetching webhook cert-manager-webhook to see if cert-manager is installed.
2023-02-16 08:53:37.350367 I | operator: failed to get cert manager
2023-02-16 08:53:37.350413 I | ceph-cluster-controller: successfully started
2023-02-16 08:53:37.350550 I | ceph-cluster-controller: enabling hotplug orchestration
2023-02-16 08:53:37.350577 I | ceph-crashcollector-controller: successfully started
2023-02-16 08:53:37.350592 I | ceph-block-pool-controller: successfully started
2023-02-16 08:53:37.350605 I | ceph-object-store-user-controller: successfully started
2023-02-16 08:53:37.350631 I | ceph-object-realm-controller: successfully started
2023-02-16 08:53:37.350640 I | ceph-object-zonegroup-controller: successfully started
2023-02-16 08:53:37.350647 I | ceph-object-zone-controller: successfully started
2023-02-16 08:53:37.350757 I | ceph-object-controller: successfully started
2023-02-16 08:53:37.350799 I | ceph-file-controller: successfully started
2023-02-16 08:53:37.350827 I | ceph-nfs-controller: successfully started
2023-02-16 08:53:37.350849 I | ceph-rbd-mirror-controller: successfully started
2023-02-16 08:53:37.350879 I | ceph-client-controller: successfully started
2023-02-16 08:53:37.350899 I | ceph-filesystem-mirror-controller: successfully started
2023-02-16 08:53:37.350952 I | operator: rook-ceph-operator-config-controller successfully started
2023-02-16 08:53:37.350983 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2023-02-16 08:53:37.351001 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2023-02-16 08:53:37.351016 I | ceph-bucket-topic: successfully started
2023-02-16 08:53:37.351051 I | ceph-bucket-notification: successfully started
2023-02-16 08:53:37.351064 I | ceph-bucket-notification: successfully started
2023-02-16 08:53:37.351071 I | ceph-fs-subvolumegroup-controller: successfully started
2023-02-16 08:53:37.351082 I | blockpool-rados-namespace-controller: successfully started
2023-02-16 08:53:37.352257 I | operator: starting the controller-runtime manager
2023-02-16 08:53:37.454149 I | op-k8sutil: ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS="15" (configmap)
2023-02-16 08:53:37.454168 I | op-k8sutil: ROOK_LOG_LEVEL="DEBUG" (configmap)
2023-02-16 08:53:37.454174 I | op-k8sutil: ROOK_ENABLE_DISCOVERY_DAEMON="false" (configmap)
2023-02-16 08:53:37.454180 D | ceph-cluster-controller: "ceph-cluster-controller": no CephCluster resource found in namespace ""
2023-02-16 08:53:37.454191 D | ceph-cluster-controller: node watcher: useAllNodes is set to false and no nodes storageClassDevicesets or volumeSources are specified in cluster "", skipping
2023-02-16 08:53:37.454216 D | ceph-cluster-controller: "ceph-cluster-controller": no CephCluster resource found in namespace ""
2023-02-16 08:53:37.454224 D | ceph-cluster-controller: node watcher: useAllNodes is set to false and no nodes storageClassDevicesets or volumeSources are specified in cluster "", skipping
2023-02-16 08:53:37.457508 D | ceph-csi: no ceph cluster found not deploying ceph csi driver
2023-02-16 08:53:37.457543 I | ceph-csi: CSI Ceph RBD driver disabled
2023-02-16 08:53:37.457550 I | op-k8sutil: removing daemonset csi-rbdplugin if it exists
2023-02-16 08:53:37.459092 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-16 08:53:37.459215 I | operator: rook-ceph-operator-config-controller done reconciling
2023-02-16 08:53:37.459260 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-16 08:53:37.459308 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-16 08:53:37.460855 D | op-k8sutil: removing csi-rbdplugin-provisioner deployment if it exists
2023-02-16 08:53:37.460880 I | op-k8sutil: removing deployment csi-rbdplugin-provisioner if it exists
2023-02-16 08:53:37.471177 D | ceph-csi: rook-ceph.rbd.csi.ceph.com CSIDriver not found; skipping deletion.
2023-02-16 08:53:37.471207 I | ceph-csi: successfully removed CSI Ceph RBD driver
2023-02-16 08:53:37.471213 I | ceph-csi: CSI CephFS driver disabled
2023-02-16 08:53:37.471219 I | op-k8sutil: removing daemonset csi-cephfsplugin if it exists
2023-02-16 08:53:37.474855 D | op-k8sutil: removing csi-cephfsplugin-provisioner deployment if it exists
2023-02-16 08:53:37.474884 I | op-k8sutil: removing deployment csi-cephfsplugin-provisioner if it exists
2023-02-16 08:53:37.483468 D | ceph-csi: rook-ceph.cephfs.csi.ceph.com CSIDriver not found; skipping deletion.
2023-02-16 08:53:37.483493 I | ceph-csi: successfully removed CSI CephFS driver
2023-02-16 08:53:37.483499 I | ceph-csi: CSI NFS driver disabled
2023-02-16 08:53:37.483504 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2023-02-16 08:53:37.487344 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2023-02-16 08:53:37.487373 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2023-02-16 08:53:37.497236 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2023-02-16 08:53:37.497262 I | ceph-csi: successfully removed CSI NFS driver
2023-02-16 08:54:36.592191 D | operator: number of goroutines 405
2023-02-16 08:55:57.101004 D | clusterdisruption-controller: create event from ceph cluster CR
2023-02-16 08:55:57.101056 D | clusterdisruption-controller: reconciling "rook-ceph/rook-ceph"
2023-02-16 08:55:57.101117 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:55:57.101153 D | ceph-cluster-controller: create event from a CR
2023-02-16 08:55:57.101246 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:55:57.101440 I | ceph-spec: adding finalizer "cephcluster.ceph.rook.io" on "rook-ceph"
2023-02-16 08:55:57.101656 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:55:57.112989 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (default)
2023-02-16 08:55:57.113025 D | ceph-csi: not a multus cluster "rook-ceph/rook-ceph" or CSI_ENABLE_HOST_NETWORK is true, not deploying the ceph-csi plugin holder
2023-02-16 08:55:57.122381 I | clusterdisruption-controller: deleted all legacy node drain canary pods
2023-02-16 08:55:57.129747 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2023-02-16 08:55:57.129903 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:55:57.129952 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:55:57.130210 D | ceph-cluster-controller: skipping resource "rook-ceph" update with unchanged spec
2023-02-16 08:55:57.133077 I | ceph-cluster-controller: clusterInfo not yet found, must be a new cluster.
2023-02-16 08:55:57.138242 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
2023-02-16 08:55:57.144569 D | ceph-cluster-controller: cluster spec successfully validated
2023-02-16 08:55:57.144630 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Detecting Ceph version"
2023-02-16 08:55:57.154345 I | op-k8sutil: ROOK_CSI_ENABLE_RBD="true" (configmap)
2023-02-16 08:55:57.154379 I | op-k8sutil: ROOK_CSI_ENABLE_CEPHFS="true" (configmap)
2023-02-16 08:55:57.154384 I | op-k8sutil: ROOK_CSI_ENABLE_NFS="false" (configmap)
2023-02-16 08:55:57.154387 I | op-k8sutil: ROOK_CSI_ALLOW_UNSUPPORTED_VERSION="false" (configmap)
2023-02-16 08:55:57.154391 I | op-k8sutil: ROOK_CSI_ENABLE_GRPC_METRICS="false" (configmap)
2023-02-16 08:55:57.154394 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (default)
2023-02-16 08:55:57.154397 I | op-k8sutil: CSI_FORCE_CEPHFS_KERNEL_CLIENT="true" (configmap)
2023-02-16 08:55:57.154399 I | op-k8sutil: CSI_GRPC_TIMEOUT_SECONDS="150" (configmap)
2023-02-16 08:55:57.154402 I | op-k8sutil: CSI_CEPHFS_GRPC_METRICS_PORT="9091" (default)
2023-02-16 08:55:57.154404 I | op-k8sutil: CSI_CEPHFS_GRPC_METRICS_PORT="9091" (default)
2023-02-16 08:55:57.154407 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2023-02-16 08:55:57.154411 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2023-02-16 08:55:57.154413 I | op-k8sutil: CSI_RBD_GRPC_METRICS_PORT="9090" (default)
2023-02-16 08:55:57.154415 I | op-k8sutil: CSI_RBD_GRPC_METRICS_PORT="9090" (default)
2023-02-16 08:55:57.154418 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2023-02-16 08:55:57.154420 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2023-02-16 08:55:57.154422 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2023-02-16 08:55:57.154424 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2023-02-16 08:55:57.154427 I | op-k8sutil: CSI_ENABLE_LIVENESS="false" (configmap)
2023-02-16 08:55:57.154429 I | op-k8sutil: CSI_PLUGIN_PRIORITY_CLASSNAME="system-node-critical" (configmap)
2023-02-16 08:55:57.154431 I | op-k8sutil: CSI_PROVISIONER_PRIORITY_CLASSNAME="system-cluster-critical" (configmap)
2023-02-16 08:55:57.154433 I | op-k8sutil: CSI_ENABLE_OMAP_GENERATOR="false" (default)
2023-02-16 08:55:57.154437 I | op-k8sutil: CSI_ENABLE_RBD_SNAPSHOTTER="true" (configmap)
2023-02-16 08:55:57.154439 I | op-k8sutil: CSI_ENABLE_CEPHFS_SNAPSHOTTER="true" (configmap)
2023-02-16 08:55:57.154441 I | op-k8sutil: CSI_ENABLE_VOLUME_REPLICATION="false" (configmap)
2023-02-16 08:55:57.154444 I | op-k8sutil: CSI_ENABLE_CSIADDONS="false" (configmap)
2023-02-16 08:55:57.154446 I | op-k8sutil: CSI_ENABLE_ENCRYPTION="false" (configmap)
2023-02-16 08:55:57.154448 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2023-02-16 08:55:57.154450 I | op-k8sutil: CSI_NFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2023-02-16 08:55:57.154452 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2023-02-16 08:55:57.154455 I | op-k8sutil: CSI_PLUGIN_ENABLE_SELINUX_HOST_MOUNT="false" (configmap)
2023-02-16 08:55:57.154457 I | ceph-csi: Kubernetes version is 1.20+
2023-02-16 08:55:57.154460 I | op-k8sutil: ROOK_CSI_RESIZER_IMAGE="registry.k8s.io/sig-storage/csi-resizer:v1.4.0" (default)
2023-02-16 08:55:57.154462 I | op-k8sutil: CSI_LOG_LEVEL="" (default)
2023-02-16 08:55:57.154464 I | op-k8sutil: CSI_SIDECAR_LOG_LEVEL="" (default)
2023-02-16 08:55:57.155631 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:55:57.155659 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:55:57.155692 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v16.2.10...
2023-02-16 08:55:57.160026 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2023-02-16 08:55:57.160966 I | op-k8sutil: CSI_PROVISIONER_REPLICAS="2" (configmap)
2023-02-16 08:55:57.160979 I | op-k8sutil: ROOK_CSI_CEPH_IMAGE="quay.io/cephcsi/cephcsi:v3.6.2" (default)
2023-02-16 08:55:57.160983 I | op-k8sutil: ROOK_CSI_NFS_IMAGE="registry.k8s.io/sig-storage/nfsplugin:v4.0.0" (default)
2023-02-16 08:55:57.160987 I | op-k8sutil: ROOK_CSI_REGISTRAR_IMAGE="registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.5.1" (default)
2023-02-16 08:55:57.160990 I | op-k8sutil: ROOK_CSI_PROVISIONER_IMAGE="registry.k8s.io/sig-storage/csi-provisioner:v3.1.0" (default)
2023-02-16 08:55:57.160992 I | op-k8sutil: ROOK_CSI_ATTACHER_IMAGE="registry.k8s.io/sig-storage/csi-attacher:v3.4.0" (default)
2023-02-16 08:55:57.160995 I | op-k8sutil: ROOK_CSI_SNAPSHOTTER_IMAGE="registry.k8s.io/sig-storage/csi-snapshotter:v6.0.1" (default)
2023-02-16 08:55:57.160999 I | op-k8sutil: ROOK_CSI_KUBELET_DIR_PATH="/var/lib/kubelet" (default)
2023-02-16 08:55:57.161002 I | op-k8sutil: CSI_VOLUME_REPLICATION_IMAGE="quay.io/csiaddons/volumereplication-operator:v0.3.0" (default)
2023-02-16 08:55:57.161005 I | op-k8sutil: ROOK_CSIADDONS_IMAGE="quay.io/csiaddons/k8s-sidecar:v0.4.0" (default)
2023-02-16 08:55:57.161007 I | op-k8sutil: ROOK_CSI_CEPHFS_POD_LABELS="" (default)
2023-02-16 08:55:57.161009 I | op-k8sutil: ROOK_CSI_NFS_POD_LABELS="" (default)
2023-02-16 08:55:57.161011 I | op-k8sutil: ROOK_CSI_RBD_POD_LABELS="" (default)
2023-02-16 08:55:57.161014 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.6.2"
2023-02-16 08:55:57.161050 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2023-02-16 08:55:57.161054 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-16 08:55:57.164120 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2023-02-16 08:55:58.372132 D | CmdReporter: job rook-ceph-detect-version has returned results
2023-02-16 08:55:58.396519 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-detect-version" cm is handled by another watcher
2023-02-16 08:55:58.397545 I | ceph-spec: detected ceph image version: "16.2.10-0 pacific"
2023-02-16 08:55:58.397573 I | ceph-cluster-controller: validating ceph version from provided image
2023-02-16 08:55:58.402408 D | ceph-cluster-controller: cluster not initialized, nothing to validate. clusterInfo is nil
2023-02-16 08:55:58.402440 I | ceph-cluster-controller: cluster "rook-ceph": version "16.2.10-0 pacific" detected for image "quay.io/ceph/ceph:v16.2.10"
2023-02-16 08:55:58.418275 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring the Ceph cluster"
2023-02-16 08:55:58.418347 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:55:58.418360 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:55:58.457185 E | ceph-spec: failed to update cluster condition to {Type:Progressing Status:True Reason:ClusterProgressing Message:Configuring the Ceph cluster LastHeartbeatTime:2023-02-16 08:55:58.418267079 +0000 UTC m=+141.988633503 LastTransitionTime:2023-02-16 08:55:58.418267013 +0000 UTC m=+141.988633449}. failed to update object "rook-ceph/rook-ceph" status: Operation cannot be fulfilled on cephclusters.ceph.rook.io "rook-ceph": the object has been modified; please apply your changes to the latest version and try again
2023-02-16 08:55:58.478764 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2023-02-16 08:55:58.478824 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mons"
2023-02-16 08:55:58.489152 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:55:58.489175 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:55:58.492120 D | op-mon: Acquiring lock for mon orchestration
2023-02-16 08:55:58.492143 D | op-mon: Acquired lock for mon orchestration
2023-02-16 08:55:58.492149 I | op-mon: start running mons
2023-02-16 08:55:58.492156 D | op-mon: establishing ceph cluster info
2023-02-16 08:55:58.506859 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/mon.keyring --gen-key -n mon. --cap mon 'allow *'
2023-02-16 08:55:58.525418 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mgr 'allow *' --cap mds 'allow'
2023-02-16 08:55:58.551506 I | ceph-spec: creating mon secrets for a new cluster
2023-02-16 08:55:58.953320 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2023-02-16 08:55:59.157340 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-16 08:55:59.157375 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-16 08:55:59.157383 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-16 08:55:59.157390 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-16 08:55:59.709536 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-csi-detect-version" cm is handled by another watcher
2023-02-16 08:55:59.709808 I | ceph-csi: Detected ceph CSI image version: "v3.6.2"
2023-02-16 08:55:59.713517 D | ceph-csi: "rook-ceph.rbd.csi.ceph.com" CSIDriver not found; skipping deletion.
2023-02-16 08:55:59.716682 D | ceph-csi: "rook-ceph.cephfs.csi.ceph.com" CSIDriver not found; skipping deletion.
2023-02-16 08:55:59.723339 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2023-02-16 08:55:59.723362 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-16 08:55:59.723368 I | op-k8sutil: CSI_PLUGIN_TOLERATIONS="" (default)
2023-02-16 08:55:59.723371 I | op-k8sutil: CSI_PLUGIN_NODE_AFFINITY="" (default)
2023-02-16 08:55:59.723375 I | op-k8sutil: CSI_RBD_PLUGIN_TOLERATIONS="" (default)
2023-02-16 08:55:59.723378 I | op-k8sutil: CSI_RBD_PLUGIN_NODE_AFFINITY="" (default)
2023-02-16 08:55:59.723382 I | op-k8sutil: CSI_RBD_PLUGIN_RESOURCE="" (default)
2023-02-16 08:55:59.726015 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-16 08:55:59.726075 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-16 08:55:59.726080 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-16 08:55:59.726084 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-16 08:55:59.735810 I | op-k8sutil: CSI_RBD_PROVISIONER_TOLERATIONS="" (default)
2023-02-16 08:55:59.735839 I | op-k8sutil: CSI_RBD_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-16 08:55:59.735845 I | op-k8sutil: CSI_RBD_PROVISIONER_RESOURCE="" (default)
2023-02-16 08:55:59.749679 I | ceph-csi: successfully started CSI Ceph RBD driver
2023-02-16 08:55:59.749709 I | op-k8sutil: CSI_CEPHFS_PLUGIN_TOLERATIONS="" (default)
2023-02-16 08:55:59.749714 I | op-k8sutil: CSI_CEPHFS_PLUGIN_NODE_AFFINITY="" (default)
2023-02-16 08:55:59.749717 I | op-k8sutil: CSI_CEPHFS_PLUGIN_RESOURCE="" (default)
2023-02-16 08:55:59.764798 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_TOLERATIONS="" (default)
2023-02-16 08:55:59.764836 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-16 08:55:59.764843 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_RESOURCE="" (default)
2023-02-16 08:55:59.838235 I | ceph-csi: successfully started CSI CephFS driver
2023-02-16 08:55:59.838268 I | op-k8sutil: CSI_RBD_FSGROUPPOLICY="ReadWriteOnceWithFSType" (configmap)
2023-02-16 08:55:59.887355 I | ceph-csi: CSIDriver object created for driver "rook-ceph.rbd.csi.ceph.com"
2023-02-16 08:55:59.887561 I | op-k8sutil: CSI_CEPHFS_FSGROUPPOLICY="ReadWriteOnceWithFSType" (configmap)
2023-02-16 08:55:59.911050 I | op-mon: existing maxMonID not found or failed to load. configmaps "rook-ceph-mon-endpoints" not found
2023-02-16 08:55:59.948567 I | ceph-csi: CSIDriver object created for driver "rook-ceph.cephfs.csi.ceph.com"
2023-02-16 08:55:59.948589 I | ceph-csi: CSI NFS driver disabled
2023-02-16 08:55:59.948596 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2023-02-16 08:55:59.973278 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2023-02-16 08:55:59.973296 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2023-02-16 08:56:00.110189 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":[],"namespace":""}] data: mapping:{"node":{}} maxMonId:-1]
2023-02-16 08:56:00.310116 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2023-02-16 08:56:00.310133 I | ceph-csi: successfully removed CSI NFS driver
2023-02-16 08:56:00.510405 D | op-config: creating config secret "rook-ceph-config"
2023-02-16 08:56:00.708727 D | op-config: updating config secret "rook-ceph-config"
2023-02-16 08:56:01.106319 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2023-02-16 08:56:01.106402 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2023-02-16 08:56:01.106410 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2023-02-16 08:56:01.519610 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-csi-config" cm is handled by another watcher
2023-02-16 08:56:01.707043 D | op-cfg-keyring: creating secret for rook-ceph-mons-keyring
2023-02-16 08:56:02.106544 D | op-cfg-keyring: creating secret for rook-ceph-admin-keyring
2023-02-16 08:56:02.706455 I | op-mon: targeting the mon count 1
2023-02-16 08:56:02.711252 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP: Port:6789 Zone: DataPathMap:0xc0010e8cc0}
2023-02-16 08:56:02.719739 I | op-mon: created canary deployment rook-ceph-mon-a-canary
2023-02-16 08:56:02.734040 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-16 08:56:02.734097 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:02.754121 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-16 08:56:02.754155 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:02.785704 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-16 08:56:02.785733 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:03.107335 I | op-mon: canary monitor deployment rook-ceph-mon-a-canary scheduled to node2
2023-02-16 08:56:03.107367 I | op-mon: mon a assigned to node node2
2023-02-16 08:56:03.107374 D | op-mon: using internal IP 10.211.55.98 for node node2
2023-02-16 08:56:03.107396 D | op-mon: mons have been scheduled
2023-02-16 08:56:03.112530 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-a-canary"
2023-02-16 08:56:03.121041 I | op-mon: creating mon a
2023-02-16 08:56:03.121062 I | op-mon: setting mon endpoints for hostnetwork mode
2023-02-16 08:56:03.121688 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-16 08:56:03.121712 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:03.130414 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-16 08:56:03.130453 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:03.176497 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-16 08:56:03.176535 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:03.510170 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2023-02-16 08:56:03.707379 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-mon-endpoints" cm is handled by another watcher
2023-02-16 08:56:03.707457 D | op-mon: mons were added or removed from the endpoints cm
2023-02-16 08:56:03.707462 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2023-02-16 08:56:03.707475 D | op-mon: mons were added or removed from the endpoints cm
2023-02-16 08:56:03.707487 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2023-02-16 08:56:03.707494 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2023-02-16 08:56:03.707514 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-16 08:56:03.707518 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.211.55.98:6789"],"namespace":""}] data:a=10.211.55.98:6789 mapping:{"node":{"a":{"Name":"node2","Hostname":"node2","Address":"10.211.55.98"}}} maxMonId:-1]
2023-02-16 08:56:03.905803 D | op-config: updating config secret "rook-ceph-config"
2023-02-16 08:56:04.108008 D | ceph-spec: object "rook-ceph-config" matched on update
2023-02-16 08:56:04.108036 D | ceph-spec: do not reconcile on "rook-ceph-config" secret changes
2023-02-16 08:56:04.306072 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2023-02-16 08:56:04.306275 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2023-02-16 08:56:04.306301 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2023-02-16 08:56:04.708066 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-csi-config" cm is handled by another watcher
2023-02-16 08:56:04.712837 I | op-mon: 0 of 1 expected mons are ready. creating or updating deployments without checking quorum in attempt to achieve a healthy mon cluster
2023-02-16 08:56:04.712901 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.211.55.98 Port:6789 Zone: DataPathMap:0xc0010e8cc0}
2023-02-16 08:56:04.716871 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2023-02-16 08:56:04.716905 D | op-mon: Starting mon: rook-ceph-mon-a
2023-02-16 08:56:04.743877 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2023-02-16 08:56:04.743897 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:04.771086 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2023-02-16 08:56:04.771105 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:04.795561 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2023-02-16 08:56:04.795599 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:04.905666 I | op-mon: updating maxMonID from -1 to 0 after committing mon "a"
2023-02-16 08:56:05.109364 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-mon-endpoints" cm is handled by another watcher
2023-02-16 08:56:05.109424 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2023-02-16 08:56:05.109437 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-16 08:56:05.509506 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2023-02-16 08:56:05.706477 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.211.55.98:6789"],"namespace":""}] data:a=10.211.55.98:6789 mapping:{"node":{"a":{"Name":"node2","Hostname":"node2","Address":"10.211.55.98"}}} maxMonId:0]
2023-02-16 08:56:05.706495 I | op-mon: waiting for mon quorum with [a]
2023-02-16 08:56:05.908444 I | op-mon: mon a is not yet running
2023-02-16 08:56:05.908473 I | op-mon: mons running: []
2023-02-16 08:56:05.908487 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:07.128898 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2023-02-16 08:56:07.133005 I | ceph-spec: parsing mon endpoints: a=10.211.55.98:6789
2023-02-16 08:56:07.133076 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc000800da0], assignment=&{Schedule:map[a:0xc0000f5800]}
2023-02-16 08:56:07.133102 I | op-k8sutil: ROOK_OBC_WATCH_OPERATOR_NAMESPACE="true" (configmap)
2023-02-16 08:56:07.133109 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "rook-ceph.ceph.rook.io/bucket"
2023-02-16 08:56:07.133370 I | op-bucket-prov: successfully reconciled bucket provisioner
I0216 08:56:07.133431       1 manager.go:135] objectbucket.io/provisioner-manager "msg"="starting provisioner"  "name"="rook-ceph.ceph.rook.io/bucket"
2023-02-16 08:56:14.251727 D | ceph-crashcollector-controller: "rook-ceph-mon-a-6d9bf6d78b-kx289" is a ceph pod!
2023-02-16 08:56:14.251872 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-16 08:56:14.252703 D | ceph-crashcollector-controller: secret "rook-ceph-crash-collector-keyring" not found. retrying in "30s". Secret "rook-ceph-crash-collector-keyring" not found
2023-02-16 08:56:15.017505 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-16 08:56:15.017535 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:15.033805 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2023-02-16 08:56:15.033832 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2023-02-16 08:56:15.033841 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2023-02-16 08:56:15.033850 D | ceph-spec: do not reconcile "rook-ceph-mon-a-canary" on monitor canary deployments
2023-02-16 08:56:15.033870 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2023-02-16 08:56:15.033877 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2023-02-16 08:56:18.118764 I | op-mon: Monitors in quorum: [a]
2023-02-16 08:56:18.118794 I | op-mon: mons created: 1
2023-02-16 08:56:18.118809 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:18.318362 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1}}
2023-02-16 08:56:18.318394 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1}}
2023-02-16 08:56:18.318453 I | op-mon: waiting for mon quorum with [a]
2023-02-16 08:56:18.336111 I | op-mon: mons running: [a]
2023-02-16 08:56:18.336148 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:18.535518 I | op-mon: Monitors in quorum: [a]
2023-02-16 08:56:18.535553 I | op-config: setting "global"="mon allow pool delete"="true" option to the mon configuration database
2023-02-16 08:56:18.535567 D | exec: Running command: ceph config set global mon_allow_pool_delete true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:18.739205 I | op-config: successfully set "global"="mon allow pool delete"="true" option to the mon configuration database
2023-02-16 08:56:18.739234 I | op-config: setting "global"="mon cluster log file"="" option to the mon configuration database
2023-02-16 08:56:18.739250 D | exec: Running command: ceph config set global mon_cluster_log_file  --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:18.938456 I | op-config: successfully set "global"="mon cluster log file"="" option to the mon configuration database
2023-02-16 08:56:18.938512 I | op-config: setting "global"="mon allow pool size one"="true" option to the mon configuration database
2023-02-16 08:56:18.938528 D | exec: Running command: ceph config set global mon_allow_pool_size_one true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:19.138297 I | op-config: successfully set "global"="mon allow pool size one"="true" option to the mon configuration database
2023-02-16 08:56:19.138329 I | op-config: setting "global"="osd scrub auto repair"="true" option to the mon configuration database
2023-02-16 08:56:19.138344 D | exec: Running command: ceph config set global osd_scrub_auto_repair true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:19.339050 I | op-config: successfully set "global"="osd scrub auto repair"="true" option to the mon configuration database
2023-02-16 08:56:19.339082 I | op-config: setting "global"="log to file"="false" option to the mon configuration database
2023-02-16 08:56:19.339099 D | exec: Running command: ceph config set global log_to_file false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:19.537222 I | op-config: successfully set "global"="log to file"="false" option to the mon configuration database
2023-02-16 08:56:19.537251 I | op-config: deleting "log file" option from the mon configuration database
2023-02-16 08:56:19.537265 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:19.735313 I | op-config: successfully deleted "log file" option from the mon configuration database
2023-02-16 08:56:19.735332 D | op-mon: mon endpoints used are: a=10.211.55.98:6789
2023-02-16 08:56:19.735335 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2023-02-16 08:56:19.735338 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2023-02-16 08:56:19.735341 D | op-mon: Released lock for mon orchestration
2023-02-16 08:56:19.735345 D | ceph-cluster-controller: monitors are up and running, executing post actions
2023-02-16 08:56:19.735351 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2023-02-16 08:56:19.735364 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:19.958574 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2023-02-16 08:56:19.958614 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:20.170776 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2023-02-16 08:56:20.170813 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r mgr allow rw osd allow rw tag cephfs metadata=* --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:20.388014 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2023-02-16 08:56:20.388049 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:20.604903 D | op-cfg-keyring: creating secret for rook-csi-cephfs-provisioner
2023-02-16 08:56:20.614290 D | op-cfg-keyring: creating secret for rook-csi-cephfs-node
2023-02-16 08:56:20.622311 D | op-cfg-keyring: creating secret for rook-csi-rbd-provisioner
2023-02-16 08:56:20.631697 D | op-cfg-keyring: creating secret for rook-csi-rbd-node
2023-02-16 08:56:20.637012 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2023-02-16 08:56:20.637042 I | cephclient: getting or creating ceph auth key "client.crash"
2023-02-16 08:56:20.637053 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:20.851170 D | op-cfg-keyring: creating secret for rook-ceph-crash-collector-keyring
2023-02-16 08:56:20.855763 I | ceph-crashcollector-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2023-02-16 08:56:20.855791 I | op-config: deleting "mon_mds_skip_sanity" option from the mon configuration database
2023-02-16 08:56:20.855802 D | exec: Running command: ceph config rm mon mon_mds_skip_sanity --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:21.054589 I | op-config: successfully deleted "mon_mds_skip_sanity" option from the mon configuration database
2023-02-16 08:56:21.054606 I | ceph-cluster-controller: setting msgr2 encryption mode to "crc secure"
2023-02-16 08:56:21.054610 I | op-config: setting "global"="ms_cluster_mode"="crc secure" option to the mon configuration database
2023-02-16 08:56:21.054625 D | exec: Running command: ceph config set global ms_cluster_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:21.259017 I | op-config: successfully set "global"="ms_cluster_mode"="crc secure" option to the mon configuration database
2023-02-16 08:56:21.259057 I | op-config: setting "global"="ms_service_mode"="crc secure" option to the mon configuration database
2023-02-16 08:56:21.259072 D | exec: Running command: ceph config set global ms_service_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:21.457756 I | op-config: successfully set "global"="ms_service_mode"="crc secure" option to the mon configuration database
2023-02-16 08:56:21.457785 I | op-config: setting "global"="ms_client_mode"="crc secure" option to the mon configuration database
2023-02-16 08:56:21.457799 D | exec: Running command: ceph config set global ms_client_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:21.656767 I | op-config: successfully set "global"="ms_client_mode"="crc secure" option to the mon configuration database
2023-02-16 08:56:21.656840 W | ceph-cluster-controller: network compression requires Ceph Quincy (v17) or newer, skipping for current ceph "16.2.10-0 pacific"
2023-02-16 08:56:21.656848 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2023-02-16 08:56:21.656852 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2023-02-16 08:56:21.656866 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:21.855993 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2023-02-16 08:56:21.856067 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-rook-ceph" in namespace "rook-ceph"
2023-02-16 08:56:21.856073 D | op-k8sutil: creating secret cluster-peer-token-rook-ceph
2023-02-16 08:56:21.863070 D | op-k8sutil: created secret cluster-peer-token-rook-ceph
2023-02-16 08:56:21.863142 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mgr(s)"
2023-02-16 08:56:21.872992 I | op-mgr: start running mgr
2023-02-16 08:56:21.873016 I | cephclient: getting or creating ceph auth key "mgr.a"
2023-02-16 08:56:21.873033 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:21.873066 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:56:21.873081 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:56:22.081618 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2023-02-16 08:56:22.085168 D | op-cfg-keyring: creating secret for rook-ceph-mgr-a-keyring
2023-02-16 08:56:22.089600 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc0003fb8c0}
2023-02-16 08:56:22.099007 I | op-config: setting "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2023-02-16 08:56:22.099031 D | exec: Running command: ceph config set mon auth_allow_insecure_global_id_reclaim false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:56:22.120339 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-16 08:56:22.120591 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:22.145486 D | ceph-crashcollector-controller: "rook-ceph-mgr-a-5c6d65fcd5-6lchh" is a ceph pod!
2023-02-16 08:56:22.145571 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-16 08:56:22.145803 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:22.146586 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-16 08:56:22.146607 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:22.160777 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "created"
2023-02-16 08:56:22.162773 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:56:22.175595 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:56:22.175663 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-16 08:56:22.176308 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:22.181912 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-16 08:56:22.181961 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:22.193813 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-16 08:56:22.193853 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:22.204602 D | ceph-crashcollector-controller: "rook-ceph-crashcollector-node1-5567689cf9-lgqhb" is a ceph pod!
2023-02-16 08:56:22.206697 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-node1": the object has been modified; please apply your changes to the latest version and try again
2023-02-16 08:56:22.206747 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-16 08:56:22.207029 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:22.214219 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-16 08:56:22.214241 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:22.220830 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-node1": the object has been modified; please apply your changes to the latest version and try again
2023-02-16 08:56:22.220908 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-16 08:56:22.222378 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:22.235713 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-16 08:56:22.243205 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:56:22.245278 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-16 08:56:22.245299 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:22.249596 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:56:22.249628 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-16 08:56:22.249782 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:22.259618 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-16 08:56:22.260861 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:56:22.268230 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:56:22.357225 I | op-config: successfully set "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2023-02-16 08:56:22.357254 I | op-config: insecure global ID is now disabled
2023-02-16 08:56:22.361951 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:25.359977 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-16 08:56:25.360007 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:25.360038 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-16 08:56:25.360246 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:25.369197 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:25.372209 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-16 08:56:25.373157 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:56:25.376201 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:56:28.378556 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:31.388575 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:34.399155 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:37.407685 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:40.416591 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:43.425616 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:44.253223 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-16 08:56:44.253459 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:44.269253 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "created"
2023-02-16 08:56:44.270532 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:56:44.273748 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:56:44.273776 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-16 08:56:44.273950 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:44.281493 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-16 08:56:44.283202 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:56:44.286827 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:56:44.294608 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-16 08:56:44.294780 D | ceph-spec: object "rook-ceph-crashcollector-node2" matched on update
2023-02-16 08:56:44.294830 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:44.294840 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:44.304855 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-16 08:56:44.306234 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:56:44.312762 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:56:44.318029 D | ceph-spec: object "rook-ceph-crashcollector-node2" matched on update
2023-02-16 08:56:44.318046 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:44.318072 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-16 08:56:44.318462 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:44.324070 D | ceph-crashcollector-controller: "rook-ceph-crashcollector-node2-5b855df466-jtft6" is a ceph pod!
2023-02-16 08:56:44.328736 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-16 08:56:44.330100 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:56:44.334707 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:56:44.334748 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-16 08:56:44.334996 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:44.342807 D | ceph-spec: object "rook-ceph-crashcollector-node2" matched on update
2023-02-16 08:56:44.342833 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:44.348316 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-node2": the object has been modified; please apply your changes to the latest version and try again
2023-02-16 08:56:44.348376 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-16 08:56:44.348573 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:44.357308 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-16 08:56:44.358329 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:56:44.360994 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:56:44.361036 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-16 08:56:44.361179 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:44.368731 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-16 08:56:44.369829 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:56:44.373712 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:56:46.434216 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:47.003004 D | ceph-spec: object "rook-ceph-crashcollector-node2" matched on update
2023-02-16 08:56:47.003053 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:56:47.003061 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-16 08:56:47.003214 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:56:47.014604 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-16 08:56:47.015685 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:56:47.019007 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:56:49.440463 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:52.449653 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:55.459764 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:56:58.469249 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:01.478904 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:04.488399 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:07.497409 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:10.504585 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:13.514166 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:16.523081 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:19.530195 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:22.540080 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:25.548704 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:28.559463 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:31.570336 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:34.583341 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-16 08:56:22 +0000 UTC LastTransitionTime:2023-02-16 08:56:22 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5c6d65fcd5" is progressing.}] CollisionCount:<nil>}
2023-02-16 08:57:37.174027 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-16 08:57:37.174043 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:37.592138 I | op-k8sutil: finished waiting for updated deployment "rook-ceph-mgr-a"
2023-02-16 08:57:37.597115 D | op-k8sutil: kubernetes version fetched 1.20.4-aliyun.1
2023-02-16 08:57:37.597150 I | op-mgr: setting services to point to mgr "a"
2023-02-16 08:57:37.597160 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2023-02-16 08:57:37.609701 D | op-k8sutil: created service rook-ceph-mgr-dashboard
2023-02-16 08:57:37.609765 D | op-k8sutil: creating service rook-ceph-mgr
2023-02-16 08:57:37.625825 D | op-k8sutil: created service rook-ceph-mgr
2023-02-16 08:57:37.632340 I | op-mgr: no need to update service "rook-ceph-mgr"
2023-02-16 08:57:37.632363 I | op-mgr: no need to update service "rook-ceph-mgr-dashboard"
2023-02-16 08:57:37.632430 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph OSDs"
2023-02-16 08:57:37.633478 D | cephclient: balancer module is already 'on' on pacific, doing nothingbalancer
2023-02-16 08:57:37.633501 I | op-mgr: successful modules: balancer
2023-02-16 08:57:37.633522 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:37.633576 D | exec: Running command: ceph mgr module enable pg_autoscaler --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:37.633676 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:37.648565 I | op-osd: start running osds in namespace "rook-ceph"
2023-02-16 08:57:37.648586 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2023-02-16 08:57:37.648769 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:57:37.648777 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:57:37.660346 D | op-osd: 0 of 0 OSD Deployments need updated
2023-02-16 08:57:37.660369 I | op-osd: start provisioning the OSDs on PVCs, if needed
2023-02-16 08:57:37.666248 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2023-02-16 08:57:37.666267 I | op-osd: start provisioning the OSDs on nodes, if needed
2023-02-16 08:57:37.666273 W | op-osd: useAllNodes is TRUE, but nodes are specified. NODES in the cluster CR will be IGNORED unless useAllNodes is FALSE.
2023-02-16 08:57:37.672785 D | op-osd: storage nodes: [{Name:node1 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}} {Name:node2 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}} {Name:node3 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}}]
2023-02-16 08:57:37.680494 I | op-k8sutil: skipping creation of OSDs on nodes [node1 node2]: placement settings do not match
2023-02-16 08:57:37.680520 I | op-osd: 1 of the 3 storage nodes are valid
2023-02-16 08:57:37.713487 I | op-osd: started OSD provisioning job for node "node3"
2023-02-16 08:57:37.721379 I | op-osd: OSD orchestration status for node node3 is "starting"
2023-02-16 08:57:37.767650 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node3-x2tqr" is a ceph pod!
2023-02-16 08:57:37.767703 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-16 08:57:37.767873 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:57:37.769216 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:57:37.777464 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:57:38.802315 I | op-config: setting "global"="osd_pool_default_pg_autoscale_mode"="on" option to the mon configuration database
2023-02-16 08:57:38.802377 D | exec: Running command: ceph config set global osd_pool_default_pg_autoscale_mode on --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:38.802777 I | op-mgr: successful modules: prometheus
2023-02-16 08:57:39.110826 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node3-status" cm is handled by another watcher
2023-02-16 08:57:39.110855 D | ceph-spec: object "rook-ceph-osd-node3-status" matched on update
2023-02-16 08:57:39.110877 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-16 08:57:39.111051 I | op-osd: OSD orchestration status for node node3 is "orchestrating"
2023-02-16 08:57:39.140662 I | op-config: successfully set "global"="osd_pool_default_pg_autoscale_mode"="on" option to the mon configuration database
2023-02-16 08:57:39.140695 I | op-config: setting "global"="mon_pg_warn_min_per_osd"="0" option to the mon configuration database
2023-02-16 08:57:39.140714 D | exec: Running command: ceph config set global mon_pg_warn_min_per_osd 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:39.475624 I | op-config: successfully set "global"="mon_pg_warn_min_per_osd"="0" option to the mon configuration database
2023-02-16 08:57:39.475654 I | op-mgr: successful modules: mgr module(s) from the spec
2023-02-16 08:57:43.818252 D | exec: Running command: ceph dashboard create-self-signed-cert --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:46.265489 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2023-02-16 08:57:46.265529 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:46.667590 I | op-mgr: setting ceph dashboard "admin" login creds
2023-02-16 08:57:46.667774 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/3033155489 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:47.214092 I | op-mgr: successfully set ceph dashboard creds
2023-02-16 08:57:47.214171 D | exec: Running command: ceph config get mgr.a mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:47.521208 D | exec: Running command: ceph config get mgr.a mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:47.824332 D | exec: Running command: ceph config get mgr.a mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:48.124202 I | op-config: setting "mgr.a"="mgr/dashboard/server_port"="8443" option to the mon configuration database
2023-02-16 08:57:48.124244 D | exec: Running command: ceph config set mgr.a mgr/dashboard/server_port 8443 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:48.228896 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node3-status" cm is handled by another watcher
2023-02-16 08:57:48.228993 D | ceph-spec: object "rook-ceph-osd-node3-status" matched on update
2023-02-16 08:57:48.229043 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-16 08:57:48.229046 I | op-osd: OSD orchestration status for node node3 is "completed"
2023-02-16 08:57:48.229056 I | op-osd: creating OSD 0 on node "node3"
2023-02-16 08:57:48.229210 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-16 08:57:48.229215 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-16 08:57:48.229271 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 0 on node \"node3\""
2023-02-16 08:57:48.244889 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:57:48.244904 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:57:48.260666 I | op-osd: creating OSD 1 on node "node3"
2023-02-16 08:57:48.260783 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-16 08:57:48.260789 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-16 08:57:48.260851 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"node3\""
2023-02-16 08:57:48.275327 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:57:48.275344 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:57:48.290233 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2023-02-16 08:57:48.290264 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:48.306320 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-16 08:57:48.306351 D | ceph-spec: do not reconcile on "rook-ceph-osd-node3-status" config map changes
2023-02-16 08:57:48.306361 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-16 08:57:48.306371 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-16 08:57:48.330165 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:48.331467 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2023-02-16 08:57:48.331503 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:48.331521 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2023-02-16 08:57:48.331525 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:48.351299 D | ceph-crashcollector-controller: "rook-ceph-osd-0-7475997cd7-2bdvv" is a ceph pod!
2023-02-16 08:57:48.351398 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-16 08:57:48.351622 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:57:48.371403 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "created"
2023-02-16 08:57:48.371727 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2023-02-16 08:57:48.371737 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:48.372903 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:57:48.383030 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:57:48.383111 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-16 08:57:48.383383 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:57:48.409439 D | ceph-crashcollector-controller: "rook-ceph-osd-1-848dbd9fdc-rrgr9" is a ceph pod!
2023-02-16 08:57:48.433464 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-16 08:57:48.435018 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:57:48.441714 D | ceph-spec: object "rook-ceph-crashcollector-node3" matched on update
2023-02-16 08:57:48.441737 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:48.457492 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2023-02-16 08:57:48.457516 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:48.482503 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:57:48.482538 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-16 08:57:48.482771 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:57:48.483709 D | ceph-crashcollector-controller: "rook-ceph-crashcollector-node3-849dfc465f-5dllm" is a ceph pod!
2023-02-16 08:57:48.505472 D | ceph-spec: object "rook-ceph-crashcollector-node3" matched on update
2023-02-16 08:57:48.505494 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:48.510312 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-node3": the object has been modified; please apply your changes to the latest version and try again
2023-02-16 08:57:48.510391 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-16 08:57:48.510682 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:57:48.518763 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2023-02-16 08:57:48.518835 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:48.522612 D | ceph-spec: object "rook-ceph-crashcollector-node3" matched on update
2023-02-16 08:57:48.522635 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:48.526513 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-node3": the object has been modified; please apply your changes to the latest version and try again
2023-02-16 08:57:48.526557 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-16 08:57:48.526749 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:57:48.535535 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-16 08:57:48.536679 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:57:48.540053 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:57:48.540100 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-16 08:57:48.540311 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:57:48.549118 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-16 08:57:48.552027 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:57:48.557258 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:57:48.599246 I | op-config: successfully set "mgr.a"="mgr/dashboard/server_port"="8443" option to the mon configuration database
2023-02-16 08:57:48.599273 D | exec: Running command: ceph config get mgr.a mgr/dashboard/ssl_server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:48.843610 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2}}
2023-02-16 08:57:48.843627 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2}}
2023-02-16 08:57:48.843657 I | op-osd: finished running OSDs in namespace "rook-ceph"
2023-02-16 08:57:48.843662 I | ceph-cluster-controller: done reconciling ceph cluster in namespace "rook-ceph"
2023-02-16 08:57:48.843697 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-16 08:57:48.859771 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "rook-ceph"
2023-02-16 08:57:48.859789 I | op-osd: ceph osd status in namespace "rook-ceph" check interval "1m0s"
2023-02-16 08:57:48.859793 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "rook-ceph"
2023-02-16 08:57:48.859798 I | ceph-cluster-controller: ceph status check interval is 1m0s
2023-02-16 08:57:48.859800 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "rook-ceph"
2023-02-16 08:57:48.859819 D | ceph-cluster-controller: successfully configured CephCluster "rook-ceph/rook-ceph"
2023-02-16 08:57:48.859901 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-16 08:57:48.859977 I | ceph-cluster-controller: reporting cluster telemetry
2023-02-16 08:57:48.859996 D | op-config: setting "rook/version"="v1.9.9" option in the mon config-key store
2023-02-16 08:57:48.860009 D | exec: Running command: ceph config-key set rook/version v1.9.9 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:48.861266 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:57:48.861298 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:57:48.861582 D | ceph-cluster-controller: checking health of cluster
2023-02-16 08:57:48.861620 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-16 08:57:48.953733 I | op-mgr: dashboard config has changed. restarting the dashboard module
2023-02-16 08:57:48.953751 I | op-mgr: restarting the mgr module
2023-02-16 08:57:48.953768 D | exec: Running command: ceph mgr module disable dashboard --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:49.604334 D | telemetry: set telemetry key: rook/version=v1.9.9
2023-02-16 08:57:49.607169 D | op-config: setting "rook/kubernetes/version"="v1.20.4-aliyun.1" option in the mon config-key store
2023-02-16 08:57:49.607202 D | exec: Running command: ceph config-key set rook/kubernetes/version v1.20.4-aliyun.1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:49.671606 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[TOO_FEW_OSDS:{Severity:HEALTH_WARN Summary:{Message:OSD count 0 < osd_pool_default_size 3}}]} FSID:a1e619d5-e14a-4cc6-9fa5-938b0c43b82c ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:0 AvailableBytes:0 TotalBytes:0 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-16 08:57:49.690105 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:49.708311 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:50.197034 D | telemetry: set telemetry key: rook/kubernetes/version=v1.20.4-aliyun.1
2023-02-16 08:57:50.197052 D | op-config: setting "rook/csi/version"="v3.6.2" option in the mon config-key store
2023-02-16 08:57:50.197066 D | exec: Running command: ceph config-key set rook/csi/version v3.6.2 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:50.280782 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2}}
2023-02-16 08:57:50.280801 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2}}
2023-02-16 08:57:50.280870 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[TOO_FEW_OSDS:{Severity:HEALTH_WARN Summary:{Message:OSD count 0 < osd_pool_default_size 3}}]} FSID:a1e619d5-e14a-4cc6-9fa5-938b0c43b82c ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:0 AvailableBytes:0 TotalBytes:0 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-16 08:57:50.280877 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-16 08:57:50.296208 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2023-02-16 08:57:50.297021 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:57:50.297030 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:57:50.302489 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "TOO_FEW_OSDS", message: "OSD count 0 < osd_pool_default_size 3"
2023-02-16 08:57:50.685589 D | telemetry: set telemetry key: rook/csi/version=v3.6.2
2023-02-16 08:57:50.685608 D | op-config: setting "rook/cluster/mon/max-id"="0" option in the mon config-key store
2023-02-16 08:57:50.685623 D | exec: Running command: ceph config-key set rook/cluster/mon/max-id 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:51.084052 D | telemetry: set telemetry key: rook/cluster/mon/max-id=0
2023-02-16 08:57:51.084097 D | op-config: setting "rook/cluster/mon/count"="1" option in the mon config-key store
2023-02-16 08:57:51.084149 D | exec: Running command: ceph config-key set rook/cluster/mon/count 1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:51.182983 I | op-mgr: successful modules: dashboard
2023-02-16 08:57:51.560325 D | telemetry: set telemetry key: rook/cluster/mon/count=1
2023-02-16 08:57:51.560359 D | op-config: setting "rook/cluster/mon/allow-multiple-per-node"="false" option in the mon config-key store
2023-02-16 08:57:51.560376 D | exec: Running command: ceph config-key set rook/cluster/mon/allow-multiple-per-node false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:52.009129 D | telemetry: set telemetry key: rook/cluster/mon/allow-multiple-per-node=false
2023-02-16 08:57:52.009153 D | op-config: setting "rook/cluster/mon/pvc/enabled"="false" option in the mon config-key store
2023-02-16 08:57:52.009169 D | exec: Running command: ceph config-key set rook/cluster/mon/pvc/enabled false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:52.424806 D | telemetry: set telemetry key: rook/cluster/mon/pvc/enabled=false
2023-02-16 08:57:52.424833 D | op-config: setting "rook/cluster/mon/stretch/enabled"="false" option in the mon config-key store
2023-02-16 08:57:52.424852 D | exec: Running command: ceph config-key set rook/cluster/mon/stretch/enabled false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:52.537998 D | ceph-spec: object "rook-ceph-crashcollector-node3" matched on update
2023-02-16 08:57:52.538059 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:57:52.538099 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-16 08:57:52.538454 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-16 08:57:52.561748 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-16 08:57:52.565370 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-16 08:57:52.574052 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-16 08:57:52.904857 D | telemetry: set telemetry key: rook/cluster/mon/stretch/enabled=false
2023-02-16 08:57:52.904898 D | op-config: setting "rook/cluster/storage/device-set/count/total"="0" option in the mon config-key store
2023-02-16 08:57:52.904935 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/total 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:53.340494 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/total=0
2023-02-16 08:57:53.340529 D | op-config: setting "rook/cluster/storage/device-set/count/portable"="0" option in the mon config-key store
2023-02-16 08:57:53.340544 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/portable 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:53.766311 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/portable=0
2023-02-16 08:57:53.766330 D | op-config: setting "rook/cluster/storage/device-set/count/non-portable"="0" option in the mon config-key store
2023-02-16 08:57:53.766341 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/non-portable 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:54.184403 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/non-portable=0
2023-02-16 08:57:54.184430 D | op-config: setting "rook/cluster/network/provider"="host" option in the mon config-key store
2023-02-16 08:57:54.184445 D | exec: Running command: ceph config-key set rook/cluster/network/provider host --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:54.600059 D | telemetry: set telemetry key: rook/cluster/network/provider=host
2023-02-16 08:57:54.600078 D | op-config: setting "rook/cluster/external-mode"="false" option in the mon config-key store
2023-02-16 08:57:54.600089 D | exec: Running command: ceph config-key set rook/cluster/external-mode false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:57:55.031667 D | telemetry: set telemetry key: rook/cluster/external-mode=false
2023-02-16 08:58:33.860543 D | op-mon: checking health of mons
2023-02-16 08:58:33.860575 D | op-mon: Acquiring lock for mon orchestration
2023-02-16 08:58:33.860595 D | op-mon: Acquired lock for mon orchestration
2023-02-16 08:58:33.868685 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-16 08:58:33.868722 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:58:34.235567 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.98:6789/0 PublicAddr:10.211.55.98:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.98:3300 Nonce:0} {Type:v1 Addr:10.211.55.98:6789 Nonce:0}]}}]}}
2023-02-16 08:58:34.235599 D | op-mon: targeting the mon count 1
2023-02-16 08:58:34.235607 D | op-mon: mon "a" found in quorum
2023-02-16 08:58:34.235610 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-16 08:58:34.242313 I | op-mon: checking if multiple mons are on the same node
2023-02-16 08:58:34.249535 D | op-mon: analyzing mon pod "rook-ceph-mon-a-6d9bf6d78b-kx289" on node "node2"
2023-02-16 08:58:34.249564 D | op-mon: Released lock for mon orchestration
2023-02-16 08:58:34.249572 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-16 08:58:48.861022 D | op-osd: checking osd processes status.
2023-02-16 08:58:48.861081 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:58:49.166829 D | op-osd: validating status of osd.0
2023-02-16 08:58:49.166859 D | op-osd: osd.0 is healthy.
2023-02-16 08:58:49.166864 D | op-osd: validating status of osd.1
2023-02-16 08:58:49.166867 D | op-osd: osd.1 is healthy.
2023-02-16 08:58:49.166877 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:58:49.476773 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:58:49.476806 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:58:50.302972 D | ceph-cluster-controller: checking health of cluster
2023-02-16 08:58:50.303041 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-16 08:58:50.671087 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[TOO_FEW_OSDS:{Severity:HEALTH_WARN Summary:{Message:OSD count 2 < osd_pool_default_size 3}}]} FSID:a1e619d5-e14a-4cc6-9fa5-938b0c43b82c ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:9912320 AvailableBytes:549745901568 TotalBytes:549755813888 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-16 08:58:50.678827 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:58:51.042989 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":4}}
2023-02-16 08:58:51.043019 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":4}}
2023-02-16 08:58:51.043080 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[TOO_FEW_OSDS:{Severity:HEALTH_WARN Summary:{Message:OSD count 2 < osd_pool_default_size 3}}]} FSID:a1e619d5-e14a-4cc6-9fa5-938b0c43b82c ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:9912320 AvailableBytes:549745901568 TotalBytes:549755813888 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-16 08:58:51.043087 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-16 08:58:51.056079 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2023-02-16 08:58:51.056321 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-16 08:58:51.056348 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-16 08:58:51.061671 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "TOO_FEW_OSDS", message: "OSD count 2 < osd_pool_default_size 3"
2023-02-16 08:59:00.122283 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2023-02-16 08:59:00.122318 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:59:13.100406 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2023-02-16 08:59:13.100426 D | ceph-spec: do not reconcile deployments updates
2023-02-16 08:59:19.250163 D | op-mon: checking health of mons
2023-02-16 08:59:19.250196 D | op-mon: Acquiring lock for mon orchestration
2023-02-16 08:59:19.250202 D | op-mon: Acquired lock for mon orchestration
2023-02-16 08:59:19.255721 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-16 08:59:19.255757 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-16 08:59:19.613311 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.98:6789/0 PublicAddr:10.211.55.98:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.98:3300 Nonce:0} {Type:v1 Addr:10.211.55.98:6789 Nonce:0}]}}]}}
2023-02-16 08:59:19.613339 D | op-mon: targeting the mon count 1
2023-02-16 08:59:19.613347 D | op-mon: mon "a" found in quorum
2023-02-16 08:59:19.613349 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-16 08:59:19.619841 D | op-mon: Released lock for mon orchestration
2023-02-16 08:59:19.619871 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
