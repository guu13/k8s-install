2023-02-17 00:45:07.298618 I | rookcmd: starting Rook v1.9.9 with arguments '/usr/local/bin/rook ceph operator'
2023-02-17 00:45:07.298701 I | rookcmd: flag values: --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-level=INFO, --operator-image=, --service-account=
2023-02-17 00:45:07.298706 I | cephcmd: starting Rook-Ceph operator
2023-02-17 00:45:07.417477 I | cephcmd: base ceph version inside the rook operator image is "ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)"
2023-02-17 00:45:07.423334 I | op-k8sutil: ROOK_CURRENT_NAMESPACE_ONLY="true" (env var)
2023-02-17 00:45:07.423361 I | operator: watching the current namespace "rook-ceph" for a Ceph CRs
2023-02-17 00:45:07.423409 I | operator: setting up schemes
2023-02-17 00:45:07.425010 I | operator: setting up the controller-runtime manager
2023-02-17 00:45:08.178013 I | operator: Fetching webhook cert-manager-webhook to see if cert-manager is installed.
2023-02-17 00:45:08.184527 I | operator: failed to get cert manager
2023-02-17 00:45:08.184613 I | ceph-cluster-controller: successfully started
2023-02-17 00:45:08.184663 I | ceph-cluster-controller: enabling hotplug orchestration
2023-02-17 00:45:08.184676 I | ceph-crashcollector-controller: successfully started
2023-02-17 00:45:08.184696 I | ceph-block-pool-controller: successfully started
2023-02-17 00:45:08.184713 I | ceph-object-store-user-controller: successfully started
2023-02-17 00:45:08.184724 I | ceph-object-realm-controller: successfully started
2023-02-17 00:45:08.184732 I | ceph-object-zonegroup-controller: successfully started
2023-02-17 00:45:08.184739 I | ceph-object-zone-controller: successfully started
2023-02-17 00:45:08.184824 I | ceph-object-controller: successfully started
2023-02-17 00:45:08.184863 I | ceph-file-controller: successfully started
2023-02-17 00:45:08.184885 I | ceph-nfs-controller: successfully started
2023-02-17 00:45:08.184898 I | ceph-rbd-mirror-controller: successfully started
2023-02-17 00:45:08.184912 I | ceph-client-controller: successfully started
2023-02-17 00:45:08.184924 I | ceph-filesystem-mirror-controller: successfully started
2023-02-17 00:45:08.184947 I | operator: rook-ceph-operator-config-controller successfully started
2023-02-17 00:45:08.184960 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2023-02-17 00:45:08.184971 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2023-02-17 00:45:08.184982 I | ceph-bucket-topic: successfully started
2023-02-17 00:45:08.185004 I | ceph-bucket-notification: successfully started
2023-02-17 00:45:08.185038 I | ceph-bucket-notification: successfully started
2023-02-17 00:45:08.185049 I | ceph-fs-subvolumegroup-controller: successfully started
2023-02-17 00:45:08.185055 I | blockpool-rados-namespace-controller: successfully started
2023-02-17 00:45:08.186987 I | operator: starting the controller-runtime manager
2023-02-17 00:45:08.290575 I | op-k8sutil: ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS="15" (configmap)
2023-02-17 00:45:08.290605 I | op-k8sutil: ROOK_LOG_LEVEL="DEBUG" (configmap)
2023-02-17 00:45:08.290614 I | op-k8sutil: ROOK_ENABLE_DISCOVERY_DAEMON="false" (configmap)
2023-02-17 00:45:08.291814 D | ceph-cluster-controller: "ceph-cluster-controller": no CephCluster resource found in namespace ""
2023-02-17 00:45:08.291830 D | ceph-cluster-controller: node watcher: useAllNodes is set to false and no nodes storageClassDevicesets or volumeSources are specified in cluster "", skipping
2023-02-17 00:45:08.291841 D | ceph-cluster-controller: "ceph-cluster-controller": no CephCluster resource found in namespace ""
2023-02-17 00:45:08.291844 D | ceph-cluster-controller: node watcher: useAllNodes is set to false and no nodes storageClassDevicesets or volumeSources are specified in cluster "", skipping
2023-02-17 00:45:08.291865 D | ceph-cluster-controller: "ceph-cluster-controller": no CephCluster resource found in namespace ""
2023-02-17 00:45:08.291868 D | ceph-cluster-controller: node watcher: useAllNodes is set to false and no nodes storageClassDevicesets or volumeSources are specified in cluster "", skipping
2023-02-17 00:45:08.292694 D | ceph-csi: no ceph cluster found not deploying ceph csi driver
2023-02-17 00:45:08.292731 I | ceph-csi: CSI Ceph RBD driver disabled
2023-02-17 00:45:08.292740 I | op-k8sutil: removing daemonset csi-rbdplugin if it exists
2023-02-17 00:45:08.293772 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:45:08.293964 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:45:08.294016 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:45:08.295363 I | operator: rook-ceph-operator-config-controller done reconciling
2023-02-17 00:45:08.296445 D | op-k8sutil: removing csi-rbdplugin-provisioner deployment if it exists
2023-02-17 00:45:08.296473 I | op-k8sutil: removing deployment csi-rbdplugin-provisioner if it exists
2023-02-17 00:45:08.306580 D | ceph-csi: rook-ceph.rbd.csi.ceph.com CSIDriver not found; skipping deletion.
2023-02-17 00:45:08.306607 I | ceph-csi: successfully removed CSI Ceph RBD driver
2023-02-17 00:45:08.306613 I | ceph-csi: CSI CephFS driver disabled
2023-02-17 00:45:08.306618 I | op-k8sutil: removing daemonset csi-cephfsplugin if it exists
2023-02-17 00:45:08.310394 D | op-k8sutil: removing csi-cephfsplugin-provisioner deployment if it exists
2023-02-17 00:45:08.310422 I | op-k8sutil: removing deployment csi-cephfsplugin-provisioner if it exists
2023-02-17 00:45:08.319430 D | ceph-csi: rook-ceph.cephfs.csi.ceph.com CSIDriver not found; skipping deletion.
2023-02-17 00:45:08.319456 I | ceph-csi: successfully removed CSI CephFS driver
2023-02-17 00:45:08.319462 I | ceph-csi: CSI NFS driver disabled
2023-02-17 00:45:08.319467 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2023-02-17 00:45:08.322075 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2023-02-17 00:45:08.322102 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2023-02-17 00:45:08.330075 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2023-02-17 00:45:08.330100 I | ceph-csi: successfully removed CSI NFS driver
2023-02-17 00:46:07.424029 D | operator: number of goroutines 405
2023-02-17 00:46:44.280557 D | clusterdisruption-controller: create event from ceph cluster CR
2023-02-17 00:46:44.280614 D | clusterdisruption-controller: reconciling "rook-ceph/rook-ceph"
2023-02-17 00:46:44.280643 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:46:44.280659 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:46:44.280748 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:46:44.280758 D | ceph-cluster-controller: create event from a CR
2023-02-17 00:46:44.281026 I | ceph-spec: adding finalizer "cephcluster.ceph.rook.io" on "rook-ceph"
2023-02-17 00:46:44.293240 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (default)
2023-02-17 00:46:44.293297 D | ceph-csi: not a multus cluster "rook-ceph/rook-ceph" or CSI_ENABLE_HOST_NETWORK is true, not deploying the ceph-csi plugin holder
2023-02-17 00:46:44.297380 I | clusterdisruption-controller: deleted all legacy node drain canary pods
2023-02-17 00:46:44.302870 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:46:44.302893 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:46:44.303007 D | ceph-cluster-controller: skipping resource "rook-ceph" update with unchanged spec
2023-02-17 00:46:44.303013 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2023-02-17 00:46:44.307261 I | ceph-cluster-controller: clusterInfo not yet found, must be a new cluster.
2023-02-17 00:46:44.310806 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
2023-02-17 00:46:44.320453 D | ceph-cluster-controller: cluster spec successfully validated
2023-02-17 00:46:44.320556 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Detecting Ceph version"
2023-02-17 00:46:44.332844 I | op-k8sutil: ROOK_CSI_ENABLE_RBD="true" (configmap)
2023-02-17 00:46:44.332861 I | op-k8sutil: ROOK_CSI_ENABLE_CEPHFS="true" (configmap)
2023-02-17 00:46:44.332865 I | op-k8sutil: ROOK_CSI_ENABLE_NFS="false" (configmap)
2023-02-17 00:46:44.332868 I | op-k8sutil: ROOK_CSI_ALLOW_UNSUPPORTED_VERSION="false" (configmap)
2023-02-17 00:46:44.332870 I | op-k8sutil: ROOK_CSI_ENABLE_GRPC_METRICS="false" (configmap)
2023-02-17 00:46:44.332873 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (default)
2023-02-17 00:46:44.332876 I | op-k8sutil: CSI_FORCE_CEPHFS_KERNEL_CLIENT="true" (configmap)
2023-02-17 00:46:44.332878 I | op-k8sutil: CSI_GRPC_TIMEOUT_SECONDS="150" (configmap)
2023-02-17 00:46:44.332881 I | op-k8sutil: CSI_CEPHFS_GRPC_METRICS_PORT="9091" (default)
2023-02-17 00:46:44.332883 I | op-k8sutil: CSI_CEPHFS_GRPC_METRICS_PORT="9091" (default)
2023-02-17 00:46:44.332886 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2023-02-17 00:46:44.332888 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2023-02-17 00:46:44.332890 I | op-k8sutil: CSI_RBD_GRPC_METRICS_PORT="9090" (default)
2023-02-17 00:46:44.332892 I | op-k8sutil: CSI_RBD_GRPC_METRICS_PORT="9090" (default)
2023-02-17 00:46:44.332894 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2023-02-17 00:46:44.332897 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2023-02-17 00:46:44.332899 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2023-02-17 00:46:44.332901 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2023-02-17 00:46:44.332903 I | op-k8sutil: CSI_ENABLE_LIVENESS="false" (configmap)
2023-02-17 00:46:44.332905 I | op-k8sutil: CSI_PLUGIN_PRIORITY_CLASSNAME="system-node-critical" (configmap)
2023-02-17 00:46:44.332908 I | op-k8sutil: CSI_PROVISIONER_PRIORITY_CLASSNAME="system-cluster-critical" (configmap)
2023-02-17 00:46:44.332911 I | op-k8sutil: CSI_ENABLE_OMAP_GENERATOR="false" (default)
2023-02-17 00:46:44.332913 I | op-k8sutil: CSI_ENABLE_RBD_SNAPSHOTTER="true" (configmap)
2023-02-17 00:46:44.332916 I | op-k8sutil: CSI_ENABLE_CEPHFS_SNAPSHOTTER="true" (configmap)
2023-02-17 00:46:44.332918 I | op-k8sutil: CSI_ENABLE_VOLUME_REPLICATION="false" (configmap)
2023-02-17 00:46:44.332920 I | op-k8sutil: CSI_ENABLE_CSIADDONS="false" (configmap)
2023-02-17 00:46:44.332922 I | op-k8sutil: CSI_ENABLE_ENCRYPTION="false" (configmap)
2023-02-17 00:46:44.332925 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2023-02-17 00:46:44.332927 I | op-k8sutil: CSI_NFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2023-02-17 00:46:44.332929 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2023-02-17 00:46:44.332932 I | op-k8sutil: CSI_PLUGIN_ENABLE_SELINUX_HOST_MOUNT="false" (configmap)
2023-02-17 00:46:44.332934 I | ceph-csi: Kubernetes version is 1.20+
2023-02-17 00:46:44.332937 I | op-k8sutil: ROOK_CSI_RESIZER_IMAGE="registry.k8s.io/sig-storage/csi-resizer:v1.4.0" (default)
2023-02-17 00:46:44.332939 I | op-k8sutil: CSI_LOG_LEVEL="" (default)
2023-02-17 00:46:44.332941 I | op-k8sutil: CSI_SIDECAR_LOG_LEVEL="" (default)
2023-02-17 00:46:44.333142 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v16.2.10...
2023-02-17 00:46:44.333209 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:46:44.333218 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:46:44.335945 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2023-02-17 00:46:44.339080 I | op-k8sutil: CSI_PROVISIONER_REPLICAS="2" (configmap)
2023-02-17 00:46:44.339126 I | op-k8sutil: ROOK_CSI_CEPH_IMAGE="quay.io/cephcsi/cephcsi:v3.6.2" (default)
2023-02-17 00:46:44.339135 I | op-k8sutil: ROOK_CSI_NFS_IMAGE="registry.k8s.io/sig-storage/nfsplugin:v4.0.0" (default)
2023-02-17 00:46:44.339140 I | op-k8sutil: ROOK_CSI_REGISTRAR_IMAGE="registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.5.1" (default)
2023-02-17 00:46:44.339145 I | op-k8sutil: ROOK_CSI_PROVISIONER_IMAGE="registry.k8s.io/sig-storage/csi-provisioner:v3.1.0" (default)
2023-02-17 00:46:44.339150 I | op-k8sutil: ROOK_CSI_ATTACHER_IMAGE="registry.k8s.io/sig-storage/csi-attacher:v3.4.0" (default)
2023-02-17 00:46:44.339154 I | op-k8sutil: ROOK_CSI_SNAPSHOTTER_IMAGE="registry.k8s.io/sig-storage/csi-snapshotter:v6.0.1" (default)
2023-02-17 00:46:44.339156 I | op-k8sutil: ROOK_CSI_KUBELET_DIR_PATH="/var/lib/kubelet" (default)
2023-02-17 00:46:44.339159 I | op-k8sutil: CSI_VOLUME_REPLICATION_IMAGE="quay.io/csiaddons/volumereplication-operator:v0.3.0" (default)
2023-02-17 00:46:44.339162 I | op-k8sutil: ROOK_CSIADDONS_IMAGE="quay.io/csiaddons/k8s-sidecar:v0.4.0" (default)
2023-02-17 00:46:44.339164 I | op-k8sutil: ROOK_CSI_CEPHFS_POD_LABELS="" (default)
2023-02-17 00:46:44.339167 I | op-k8sutil: ROOK_CSI_NFS_POD_LABELS="" (default)
2023-02-17 00:46:44.339169 I | op-k8sutil: ROOK_CSI_RBD_POD_LABELS="" (default)
2023-02-17 00:46:44.339172 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.6.2"
2023-02-17 00:46:44.339216 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2023-02-17 00:46:44.339224 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-17 00:46:44.343890 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2023-02-17 00:46:45.719638 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2023-02-17 00:46:45.750118 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-csi-detect-version" cm is handled by another watcher
2023-02-17 00:46:45.750667 I | ceph-csi: Detected ceph CSI image version: "v3.6.2"
2023-02-17 00:46:45.754896 D | ceph-csi: "rook-ceph.rbd.csi.ceph.com" CSIDriver not found; skipping deletion.
2023-02-17 00:46:45.758762 D | ceph-csi: "rook-ceph.cephfs.csi.ceph.com" CSIDriver not found; skipping deletion.
2023-02-17 00:46:45.762461 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2023-02-17 00:46:45.762476 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-17 00:46:45.762480 I | op-k8sutil: CSI_PLUGIN_TOLERATIONS="" (default)
2023-02-17 00:46:45.762483 I | op-k8sutil: CSI_PLUGIN_NODE_AFFINITY="" (default)
2023-02-17 00:46:45.762486 I | op-k8sutil: CSI_RBD_PLUGIN_TOLERATIONS="" (default)
2023-02-17 00:46:45.762488 I | op-k8sutil: CSI_RBD_PLUGIN_NODE_AFFINITY="" (default)
2023-02-17 00:46:45.762490 I | op-k8sutil: CSI_RBD_PLUGIN_RESOURCE="" (default)
2023-02-17 00:46:45.777327 I | op-k8sutil: CSI_RBD_PROVISIONER_TOLERATIONS="" (default)
2023-02-17 00:46:45.777342 I | op-k8sutil: CSI_RBD_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-17 00:46:45.777346 I | op-k8sutil: CSI_RBD_PROVISIONER_RESOURCE="" (default)
2023-02-17 00:46:45.792868 I | ceph-csi: successfully started CSI Ceph RBD driver
2023-02-17 00:46:45.792891 I | op-k8sutil: CSI_CEPHFS_PLUGIN_TOLERATIONS="" (default)
2023-02-17 00:46:45.792897 I | op-k8sutil: CSI_CEPHFS_PLUGIN_NODE_AFFINITY="" (default)
2023-02-17 00:46:45.792903 I | op-k8sutil: CSI_CEPHFS_PLUGIN_RESOURCE="" (default)
2023-02-17 00:46:45.820155 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_TOLERATIONS="" (default)
2023-02-17 00:46:45.820183 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-17 00:46:45.820190 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_RESOURCE="" (default)
2023-02-17 00:46:45.822159 D | CmdReporter: job rook-ceph-detect-version has returned results
2023-02-17 00:46:45.867378 I | ceph-csi: successfully started CSI CephFS driver
2023-02-17 00:46:45.867407 I | op-k8sutil: CSI_RBD_FSGROUPPOLICY="ReadWriteOnceWithFSType" (configmap)
2023-02-17 00:46:45.957657 I | ceph-spec: detected ceph image version: "16.2.10-0 pacific"
2023-02-17 00:46:45.957694 I | ceph-cluster-controller: validating ceph version from provided image
2023-02-17 00:46:45.960186 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-detect-version" cm is handled by another watcher
2023-02-17 00:46:45.972869 I | ceph-csi: CSIDriver object created for driver "rook-ceph.rbd.csi.ceph.com"
2023-02-17 00:46:45.972899 I | op-k8sutil: CSI_CEPHFS_FSGROUPPOLICY="ReadWriteOnceWithFSType" (configmap)
2023-02-17 00:46:46.008121 D | ceph-cluster-controller: cluster not initialized, nothing to validate. clusterInfo is nil
2023-02-17 00:46:46.008162 I | ceph-cluster-controller: cluster "rook-ceph": version "16.2.10-0 pacific" detected for image "quay.io/ceph/ceph:v16.2.10"
2023-02-17 00:46:46.039820 I | ceph-csi: CSIDriver object created for driver "rook-ceph.cephfs.csi.ceph.com"
2023-02-17 00:46:46.039847 I | ceph-csi: CSI NFS driver disabled
2023-02-17 00:46:46.039855 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2023-02-17 00:46:46.050746 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2023-02-17 00:46:46.050770 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2023-02-17 00:46:46.051061 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:46:46.051073 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:46:46.051173 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring the Ceph cluster"
2023-02-17 00:46:46.069388 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:46:46.069465 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:46:46.077937 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2023-02-17 00:46:46.078025 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mons"
2023-02-17 00:46:46.091568 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:46:46.091598 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:46:46.092669 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2023-02-17 00:46:46.092696 I | ceph-csi: successfully removed CSI NFS driver
2023-02-17 00:46:46.093166 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:46:46.093222 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:46:46.093234 I | op-mon: start running mons
2023-02-17 00:46:46.093239 D | op-mon: establishing ceph cluster info
2023-02-17 00:46:46.284491 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/mon.keyring --gen-key -n mon. --cap mon 'allow *'
2023-02-17 00:46:46.302760 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mgr 'allow *' --cap mds 'allow'
2023-02-17 00:46:46.330223 I | ceph-spec: creating mon secrets for a new cluster
2023-02-17 00:46:46.545193 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-17 00:46:46.545233 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-17 00:46:46.545249 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-17 00:46:46.545257 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-17 00:46:46.552692 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:46:46.552729 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:46:46.552742 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:46:46.552750 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:46:47.285662 I | op-mon: existing maxMonID not found or failed to load. configmaps "rook-ceph-mon-endpoints" not found
2023-02-17 00:46:47.488996 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":[],"namespace":""}] data: mapping:{"node":{}} maxMonId:-1]
2023-02-17 00:46:47.687613 D | op-config: creating config secret "rook-ceph-config"
2023-02-17 00:46:47.888034 D | op-config: updating config secret "rook-ceph-config"
2023-02-17 00:46:48.285240 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2023-02-17 00:46:48.285378 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2023-02-17 00:46:48.285402 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2023-02-17 00:46:48.685990 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-csi-config" cm is handled by another watcher
2023-02-17 00:46:48.884943 D | op-cfg-keyring: creating secret for rook-ceph-mons-keyring
2023-02-17 00:46:49.285706 D | op-cfg-keyring: creating secret for rook-ceph-admin-keyring
2023-02-17 00:46:49.885165 I | op-mon: targeting the mon count 1
2023-02-17 00:46:49.889461 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP: Port:6789 Zone: DataPathMap:0xc00089cc60}
2023-02-17 00:46:49.896078 I | op-mon: created canary deployment rook-ceph-mon-a-canary
2023-02-17 00:46:49.912103 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-17 00:46:49.912151 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:46:49.935838 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-17 00:46:49.935862 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:46:49.979878 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-17 00:46:49.979904 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:46:50.286925 I | op-mon: canary monitor deployment rook-ceph-mon-a-canary scheduled to node1
2023-02-17 00:46:50.286955 I | op-mon: mon a assigned to node node1
2023-02-17 00:46:50.286962 D | op-mon: using internal IP 10.211.55.97 for node node1
2023-02-17 00:46:50.286984 D | op-mon: mons have been scheduled
2023-02-17 00:46:50.290928 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-a-canary"
2023-02-17 00:46:50.300465 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-17 00:46:50.300497 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:46:50.300584 I | op-mon: creating mon a
2023-02-17 00:46:50.300609 I | op-mon: setting mon endpoints for hostnetwork mode
2023-02-17 00:46:50.309443 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-17 00:46:50.309474 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:46:50.354053 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-17 00:46:50.354068 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:46:50.688648 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2023-02-17 00:46:50.886296 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2023-02-17 00:46:50.886351 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:46:50.886359 D | op-mon: mons were added or removed from the endpoints cm
2023-02-17 00:46:50.886387 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2023-02-17 00:46:50.886447 D | op-mon: mons were added or removed from the endpoints cm
2023-02-17 00:46:50.886484 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2023-02-17 00:46:50.886607 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-mon-endpoints" cm is handled by another watcher
2023-02-17 00:46:50.886916 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.211.55.97:6789"],"namespace":""}] data:a=10.211.55.97:6789 mapping:{"node":{"a":{"Name":"node1","Hostname":"node1","Address":"10.211.55.97"}}} maxMonId:-1]
2023-02-17 00:46:51.085890 D | op-config: updating config secret "rook-ceph-config"
2023-02-17 00:46:51.286753 D | ceph-spec: object "rook-ceph-config" matched on update
2023-02-17 00:46:51.286781 D | ceph-spec: do not reconcile on "rook-ceph-config" secret changes
2023-02-17 00:46:51.485696 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2023-02-17 00:46:51.486118 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2023-02-17 00:46:51.486146 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2023-02-17 00:46:51.886568 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-csi-config" cm is handled by another watcher
2023-02-17 00:46:51.891516 I | op-mon: 0 of 1 expected mons are ready. creating or updating deployments without checking quorum in attempt to achieve a healthy mon cluster
2023-02-17 00:46:51.891571 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.211.55.97 Port:6789 Zone: DataPathMap:0xc00089cc60}
2023-02-17 00:46:51.895431 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2023-02-17 00:46:51.895460 D | op-mon: Starting mon: rook-ceph-mon-a
2023-02-17 00:46:51.927209 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2023-02-17 00:46:51.927231 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:46:51.960913 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2023-02-17 00:46:51.960953 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:46:51.986500 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2023-02-17 00:46:51.986516 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:46:52.087599 I | op-mon: updating maxMonID from -1 to 0 after committing mon "a"
2023-02-17 00:46:52.287218 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-mon-endpoints" cm is handled by another watcher
2023-02-17 00:46:52.287252 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2023-02-17 00:46:52.287304 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:46:52.688794 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2023-02-17 00:46:52.885721 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.211.55.97:6789"],"namespace":""}] data:a=10.211.55.97:6789 mapping:{"node":{"a":{"Name":"node1","Hostname":"node1","Address":"10.211.55.97"}}} maxMonId:0]
2023-02-17 00:46:52.885754 I | op-mon: waiting for mon quorum with [a]
2023-02-17 00:46:53.090684 I | op-mon: mon a is not yet running
2023-02-17 00:46:53.090702 I | op-mon: mons running: []
2023-02-17 00:46:53.090716 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:46:54.302419 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2023-02-17 00:46:54.305800 I | ceph-spec: parsing mon endpoints: a=10.211.55.97:6789
2023-02-17 00:46:54.305845 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0011967a0], assignment=&{Schedule:map[a:0xc000b142c0]}
2023-02-17 00:46:54.305864 I | op-k8sutil: ROOK_OBC_WATCH_OPERATOR_NAMESPACE="true" (configmap)
2023-02-17 00:46:54.305869 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "rook-ceph.ceph.rook.io/bucket"
2023-02-17 00:46:54.306126 I | op-bucket-prov: successfully reconciled bucket provisioner
I0217 00:46:54.306178       1 manager.go:135] objectbucket.io/provisioner-manager "msg"="starting provisioner"  "name"="rook-ceph.ceph.rook.io/bucket"
2023-02-17 00:46:54.670641 D | ceph-crashcollector-controller: "rook-ceph-mon-a-8ffc47794-75g5g" is a ceph pod!
2023-02-17 00:46:54.670735 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:46:54.670918 D | ceph-crashcollector-controller: secret "rook-ceph-crash-collector-keyring" not found. retrying in "30s". Secret "rook-ceph-crash-collector-keyring" not found
2023-02-17 00:46:55.435428 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2023-02-17 00:46:55.435475 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:46:55.471418 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2023-02-17 00:46:55.471451 D | ceph-spec: do not reconcile "rook-ceph-mon-a-canary" on monitor canary deployments
2023-02-17 00:46:55.471456 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2023-02-17 00:46:55.471465 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2023-02-17 00:46:55.471469 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2023-02-17 00:46:55.471471 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2023-02-17 00:46:57.690684 I | op-mon: Monitors in quorum: [a]
2023-02-17 00:46:57.690715 I | op-mon: mons created: 1
2023-02-17 00:46:57.690728 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:46:57.888807 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1}}
2023-02-17 00:46:57.888837 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1}}
2023-02-17 00:46:57.888868 I | op-mon: waiting for mon quorum with [a]
2023-02-17 00:46:57.899559 I | op-mon: mon a is not yet running
2023-02-17 00:46:57.899581 I | op-mon: mons running: []
2023-02-17 00:47:02.914597 I | op-mon: mons running: [a]
2023-02-17 00:47:02.914634 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:03.105596 I | op-mon: Monitors in quorum: [a]
2023-02-17 00:47:03.105624 I | op-config: setting "global"="mon allow pool delete"="true" option to the mon configuration database
2023-02-17 00:47:03.105644 D | exec: Running command: ceph config set global mon_allow_pool_delete true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:03.301626 I | op-config: successfully set "global"="mon allow pool delete"="true" option to the mon configuration database
2023-02-17 00:47:03.301655 I | op-config: setting "global"="mon cluster log file"="" option to the mon configuration database
2023-02-17 00:47:03.301697 D | exec: Running command: ceph config set global mon_cluster_log_file  --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:03.498491 I | op-config: successfully set "global"="mon cluster log file"="" option to the mon configuration database
2023-02-17 00:47:03.498522 I | op-config: setting "global"="mon allow pool size one"="true" option to the mon configuration database
2023-02-17 00:47:03.498537 D | exec: Running command: ceph config set global mon_allow_pool_size_one true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:03.694245 I | op-config: successfully set "global"="mon allow pool size one"="true" option to the mon configuration database
2023-02-17 00:47:03.694317 I | op-config: setting "global"="osd scrub auto repair"="true" option to the mon configuration database
2023-02-17 00:47:03.694337 D | exec: Running command: ceph config set global osd_scrub_auto_repair true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:03.887422 I | op-config: successfully set "global"="osd scrub auto repair"="true" option to the mon configuration database
2023-02-17 00:47:03.887454 I | op-config: setting "global"="log to file"="false" option to the mon configuration database
2023-02-17 00:47:03.887474 D | exec: Running command: ceph config set global log_to_file false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:04.089095 I | op-config: successfully set "global"="log to file"="false" option to the mon configuration database
2023-02-17 00:47:04.089125 I | op-config: deleting "log file" option from the mon configuration database
2023-02-17 00:47:04.089144 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:04.285602 I | op-config: successfully deleted "log file" option from the mon configuration database
2023-02-17 00:47:04.285634 D | op-mon: mon endpoints used are: a=10.211.55.97:6789
2023-02-17 00:47:04.285638 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2023-02-17 00:47:04.285641 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2023-02-17 00:47:04.285644 D | op-mon: Released lock for mon orchestration
2023-02-17 00:47:04.285648 D | ceph-cluster-controller: monitors are up and running, executing post actions
2023-02-17 00:47:04.285653 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2023-02-17 00:47:04.285663 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:04.487594 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2023-02-17 00:47:04.487628 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:04.690256 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2023-02-17 00:47:04.690306 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r mgr allow rw osd allow rw tag cephfs metadata=* --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:04.892863 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2023-02-17 00:47:04.892902 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:05.102778 D | op-cfg-keyring: creating secret for rook-csi-rbd-provisioner
2023-02-17 00:47:05.110064 D | op-cfg-keyring: creating secret for rook-csi-rbd-node
2023-02-17 00:47:05.117848 D | op-cfg-keyring: creating secret for rook-csi-cephfs-provisioner
2023-02-17 00:47:05.125596 D | op-cfg-keyring: creating secret for rook-csi-cephfs-node
2023-02-17 00:47:05.130794 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2023-02-17 00:47:05.130821 I | cephclient: getting or creating ceph auth key "client.crash"
2023-02-17 00:47:05.130836 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:05.337104 D | op-cfg-keyring: creating secret for rook-ceph-crash-collector-keyring
2023-02-17 00:47:05.341706 I | ceph-crashcollector-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2023-02-17 00:47:05.341736 I | op-config: deleting "mon_mds_skip_sanity" option from the mon configuration database
2023-02-17 00:47:05.341748 D | exec: Running command: ceph config rm mon mon_mds_skip_sanity --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:05.534919 I | op-config: successfully deleted "mon_mds_skip_sanity" option from the mon configuration database
2023-02-17 00:47:05.534945 I | ceph-cluster-controller: setting msgr2 encryption mode to "crc secure"
2023-02-17 00:47:05.534951 I | op-config: setting "global"="ms_cluster_mode"="crc secure" option to the mon configuration database
2023-02-17 00:47:05.534964 D | exec: Running command: ceph config set global ms_cluster_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:05.727086 I | op-config: successfully set "global"="ms_cluster_mode"="crc secure" option to the mon configuration database
2023-02-17 00:47:05.727135 I | op-config: setting "global"="ms_service_mode"="crc secure" option to the mon configuration database
2023-02-17 00:47:05.727152 D | exec: Running command: ceph config set global ms_service_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:05.923792 I | op-config: successfully set "global"="ms_service_mode"="crc secure" option to the mon configuration database
2023-02-17 00:47:05.923843 I | op-config: setting "global"="ms_client_mode"="crc secure" option to the mon configuration database
2023-02-17 00:47:05.923860 D | exec: Running command: ceph config set global ms_client_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:06.136401 I | op-config: successfully set "global"="ms_client_mode"="crc secure" option to the mon configuration database
2023-02-17 00:47:06.136431 W | ceph-cluster-controller: network compression requires Ceph Quincy (v17) or newer, skipping for current ceph "16.2.10-0 pacific"
2023-02-17 00:47:06.136438 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2023-02-17 00:47:06.136441 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2023-02-17 00:47:06.136453 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:06.335704 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2023-02-17 00:47:06.335862 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-rook-ceph" in namespace "rook-ceph"
2023-02-17 00:47:06.335867 D | op-k8sutil: creating secret cluster-peer-token-rook-ceph
2023-02-17 00:47:06.343209 D | op-k8sutil: created secret cluster-peer-token-rook-ceph
2023-02-17 00:47:06.343299 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mgr(s)"
2023-02-17 00:47:06.351426 I | op-mgr: start running mgr
2023-02-17 00:47:06.351482 I | cephclient: getting or creating ceph auth key "mgr.a"
2023-02-17 00:47:06.351493 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:06.351570 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:47:06.351582 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:47:06.563864 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2023-02-17 00:47:06.567656 D | op-cfg-keyring: creating secret for rook-ceph-mgr-a-keyring
2023-02-17 00:47:06.572543 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc0010cacc0}
2023-02-17 00:47:06.582090 I | op-config: setting "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2023-02-17 00:47:06.582128 D | exec: Running command: ceph config set mon auth_allow_insecure_global_id_reclaim false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:47:06.601228 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-17 00:47:06.601242 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:47:06.620985 D | ceph-crashcollector-controller: "rook-ceph-mgr-a-8759b445f-h2hhr" is a ceph pod!
2023-02-17 00:47:06.621022 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:47:06.621168 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:47:06.635730 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "created"
2023-02-17 00:47:06.636734 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-17 00:47:06.636759 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:47:06.637396 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:47:06.650860 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:47:06.650907 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:47:06.651068 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:47:06.664220 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:47:06.665357 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:47:06.671039 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-17 00:47:06.671059 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:47:06.677813 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:47:06.677913 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:47:06.678128 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:47:06.691312 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:47:06.695707 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:47:06.704785 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-17 00:47:06.704853 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:47:06.709244 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:47:06.710635 D | ceph-crashcollector-controller: "rook-ceph-crashcollector-node1-84c9c876cd-lj4jk" is a ceph pod!
2023-02-17 00:47:06.710675 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:47:06.710892 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:47:06.719104 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-17 00:47:06.719127 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:47:06.722366 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-node1": the object has been modified; please apply your changes to the latest version and try again
2023-02-17 00:47:06.722462 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:47:06.722618 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:47:06.733344 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:47:06.734328 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:47:06.740450 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:47:06.740564 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:47:06.740978 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:47:06.752866 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:47:06.754045 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:47:06.755315 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-17 00:47:06.755340 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:47:06.758260 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:47:06.758339 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:47:06.758491 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:47:06.766766 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:47:06.767656 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:47:06.770924 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:47:06.842188 I | op-config: successfully set "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2023-02-17 00:47:06.842214 I | op-config: insecure global ID is now disabled
2023-02-17 00:47:06.847845 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:09.858472 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:10.400863 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:47:10.401037 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:47:10.401348 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-17 00:47:10.401369 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:47:10.410662 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:47:10.411654 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:47:10.415299 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:47:12.866691 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:15.874544 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:18.884068 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:21.893244 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:24.902896 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:27.912500 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:30.921194 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:33.930754 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:36.938486 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:39.947464 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:42.958939 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:45.968020 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:48.978981 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:51.994050 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:55.003530 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:47:58.012308 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:48:01.020969 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:48:04.030654 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:48:07.038944 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:48:10.049430 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:47:06 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:48:12.679724 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-17 00:48:12.679805 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:13.057633 I | op-k8sutil: finished waiting for updated deployment "rook-ceph-mgr-a"
2023-02-17 00:48:13.062371 D | op-k8sutil: kubernetes version fetched 1.20.4-aliyun.1
2023-02-17 00:48:13.062414 I | op-mgr: setting services to point to mgr "a"
2023-02-17 00:48:13.062426 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2023-02-17 00:48:13.075791 D | op-k8sutil: created service rook-ceph-mgr-dashboard
2023-02-17 00:48:13.075846 D | op-k8sutil: creating service rook-ceph-mgr
2023-02-17 00:48:13.091239 D | op-k8sutil: created service rook-ceph-mgr
2023-02-17 00:48:13.098739 I | op-mgr: no need to update service "rook-ceph-mgr"
2023-02-17 00:48:13.098780 I | op-mgr: no need to update service "rook-ceph-mgr-dashboard"
2023-02-17 00:48:13.098853 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph OSDs"
2023-02-17 00:48:13.099795 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:13.100200 D | cephclient: balancer module is already 'on' on pacific, doing nothingbalancer
2023-02-17 00:48:13.100240 I | op-mgr: successful modules: balancer
2023-02-17 00:48:13.100247 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:13.100445 D | exec: Running command: ceph mgr module enable pg_autoscaler --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:13.113432 I | op-osd: start running osds in namespace "rook-ceph"
2023-02-17 00:48:13.113451 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2023-02-17 00:48:13.113712 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:48:13.113723 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:48:13.127054 D | op-osd: 0 of 0 OSD Deployments need updated
2023-02-17 00:48:13.127081 I | op-osd: start provisioning the OSDs on PVCs, if needed
2023-02-17 00:48:13.131920 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2023-02-17 00:48:13.131941 I | op-osd: start provisioning the OSDs on nodes, if needed
2023-02-17 00:48:13.137733 D | op-osd: storage nodes: [{Name:node1 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}} {Name:node2 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}} {Name:node3 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}}]
2023-02-17 00:48:13.146488 I | op-osd: 3 of the 3 storage nodes are valid
2023-02-17 00:48:13.173579 I | op-osd: started OSD provisioning job for node "node1"
2023-02-17 00:48:13.219718 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node1-kdbmm" is a ceph pod!
2023-02-17 00:48:13.219788 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:48:13.220116 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:13.224484 I | op-osd: started OSD provisioning job for node "node2"
2023-02-17 00:48:13.238769 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:48:13.240658 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:13.253091 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:13.277072 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node2-4sc47" is a ceph pod!
2023-02-17 00:48:13.277115 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:48:13.286195 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:13.288455 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:13.293252 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:13.483012 I | op-osd: started OSD provisioning job for node "node3"
2023-02-17 00:48:13.570979 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node3-l25sz" is a ceph pod!
2023-02-17 00:48:13.571052 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:48:13.572721 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:13.574361 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:13.584573 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:13.667636 I | op-osd: OSD orchestration status for node node1 is "starting"
2023-02-17 00:48:13.667673 I | op-osd: OSD orchestration status for node node2 is "starting"
2023-02-17 00:48:13.667680 I | op-osd: OSD orchestration status for node node3 is "starting"
2023-02-17 00:48:14.194135 D | ceph-spec: object "rook-ceph-osd-node1-status" matched on update
2023-02-17 00:48:14.194168 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:48:14.194174 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node1-status" cm is handled by another watcher
2023-02-17 00:48:14.194218 I | op-osd: OSD orchestration status for node node1 is "orchestrating"
2023-02-17 00:48:14.357577 D | ceph-spec: object "rook-ceph-osd-node2-status" matched on update
2023-02-17 00:48:14.357612 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:48:14.357622 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node2-status" cm is handled by another watcher
2023-02-17 00:48:14.357842 I | op-osd: OSD orchestration status for node node2 is "orchestrating"
2023-02-17 00:48:14.443245 I | op-mgr: successful modules: prometheus
2023-02-17 00:48:14.443549 I | op-config: setting "global"="osd_pool_default_pg_autoscale_mode"="on" option to the mon configuration database
2023-02-17 00:48:14.443624 D | exec: Running command: ceph config set global osd_pool_default_pg_autoscale_mode on --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:14.783410 I | op-config: successfully set "global"="osd_pool_default_pg_autoscale_mode"="on" option to the mon configuration database
2023-02-17 00:48:14.783430 I | op-config: setting "global"="mon_pg_warn_min_per_osd"="0" option to the mon configuration database
2023-02-17 00:48:14.783444 D | exec: Running command: ceph config set global mon_pg_warn_min_per_osd 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:14.953957 I | op-osd: OSD orchestration status for node node3 is "orchestrating"
2023-02-17 00:48:14.954170 D | ceph-spec: object "rook-ceph-osd-node3-status" matched on update
2023-02-17 00:48:14.954187 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:48:14.954197 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node3-status" cm is handled by another watcher
2023-02-17 00:48:15.191412 I | op-config: successfully set "global"="mon_pg_warn_min_per_osd"="0" option to the mon configuration database
2023-02-17 00:48:15.191440 I | op-mgr: successful modules: mgr module(s) from the spec
2023-02-17 00:48:15.762935 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2023-02-17 00:48:15.763004 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:19.455779 D | exec: Running command: ceph dashboard create-self-signed-cert --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:22.849754 I | op-mgr: setting ceph dashboard "admin" login creds
2023-02-17 00:48:22.849909 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/4253490316 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:23.486648 I | op-mgr: successfully set ceph dashboard creds
2023-02-17 00:48:23.486732 D | exec: Running command: ceph config get mgr.a mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:23.807565 D | exec: Running command: ceph config get mgr.a mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:23.850853 D | ceph-spec: object "rook-ceph-osd-node1-status" matched on update
2023-02-17 00:48:23.850875 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:48:23.850884 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node1-status" cm is handled by another watcher
2023-02-17 00:48:23.851353 I | op-osd: OSD orchestration status for node node1 is "completed"
2023-02-17 00:48:23.851427 I | op-osd: creating OSD 3 on node "node1"
2023-02-17 00:48:23.851769 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:48:23.851781 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:48:23.852056 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 3 on node \"node1\""
2023-02-17 00:48:23.871965 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:48:23.871998 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:48:23.883160 I | op-osd: creating OSD 0 on node "node1"
2023-02-17 00:48:23.883338 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:48:23.883347 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:48:23.883426 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 0 on node \"node1\""
2023-02-17 00:48:23.915610 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:48:23.915629 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:48:23.936808 D | ceph-spec: object "rook-ceph-osd-3" matched on update
2023-02-17 00:48:23.936828 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:23.958110 D | ceph-spec: object "rook-ceph-osd-node1-status" did not match on delete
2023-02-17 00:48:23.958142 D | ceph-spec: do not reconcile on "rook-ceph-osd-node1-status" config map changes
2023-02-17 00:48:23.958155 D | ceph-spec: object "rook-ceph-osd-node1-status" did not match on delete
2023-02-17 00:48:23.958163 D | ceph-spec: object "rook-ceph-osd-node1-status" did not match on delete
2023-02-17 00:48:23.958219 D | op-osd: not processing DELETED event for object "rook-ceph-osd-node1-status"
2023-02-17 00:48:23.986163 D | ceph-crashcollector-controller: "rook-ceph-osd-3-699fc69996-s76j8" is a ceph pod!
2023-02-17 00:48:23.986248 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:48:23.986934 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:23.987965 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2023-02-17 00:48:23.987986 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.005231 D | ceph-spec: object "rook-ceph-osd-3" matched on update
2023-02-17 00:48:24.005417 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.039809 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-17 00:48:24.039833 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.040088 D | ceph-crashcollector-controller: "rook-ceph-osd-0-c7bd8b985-bd7fq" is a ceph pod!
2023-02-17 00:48:24.040785 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:48:24.047147 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:24.064258 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:24.064351 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:48:24.064682 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.077853 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2023-02-17 00:48:24.077896 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.087808 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-17 00:48:24.087851 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.094900 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-node1": the object has been modified; please apply your changes to the latest version and try again
2023-02-17 00:48:24.094951 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:48:24.095159 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.111726 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:48:24.113397 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:24.120578 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2023-02-17 00:48:24.120595 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.127943 D | ceph-crashcollector-controller: "rook-ceph-crashcollector-node1-9b589d67b-gn5pl" is a ceph pod!
2023-02-17 00:48:24.128029 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:24.128052 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:48:24.129411 D | ceph-spec: object "rook-ceph-osd-3" matched on update
2023-02-17 00:48:24.129432 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.129703 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.138148 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-17 00:48:24.138181 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.152017 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-node1": the object has been modified; please apply your changes to the latest version and try again
2023-02-17 00:48:24.152075 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:48:24.152607 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.199957 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:48:24.202550 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:24.210629 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:24.210695 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:48:24.210785 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-17 00:48:24.210794 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.211020 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.222628 D | ceph-spec: object "rook-ceph-osd-node2-status" matched on update
2023-02-17 00:48:24.222662 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:48:24.222673 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node2-status" cm is handled by another watcher
2023-02-17 00:48:24.222810 I | op-osd: OSD orchestration status for node node2 is "completed"
2023-02-17 00:48:24.222825 I | op-osd: creating OSD 1 on node "node2"
2023-02-17 00:48:24.223007 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:48:24.223019 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:48:24.224776 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"node2\""
2023-02-17 00:48:24.225749 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:48:24.235019 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:24.243608 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:24.243652 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:48:24.243982 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.245474 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:48:24.245493 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:48:24.256608 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:48:24.258530 I | op-osd: creating OSD 4 on node "node2"
2023-02-17 00:48:24.258749 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:48:24.258760 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:48:24.258836 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 4 on node \"node2\""
2023-02-17 00:48:24.261345 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:24.274494 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:24.287359 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:48:24.287379 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:48:24.297741 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2023-02-17 00:48:24.297790 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.338253 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2023-02-17 00:48:24.338296 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.351739 D | ceph-spec: object "rook-ceph-osd-4" matched on update
2023-02-17 00:48:24.351816 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.352203 D | op-osd: not processing DELETED event for object "rook-ceph-osd-node2-status"
2023-02-17 00:48:24.352312 D | ceph-spec: object "rook-ceph-osd-node2-status" did not match on delete
2023-02-17 00:48:24.352337 D | ceph-spec: do not reconcile on "rook-ceph-osd-node2-status" config map changes
2023-02-17 00:48:24.352378 D | ceph-spec: object "rook-ceph-osd-node2-status" did not match on delete
2023-02-17 00:48:24.352387 D | ceph-spec: object "rook-ceph-osd-node2-status" did not match on delete
2023-02-17 00:48:24.359912 D | exec: Running command: ceph config get mgr.a mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:24.383728 D | ceph-crashcollector-controller: "rook-ceph-osd-1-6b676cfb6f-w87jt" is a ceph pod!
2023-02-17 00:48:24.383864 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:48:24.384249 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.430806 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "created"
2023-02-17 00:48:24.435426 D | ceph-crashcollector-controller: "rook-ceph-osd-4-66d7545fb6-xnlkm" is a ceph pod!
2023-02-17 00:48:24.450684 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:24.451677 D | ceph-spec: object "rook-ceph-osd-4" matched on update
2023-02-17 00:48:24.451715 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.516330 D | ceph-spec: object "rook-ceph-crashcollector-node2" matched on update
2023-02-17 00:48:24.516353 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.518156 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2023-02-17 00:48:24.518179 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.522758 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:24.522808 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:48:24.523220 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.561192 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-17 00:48:24.568478 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:24.576127 D | ceph-crashcollector-controller: "rook-ceph-crashcollector-node2-58d5b5df77-lg6m6" is a ceph pod!
2023-02-17 00:48:24.586220 D | ceph-spec: object "rook-ceph-crashcollector-node2" matched on update
2023-02-17 00:48:24.586244 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.587992 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:24.588028 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:48:24.588743 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.620484 D | ceph-spec: object "rook-ceph-osd-4" matched on update
2023-02-17 00:48:24.620504 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.622855 I | op-osd: OSD orchestration status for node node3 is "completed"
2023-02-17 00:48:24.622891 I | op-osd: creating OSD 2 on node "node3"
2023-02-17 00:48:24.623015 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:48:24.623024 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:48:24.623084 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 2 on node \"node3\""
2023-02-17 00:48:24.624667 D | ceph-spec: object "rook-ceph-crashcollector-node2" matched on update
2023-02-17 00:48:24.624686 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node3-status" cm is handled by another watcher
2023-02-17 00:48:24.624696 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.624700 D | ceph-spec: object "rook-ceph-osd-node3-status" matched on update
2023-02-17 00:48:24.624703 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:48:24.628339 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-node2": the object has been modified; please apply your changes to the latest version and try again
2023-02-17 00:48:24.628388 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:48:24.628667 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.646576 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-17 00:48:24.646719 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:48:24.646728 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:48:24.647764 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:24.661942 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:24.661995 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:48:24.662495 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.665642 I | op-osd: creating OSD 5 on node "node3"
2023-02-17 00:48:24.665767 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:48:24.665775 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:48:24.665825 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 5 on node \"node3\""
2023-02-17 00:48:24.675894 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-17 00:48:24.680795 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:48:24.680813 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:48:24.680910 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:24.695071 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:24.707055 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2023-02-17 00:48:24.707104 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.726464 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-17 00:48:24.726511 D | ceph-spec: do not reconcile on "rook-ceph-osd-node3-status" config map changes
2023-02-17 00:48:24.726550 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-17 00:48:24.726560 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-17 00:48:24.743259 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:24.743725 D | ceph-spec: object "rook-ceph-osd-5" matched on update
2023-02-17 00:48:24.743738 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.768885 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2023-02-17 00:48:24.768925 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.800866 D | ceph-crashcollector-controller: "rook-ceph-osd-2-65f5f4657d-wddpm" is a ceph pod!
2023-02-17 00:48:24.800923 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:48:24.801607 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.825355 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "created"
2023-02-17 00:48:24.827128 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:24.833127 D | ceph-crashcollector-controller: "rook-ceph-osd-5-6f89876499-k8tsb" is a ceph pod!
2023-02-17 00:48:24.844650 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:24.844738 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:48:24.845745 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:24.848539 D | ceph-spec: object "rook-ceph-osd-5" matched on update
2023-02-17 00:48:24.848558 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.871538 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:48:24.874937 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:24.898137 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:24.947144 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2023-02-17 00:48:24.947168 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:24.977456 D | ceph-crashcollector-controller: "rook-ceph-crashcollector-node3-6d569747bb-bdfv9" is a ceph pod!
2023-02-17 00:48:24.977521 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:48:24.978023 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:25.049498 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:48:25.055134 D | ceph-spec: object "rook-ceph-crashcollector-node3" matched on update
2023-02-17 00:48:25.055168 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:25.060986 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:25.069054 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:25.069210 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:48:25.070498 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:25.081649 I | op-config: setting "mgr.a"="mgr/dashboard/server_port"="8443" option to the mon configuration database
2023-02-17 00:48:25.081702 D | exec: Running command: ceph config set mgr.a mgr/dashboard/server_port 8443 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:25.082786 D | ceph-spec: object "rook-ceph-osd-5" matched on update
2023-02-17 00:48:25.082800 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:25.092078 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:48:25.093919 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:25.104970 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:25.160399 D | ceph-spec: object "rook-ceph-crashcollector-node3" matched on update
2023-02-17 00:48:25.160443 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:25.160486 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:48:25.160857 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:25.174465 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:48:25.176360 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:25.182859 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:25.629596 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2}}
2023-02-17 00:48:25.629617 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2}}
2023-02-17 00:48:25.629645 I | op-osd: finished running OSDs in namespace "rook-ceph"
2023-02-17 00:48:25.629650 I | ceph-cluster-controller: done reconciling ceph cluster in namespace "rook-ceph"
2023-02-17 00:48:25.629684 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:48:25.648990 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "rook-ceph"
2023-02-17 00:48:25.649014 I | op-osd: ceph osd status in namespace "rook-ceph" check interval "1m0s"
2023-02-17 00:48:25.649019 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "rook-ceph"
2023-02-17 00:48:25.649025 I | ceph-cluster-controller: ceph status check interval is 1m0s
2023-02-17 00:48:25.649029 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "rook-ceph"
2023-02-17 00:48:25.649049 D | ceph-cluster-controller: successfully configured CephCluster "rook-ceph/rook-ceph"
2023-02-17 00:48:25.649162 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:48:25.649184 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:48:25.649205 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:48:25.649817 I | ceph-cluster-controller: reporting cluster telemetry
2023-02-17 00:48:25.649838 D | op-config: setting "rook/version"="v1.9.9" option in the mon config-key store
2023-02-17 00:48:25.649852 D | exec: Running command: ceph config-key set rook/version v1.9.9 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:25.652119 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:48:25.652137 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:48:25.730491 I | op-config: successfully set "mgr.a"="mgr/dashboard/server_port"="8443" option to the mon configuration database
2023-02-17 00:48:25.730515 D | exec: Running command: ceph config get mgr.a mgr/dashboard/ssl_server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:26.437371 D | telemetry: set telemetry key: rook/version=v1.9.9
2023-02-17 00:48:26.440520 D | op-config: setting "rook/kubernetes/version"="v1.20.4-aliyun.1" option in the mon config-key store
2023-02-17 00:48:26.440547 D | exec: Running command: ceph config-key set rook/kubernetes/version v1.20.4-aliyun.1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:26.589586 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_WARN Checks:map[TOO_FEW_OSDS:{Severity:HEALTH_WARN Summary:{Message:OSD count 0 < osd_pool_default_size 3}}]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:0 AvailableBytes:0 TotalBytes:0 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:48:26.656440 I | op-mgr: dashboard config has changed. restarting the dashboard module
2023-02-17 00:48:26.656465 I | op-mgr: restarting the mgr module
2023-02-17 00:48:26.656529 D | exec: Running command: ceph mgr module disable dashboard --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:26.779946 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:27.352508 D | telemetry: set telemetry key: rook/kubernetes/version=v1.20.4-aliyun.1
2023-02-17 00:48:27.352528 D | op-config: setting "rook/csi/version"="v3.6.2" option in the mon config-key store
2023-02-17 00:48:27.352540 D | exec: Running command: ceph config-key set rook/csi/version v3.6.2 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:27.451321 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:27.506433 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2}}
2023-02-17 00:48:27.506453 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2}}
2023-02-17 00:48:27.506533 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_WARN Checks:map[TOO_FEW_OSDS:{Severity:HEALTH_WARN Summary:{Message:OSD count 0 < osd_pool_default_size 3}}]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:0 AvailableBytes:0 TotalBytes:0 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:48:27.506547 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:48:27.530126 D | ceph-cluster-controller: checking for stuck pods on not ready nodes
2023-02-17 00:48:27.531303 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:48:27.531320 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:48:27.541016 D | ceph-cluster-controller: Health: "HEALTH_WARN", code: "TOO_FEW_OSDS", message: "OSD count 0 < osd_pool_default_size 3"
2023-02-17 00:48:27.990643 D | telemetry: set telemetry key: rook/csi/version=v3.6.2
2023-02-17 00:48:27.990674 D | op-config: setting "rook/cluster/mon/max-id"="0" option in the mon config-key store
2023-02-17 00:48:27.990692 D | exec: Running command: ceph config-key set rook/cluster/mon/max-id 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:28.234387 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:48:28.234451 D | ceph-spec: object "rook-ceph-crashcollector-node3" matched on update
2023-02-17 00:48:28.234462 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:28.234663 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:28.248212 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:48:28.249638 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:28.253035 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:28.430822 D | telemetry: set telemetry key: rook/cluster/mon/max-id=0
2023-02-17 00:48:28.430858 D | op-config: setting "rook/cluster/mon/count"="1" option in the mon config-key store
2023-02-17 00:48:28.430873 D | exec: Running command: ceph config-key set rook/cluster/mon/count 1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:28.608517 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:48:28.608970 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:28.610372 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-17 00:48:28.610423 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:28.621448 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:48:28.623517 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:28.637913 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:28.688618 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:48:28.688724 D | ceph-spec: object "rook-ceph-crashcollector-node1" matched on update
2023-02-17 00:48:28.688736 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:28.689077 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:28.701745 D | ceph-spec: object "rook-ceph-crashcollector-node2" matched on update
2023-02-17 00:48:28.701774 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:48:28.702345 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:48:28.705368 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:28.710034 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:28.710071 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:48:28.710730 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:28.730648 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-17 00:48:28.732470 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:28.736936 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:28.746717 I | op-mgr: successful modules: dashboard
2023-02-17 00:48:28.946771 D | telemetry: set telemetry key: rook/cluster/mon/count=1
2023-02-17 00:48:28.946802 D | op-config: setting "rook/cluster/mon/allow-multiple-per-node"="false" option in the mon config-key store
2023-02-17 00:48:28.946818 D | exec: Running command: ceph config-key set rook/cluster/mon/allow-multiple-per-node false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:29.421121 D | telemetry: set telemetry key: rook/cluster/mon/allow-multiple-per-node=false
2023-02-17 00:48:29.421153 D | op-config: setting "rook/cluster/mon/pvc/enabled"="false" option in the mon config-key store
2023-02-17 00:48:29.421170 D | exec: Running command: ceph config-key set rook/cluster/mon/pvc/enabled false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:29.906840 D | telemetry: set telemetry key: rook/cluster/mon/pvc/enabled=false
2023-02-17 00:48:29.906893 D | op-config: setting "rook/cluster/mon/stretch/enabled"="false" option in the mon config-key store
2023-02-17 00:48:29.906908 D | exec: Running command: ceph config-key set rook/cluster/mon/stretch/enabled false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:30.333994 D | telemetry: set telemetry key: rook/cluster/mon/stretch/enabled=false
2023-02-17 00:48:30.334015 D | op-config: setting "rook/cluster/storage/device-set/count/total"="0" option in the mon config-key store
2023-02-17 00:48:30.334029 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/total 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:30.701876 D | ceph-crashcollector-controller: "rook-ceph-crashcollector-node1-84c9c876cd-lj4jk" is a ceph pod!
2023-02-17 00:48:30.701962 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:48:30.702429 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:48:30.712730 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:48:30.714046 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:48:30.718499 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:48:30.777336 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/total=0
2023-02-17 00:48:30.777354 D | op-config: setting "rook/cluster/storage/device-set/count/portable"="0" option in the mon config-key store
2023-02-17 00:48:30.777365 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/portable 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:31.180295 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/portable=0
2023-02-17 00:48:31.180325 D | op-config: setting "rook/cluster/storage/device-set/count/non-portable"="0" option in the mon config-key store
2023-02-17 00:48:31.180339 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/non-portable 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:31.609439 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/non-portable=0
2023-02-17 00:48:31.609471 D | op-config: setting "rook/cluster/network/provider"="host" option in the mon config-key store
2023-02-17 00:48:31.609485 D | exec: Running command: ceph config-key set rook/cluster/network/provider host --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:32.015940 D | telemetry: set telemetry key: rook/cluster/network/provider=host
2023-02-17 00:48:32.015959 D | op-config: setting "rook/cluster/external-mode"="false" option in the mon config-key store
2023-02-17 00:48:32.015971 D | exec: Running command: ceph config-key set rook/cluster/external-mode false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:48:32.452658 D | telemetry: set telemetry key: rook/cluster/external-mode=false
2023-02-17 00:49:10.649513 D | op-mon: checking health of mons
2023-02-17 00:49:10.649543 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:49:10.649591 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:49:10.654966 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 00:49:10.655001 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:49:11.010335 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}}]}}
2023-02-17 00:49:11.010363 D | op-mon: targeting the mon count 1
2023-02-17 00:49:11.010371 D | op-mon: mon "a" found in quorum
2023-02-17 00:49:11.010373 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 00:49:11.019556 I | op-mon: checking if multiple mons are on the same node
2023-02-17 00:49:11.027505 D | op-mon: analyzing mon pod "rook-ceph-mon-a-8ffc47794-75g5g" on node "node1"
2023-02-17 00:49:11.027534 D | op-mon: Released lock for mon orchestration
2023-02-17 00:49:11.027542 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:49:25.649536 D | op-osd: checking osd processes status.
2023-02-17 00:49:25.649584 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:49:25.969899 D | op-osd: validating status of osd.0
2023-02-17 00:49:25.969933 D | op-osd: osd.0 is healthy.
2023-02-17 00:49:25.969939 D | op-osd: validating status of osd.1
2023-02-17 00:49:25.969942 D | op-osd: osd.1 is healthy.
2023-02-17 00:49:25.969944 D | op-osd: validating status of osd.2
2023-02-17 00:49:25.969947 D | op-osd: osd.2 is healthy.
2023-02-17 00:49:25.969949 D | op-osd: validating status of osd.3
2023-02-17 00:49:25.969951 D | op-osd: osd.3 is healthy.
2023-02-17 00:49:25.969954 D | op-osd: validating status of osd.4
2023-02-17 00:49:25.969956 D | op-osd: osd.4 is healthy.
2023-02-17 00:49:25.969958 D | op-osd: validating status of osd.5
2023-02-17 00:49:25.969960 D | op-osd: osd.5 is healthy.
2023-02-17 00:49:25.969971 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:49:26.315126 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:49:26.315153 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:49:27.542062 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:49:27.542115 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:49:27.937970 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:29933568 AvailableBytes:1649237508096 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:49:27.948168 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:49:28.331606 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:49:28.331638 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:49:28.331700 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:29933568 AvailableBytes:1649237508096 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:49:28.331719 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:49:28.342149 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:49:28.342179 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:49:39.539763 D | ceph-spec: object "rook-ceph-osd-2" matched on update
2023-02-17 00:49:39.539795 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:49:40.507156 D | ceph-spec: object "rook-ceph-osd-5" matched on update
2023-02-17 00:49:40.507184 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:49:45.787492 D | ceph-spec: object "rook-ceph-osd-1" matched on update
2023-02-17 00:49:45.787521 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:49:51.698773 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2023-02-17 00:49:51.698788 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:49:51.781600 D | ceph-spec: object "rook-ceph-osd-4" matched on update
2023-02-17 00:49:51.781633 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:49:53.677818 D | ceph-spec: object "rook-ceph-osd-3" matched on update
2023-02-17 00:49:53.677847 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:49:56.028468 D | op-mon: checking health of mons
2023-02-17 00:49:56.028504 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:49:56.028510 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:49:56.037535 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 00:49:56.037571 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:49:56.391922 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}}]}}
2023-02-17 00:49:56.391952 D | op-mon: targeting the mon count 1
2023-02-17 00:49:56.391961 D | op-mon: mon "a" found in quorum
2023-02-17 00:49:56.391963 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 00:49:56.398499 D | op-mon: Released lock for mon orchestration
2023-02-17 00:49:56.398533 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:50:26.315454 D | op-osd: checking osd processes status.
2023-02-17 00:50:26.315500 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:50:26.621625 D | op-osd: validating status of osd.0
2023-02-17 00:50:26.621653 D | op-osd: osd.0 is healthy.
2023-02-17 00:50:26.621659 D | op-osd: validating status of osd.1
2023-02-17 00:50:26.621661 D | op-osd: osd.1 is healthy.
2023-02-17 00:50:26.621665 D | op-osd: validating status of osd.2
2023-02-17 00:50:26.621669 D | op-osd: osd.2 is healthy.
2023-02-17 00:50:26.621671 D | op-osd: validating status of osd.3
2023-02-17 00:50:26.621673 D | op-osd: osd.3 is healthy.
2023-02-17 00:50:26.621674 D | op-osd: validating status of osd.4
2023-02-17 00:50:26.621677 D | op-osd: osd.4 is healthy.
2023-02-17 00:50:26.621679 D | op-osd: validating status of osd.5
2023-02-17 00:50:26.621681 D | op-osd: osd.5 is healthy.
2023-02-17 00:50:26.621694 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:50:28.341965 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:50:28.342000 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:50:28.693657 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:29933568 AvailableBytes:1649237508096 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:50:28.700486 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:50:29.057542 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:50:29.057561 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:50:29.057630 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:29933568 AvailableBytes:1649237508096 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:50:29.057638 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:50:29.067788 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:50:29.067803 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:50:41.399511 D | op-mon: checking health of mons
2023-02-17 00:50:41.399536 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:50:41.399539 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:50:41.417256 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 00:50:41.417304 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:50:41.766474 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}}]}}
2023-02-17 00:50:41.766490 D | op-mon: targeting the mon count 1
2023-02-17 00:50:41.766496 D | op-mon: mon "a" found in quorum
2023-02-17 00:50:41.766498 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 00:50:41.775015 D | op-mon: Released lock for mon orchestration
2023-02-17 00:50:41.775050 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:51:26.775176 D | op-mon: checking health of mons
2023-02-17 00:51:26.775222 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:51:26.775228 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:51:26.781725 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 00:51:26.781749 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:51:26.938158 D | op-osd: checking osd processes status.
2023-02-17 00:51:26.938193 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:51:27.161900 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}}]}}
2023-02-17 00:51:27.162088 D | op-mon: targeting the mon count 1
2023-02-17 00:51:27.162102 D | op-mon: mon "a" found in quorum
2023-02-17 00:51:27.162107 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 00:51:27.171873 D | op-mon: Released lock for mon orchestration
2023-02-17 00:51:27.171907 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:51:27.247756 D | op-osd: validating status of osd.0
2023-02-17 00:51:27.247783 D | op-osd: osd.0 is healthy.
2023-02-17 00:51:27.247789 D | op-osd: validating status of osd.1
2023-02-17 00:51:27.247793 D | op-osd: osd.1 is healthy.
2023-02-17 00:51:27.247795 D | op-osd: validating status of osd.2
2023-02-17 00:51:27.247797 D | op-osd: osd.2 is healthy.
2023-02-17 00:51:27.247799 D | op-osd: validating status of osd.3
2023-02-17 00:51:27.247800 D | op-osd: osd.3 is healthy.
2023-02-17 00:51:27.247802 D | op-osd: validating status of osd.4
2023-02-17 00:51:27.247805 D | op-osd: osd.4 is healthy.
2023-02-17 00:51:27.247807 D | op-osd: validating status of osd.5
2023-02-17 00:51:27.247809 D | op-osd: osd.5 is healthy.
2023-02-17 00:51:27.247819 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:51:29.068449 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:51:29.068491 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:51:29.432839 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:29933568 AvailableBytes:1649237508096 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:51:29.440176 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:51:29.794766 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:51:29.794795 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:51:29.794858 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:29933568 AvailableBytes:1649237508096 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:51:29.794869 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:51:29.805376 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:51:29.805392 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:52:12.172604 D | op-mon: checking health of mons
2023-02-17 00:52:12.172637 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:52:12.172642 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:52:12.187444 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 00:52:12.187480 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:12.535663 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}}]}}
2023-02-17 00:52:12.535689 D | op-mon: targeting the mon count 1
2023-02-17 00:52:12.535697 D | op-mon: mon "a" found in quorum
2023-02-17 00:52:12.535699 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 00:52:12.540925 D | op-mon: Released lock for mon orchestration
2023-02-17 00:52:12.540956 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:52:26.249863 D | clusterdisruption-controller: reconciling "rook-ceph/rook-ceph"
2023-02-17 00:52:26.249941 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:52:26.249947 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:52:26.250240 I | ceph-cluster-controller: CR has changed for "rook-ceph". diff=v1.ClusterSpec{
	... // 11 identical fields
	WaitTimeoutForHealthyOSDInMinutes: s"10ns",
	DisruptionManagement:              {ManagePodBudgets: true, OSDMaintenanceTimeout: s"30ns", MachineDisruptionBudgetNamespace: "openshift-machine-api"},
	Mon: v1.MonSpec{
-		Count:                1,
+		Count:                2,
		AllowMultiplePerNode: false,
		StretchCluster:       nil,
		VolumeClaimTemplate:  nil,
	},
	CrashCollector: {},
	Dashboard:      {Enabled: true, SSL: true},
	Monitoring:     {},
	External:       {},
	Mgr: v1.MgrSpec{
-		Count:                1,
+		Count:                2,
		AllowMultiplePerNode: false,
		Modules:              {{Name: "pg_autoscaler", Enabled: true}},
	},
	RemoveOSDsIfOutAndSafeToRemove: false,
	CleanupPolicy:                  {SanitizeDisks: {Method: "quick", DataSource: "zero", Iteration: 1}},
	... // 3 identical fields
}
2023-02-17 00:52:26.250394 I | operator: reloading operator's CRDs manager, cancelling all orchestrations!
I0217 00:52:26.250447       1 manager.go:148] objectbucket.io/provisioner-manager "msg"="stopping provisioner"  "name"="rook-ceph.ceph.rook.io/bucket" "reason"="context canceled"
2023-02-17 00:52:26.250466 I | op-mon: stopping monitoring of mons in namespace "rook-ceph"
2023-02-17 00:52:26.250578 I | op-osd: stopping monitoring of OSDs in namespace "rook-ceph"
2023-02-17 00:52:26.250643 I | ceph-cluster-controller: stopping monitoring of ceph status
2023-02-17 00:52:26.250929 I | operator: successfully started the controller-runtime manager
2023-02-17 00:52:26.255570 I | op-k8sutil: ROOK_CURRENT_NAMESPACE_ONLY="true" (env var)
2023-02-17 00:52:26.255599 I | operator: watching the current namespace "rook-ceph" for a Ceph CRs
2023-02-17 00:52:26.255629 I | operator: setting up schemes
2023-02-17 00:52:26.256817 I | operator: setting up the controller-runtime manager
2023-02-17 00:52:27.009809 I | operator: Fetching webhook cert-manager-webhook to see if cert-manager is installed.
2023-02-17 00:52:27.012784 I | operator: failed to get cert manager
2023-02-17 00:52:27.012821 I | ceph-cluster-controller: successfully started
2023-02-17 00:52:27.012852 I | ceph-cluster-controller: enabling hotplug orchestration
2023-02-17 00:52:27.012861 I | ceph-crashcollector-controller: successfully started
2023-02-17 00:52:27.012863 D | ceph-crashcollector-controller: watch for changes to the nodes
2023-02-17 00:52:27.012868 D | ceph-crashcollector-controller: watch for changes to the ceph-crash deployments
2023-02-17 00:52:27.012872 D | ceph-crashcollector-controller: watch for changes to the ceph pod nodename and enqueue their nodes
2023-02-17 00:52:27.012880 I | ceph-block-pool-controller: successfully started
2023-02-17 00:52:27.012889 I | ceph-object-store-user-controller: successfully started
2023-02-17 00:52:27.012897 I | ceph-object-realm-controller: successfully started
2023-02-17 00:52:27.012922 I | ceph-object-zonegroup-controller: successfully started
2023-02-17 00:52:27.012930 I | ceph-object-zone-controller: successfully started
2023-02-17 00:52:27.013009 I | ceph-object-controller: successfully started
2023-02-17 00:52:27.013041 I | ceph-file-controller: successfully started
2023-02-17 00:52:27.013056 I | ceph-nfs-controller: successfully started
2023-02-17 00:52:27.013066 I | ceph-rbd-mirror-controller: successfully started
2023-02-17 00:52:27.013076 I | ceph-client-controller: successfully started
2023-02-17 00:52:27.013082 I | ceph-filesystem-mirror-controller: successfully started
2023-02-17 00:52:27.013094 I | operator: rook-ceph-operator-config-controller successfully started
2023-02-17 00:52:27.013101 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2023-02-17 00:52:27.013121 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2023-02-17 00:52:27.013129 I | ceph-bucket-topic: successfully started
2023-02-17 00:52:27.013134 I | ceph-bucket-notification: successfully started
2023-02-17 00:52:27.013139 I | ceph-bucket-notification: successfully started
2023-02-17 00:52:27.013144 I | ceph-fs-subvolumegroup-controller: successfully started
2023-02-17 00:52:27.013148 I | blockpool-rados-namespace-controller: successfully started
2023-02-17 00:52:27.014024 D | op-k8sutil: kubernetes version fetched 1.20.4-aliyun.1
2023-02-17 00:52:27.014059 I | operator: starting the controller-runtime manager
2023-02-17 00:52:27.115016 D | clusterdisruption-controller: create event from ceph cluster CR
2023-02-17 00:52:27.116497 D | ceph-crashcollector-controller: "rook-ceph-osd-2-65f5f4657d-wddpm" is a ceph pod!
2023-02-17 00:52:27.116527 D | ceph-crashcollector-controller: "rook-ceph-osd-3-699fc69996-s76j8" is a ceph pod!
2023-02-17 00:52:27.116534 D | ceph-crashcollector-controller: "rook-ceph-osd-0-c7bd8b985-bd7fq" is a ceph pod!
2023-02-17 00:52:27.116540 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node1-kdbmm" is a ceph pod!
2023-02-17 00:52:27.116545 D | ceph-crashcollector-controller: "rook-ceph-osd-1-6b676cfb6f-w87jt" is a ceph pod!
2023-02-17 00:52:27.116550 D | ceph-crashcollector-controller: "rook-ceph-mgr-a-8759b445f-h2hhr" is a ceph pod!
2023-02-17 00:52:27.116554 D | ceph-crashcollector-controller: "rook-ceph-crashcollector-node2-58d5b5df77-lg6m6" is a ceph pod!
2023-02-17 00:52:27.116557 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node2-4sc47" is a ceph pod!
2023-02-17 00:52:27.116563 D | ceph-crashcollector-controller: "rook-ceph-crashcollector-node1-9b589d67b-gn5pl" is a ceph pod!
2023-02-17 00:52:27.116566 D | ceph-crashcollector-controller: "rook-ceph-osd-5-6f89876499-k8tsb" is a ceph pod!
2023-02-17 00:52:27.116570 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node3-l25sz" is a ceph pod!
2023-02-17 00:52:27.116577 D | ceph-crashcollector-controller: "rook-ceph-crashcollector-node3-6d569747bb-bdfv9" is a ceph pod!
2023-02-17 00:52:27.116581 D | ceph-crashcollector-controller: "rook-ceph-mon-a-8ffc47794-75g5g" is a ceph pod!
2023-02-17 00:52:27.116584 D | ceph-crashcollector-controller: "rook-ceph-osd-4-66d7545fb6-xnlkm" is a ceph pod!
2023-02-17 00:52:27.116624 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:52:27.116660 D | operator: reconciling rook-ceph/rook-ceph-operator-config
2023-02-17 00:52:27.116683 I | op-k8sutil: ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS="15" (configmap)
2023-02-17 00:52:27.116699 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:52:27.116810 I | op-k8sutil: ROOK_LOG_LEVEL="DEBUG" (configmap)
2023-02-17 00:52:27.116825 I | op-k8sutil: ROOK_ENABLE_DISCOVERY_DAEMON="false" (configmap)
2023-02-17 00:52:27.116944 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:52:27.117020 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:52:27.117110 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:52:27.117118 D | ceph-cluster-controller: create event from a CR
2023-02-17 00:52:27.117152 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2023-02-17 00:52:27.117436 D | clusterdisruption-controller: reconciling "rook-ceph/rook-ceph"
2023-02-17 00:52:27.118168 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (default)
2023-02-17 00:52:27.118185 D | ceph-csi: not a multus cluster "rook-ceph/rook-ceph-operator-config" or CSI_ENABLE_HOST_NETWORK is true, not deploying the ceph-csi plugin holder
2023-02-17 00:52:27.120161 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2023-02-17 00:52:27.121472 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2023-02-17 00:52:27.122483 I | operator: rook-ceph-operator-config-controller done reconciling
2023-02-17 00:52:27.125661 I | ceph-spec: parsing mon endpoints: a=10.211.55.97:6789
2023-02-17 00:52:27.125704 I | ceph-spec: parsing mon endpoints: a=10.211.55.97:6789
2023-02-17 00:52:27.125722 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc000cbe3e0], assignment=&{Schedule:map[a:0xc00073bb40]}
2023-02-17 00:52:27.125732 I | op-k8sutil: ROOK_OBC_WATCH_OPERATOR_NAMESPACE="true" (configmap)
2023-02-17 00:52:27.125737 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "rook-ceph.ceph.rook.io/bucket"
2023-02-17 00:52:27.125887 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc000059bc0], assignment=&{Schedule:map[a:0xc000752b40]}
2023-02-17 00:52:27.125983 I | op-bucket-prov: successfully reconciled bucket provisioner
I0217 00:52:27.126021       1 manager.go:135] objectbucket.io/provisioner-manager "msg"="starting provisioner"  "name"="rook-ceph.ceph.rook.io/bucket"
2023-02-17 00:52:27.128895 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:52:27.130126 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:52:27.134571 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:52:27.134610 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:52:27.134953 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:52:27.136416 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "rook-ceph"
2023-02-17 00:52:27.136449 I | op-osd: ceph osd status in namespace "rook-ceph" check interval "1m0s"
2023-02-17 00:52:27.136455 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "rook-ceph"
2023-02-17 00:52:27.136466 I | ceph-cluster-controller: ceph status check interval is 1m0s
2023-02-17 00:52:27.136468 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "rook-ceph"
2023-02-17 00:52:27.136518 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:52:27.136542 I | op-mon: stopping monitoring of mons in namespace "rook-ceph"
2023-02-17 00:52:27.136559 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:52:27.136583 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:52:27.139845 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
2023-02-17 00:52:27.143994 D | ceph-cluster-controller: cluster spec successfully validated
2023-02-17 00:52:27.144079 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Detecting Ceph version"
2023-02-17 00:52:27.145975 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:52:27.147117 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:52:27.149060 I | op-k8sutil: ROOK_CSI_ENABLE_RBD="true" (configmap)
2023-02-17 00:52:27.149094 I | op-k8sutil: ROOK_CSI_ENABLE_CEPHFS="true" (configmap)
2023-02-17 00:52:27.149103 I | op-k8sutil: ROOK_CSI_ENABLE_NFS="false" (configmap)
2023-02-17 00:52:27.149108 I | op-k8sutil: ROOK_CSI_ALLOW_UNSUPPORTED_VERSION="false" (configmap)
2023-02-17 00:52:27.149111 I | op-k8sutil: ROOK_CSI_ENABLE_GRPC_METRICS="false" (configmap)
2023-02-17 00:52:27.149116 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (default)
2023-02-17 00:52:27.149120 I | op-k8sutil: CSI_FORCE_CEPHFS_KERNEL_CLIENT="true" (configmap)
2023-02-17 00:52:27.149123 I | op-k8sutil: CSI_GRPC_TIMEOUT_SECONDS="150" (configmap)
2023-02-17 00:52:27.149127 I | op-k8sutil: CSI_CEPHFS_GRPC_METRICS_PORT="9091" (default)
2023-02-17 00:52:27.149131 I | op-k8sutil: CSI_CEPHFS_GRPC_METRICS_PORT="9091" (default)
2023-02-17 00:52:27.149134 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2023-02-17 00:52:27.149138 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2023-02-17 00:52:27.149141 I | op-k8sutil: CSI_RBD_GRPC_METRICS_PORT="9090" (default)
2023-02-17 00:52:27.149145 I | op-k8sutil: CSI_RBD_GRPC_METRICS_PORT="9090" (default)
2023-02-17 00:52:27.149149 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2023-02-17 00:52:27.149152 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2023-02-17 00:52:27.149156 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2023-02-17 00:52:27.149159 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2023-02-17 00:52:27.149163 I | op-k8sutil: CSI_ENABLE_LIVENESS="false" (configmap)
2023-02-17 00:52:27.149166 I | op-k8sutil: CSI_PLUGIN_PRIORITY_CLASSNAME="system-node-critical" (configmap)
2023-02-17 00:52:27.149170 I | op-k8sutil: CSI_PROVISIONER_PRIORITY_CLASSNAME="system-cluster-critical" (configmap)
2023-02-17 00:52:27.149174 I | op-k8sutil: CSI_ENABLE_OMAP_GENERATOR="false" (default)
2023-02-17 00:52:27.149177 I | op-k8sutil: CSI_ENABLE_RBD_SNAPSHOTTER="true" (configmap)
2023-02-17 00:52:27.149180 I | op-k8sutil: CSI_ENABLE_CEPHFS_SNAPSHOTTER="true" (configmap)
2023-02-17 00:52:27.149182 I | op-k8sutil: CSI_ENABLE_VOLUME_REPLICATION="false" (configmap)
2023-02-17 00:52:27.149184 I | op-k8sutil: CSI_ENABLE_CSIADDONS="false" (configmap)
2023-02-17 00:52:27.149186 I | op-k8sutil: CSI_ENABLE_ENCRYPTION="false" (configmap)
2023-02-17 00:52:27.149188 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2023-02-17 00:52:27.149191 I | op-k8sutil: CSI_NFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2023-02-17 00:52:27.149193 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2023-02-17 00:52:27.149195 I | op-k8sutil: CSI_PLUGIN_ENABLE_SELINUX_HOST_MOUNT="false" (configmap)
2023-02-17 00:52:27.149197 I | ceph-csi: Kubernetes version is 1.20+
2023-02-17 00:52:27.149200 I | op-k8sutil: ROOK_CSI_RESIZER_IMAGE="registry.k8s.io/sig-storage/csi-resizer:v1.4.0" (default)
2023-02-17 00:52:27.149202 I | op-k8sutil: CSI_LOG_LEVEL="" (default)
2023-02-17 00:52:27.149204 I | op-k8sutil: CSI_SIDECAR_LOG_LEVEL="" (default)
2023-02-17 00:52:27.152437 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:52:27.152521 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:52:27.152823 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:52:27.155003 I | op-k8sutil: CSI_PROVISIONER_REPLICAS="2" (configmap)
2023-02-17 00:52:27.155035 I | op-k8sutil: ROOK_CSI_CEPH_IMAGE="quay.io/cephcsi/cephcsi:v3.6.2" (default)
2023-02-17 00:52:27.155041 I | op-k8sutil: ROOK_CSI_NFS_IMAGE="registry.k8s.io/sig-storage/nfsplugin:v4.0.0" (default)
2023-02-17 00:52:27.155045 I | op-k8sutil: ROOK_CSI_REGISTRAR_IMAGE="registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.5.1" (default)
2023-02-17 00:52:27.155316 I | op-k8sutil: ROOK_CSI_PROVISIONER_IMAGE="registry.k8s.io/sig-storage/csi-provisioner:v3.1.0" (default)
2023-02-17 00:52:27.155326 I | op-k8sutil: ROOK_CSI_ATTACHER_IMAGE="registry.k8s.io/sig-storage/csi-attacher:v3.4.0" (default)
2023-02-17 00:52:27.155329 I | op-k8sutil: ROOK_CSI_SNAPSHOTTER_IMAGE="registry.k8s.io/sig-storage/csi-snapshotter:v6.0.1" (default)
2023-02-17 00:52:27.155333 I | op-k8sutil: ROOK_CSI_KUBELET_DIR_PATH="/var/lib/kubelet" (default)
2023-02-17 00:52:27.155340 I | op-k8sutil: CSI_VOLUME_REPLICATION_IMAGE="quay.io/csiaddons/volumereplication-operator:v0.3.0" (default)
2023-02-17 00:52:27.155343 I | op-k8sutil: ROOK_CSIADDONS_IMAGE="quay.io/csiaddons/k8s-sidecar:v0.4.0" (default)
2023-02-17 00:52:27.155345 I | op-k8sutil: ROOK_CSI_CEPHFS_POD_LABELS="" (default)
2023-02-17 00:52:27.155349 I | op-k8sutil: ROOK_CSI_NFS_POD_LABELS="" (default)
2023-02-17 00:52:27.155353 I | op-k8sutil: ROOK_CSI_RBD_POD_LABELS="" (default)
2023-02-17 00:52:27.155357 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.6.2"
2023-02-17 00:52:27.155650 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2023-02-17 00:52:27.155658 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-17 00:52:27.159776 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v16.2.10...
2023-02-17 00:52:27.161612 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2023-02-17 00:52:27.161675 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:52:27.161715 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:52:27.162683 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-17 00:52:27.164170 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:52:27.167136 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:52:27.321496 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2023-02-17 00:52:27.570117 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:29933568 AvailableBytes:1649237508096 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:52:27.576413 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:27.996228 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:52:27.996245 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:52:27.996331 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:29933568 AvailableBytes:1649237508096 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:52:27.996341 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:52:28.010350 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:52:28.010392 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:52:28.207383 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2023-02-17 00:52:28.238151 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-csi-detect-version" cm is handled by another watcher
2023-02-17 00:52:28.238827 I | ceph-csi: Detected ceph CSI image version: "v3.6.2"
2023-02-17 00:52:28.260702 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2023-02-17 00:52:28.260734 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-17 00:52:28.260740 I | op-k8sutil: CSI_PLUGIN_TOLERATIONS="" (default)
2023-02-17 00:52:28.260743 I | op-k8sutil: CSI_PLUGIN_NODE_AFFINITY="" (default)
2023-02-17 00:52:28.260745 I | op-k8sutil: CSI_RBD_PLUGIN_TOLERATIONS="" (default)
2023-02-17 00:52:28.260747 I | op-k8sutil: CSI_RBD_PLUGIN_NODE_AFFINITY="" (default)
2023-02-17 00:52:28.260750 I | op-k8sutil: CSI_RBD_PLUGIN_RESOURCE="" (default)
2023-02-17 00:52:28.282846 I | op-k8sutil: CSI_RBD_PROVISIONER_TOLERATIONS="" (default)
2023-02-17 00:52:28.282866 I | op-k8sutil: CSI_RBD_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-17 00:52:28.282871 I | op-k8sutil: CSI_RBD_PROVISIONER_RESOURCE="" (default)
2023-02-17 00:52:28.306419 I | ceph-csi: successfully started CSI Ceph RBD driver
2023-02-17 00:52:28.306454 I | op-k8sutil: CSI_CEPHFS_PLUGIN_TOLERATIONS="" (default)
2023-02-17 00:52:28.306460 I | op-k8sutil: CSI_CEPHFS_PLUGIN_NODE_AFFINITY="" (default)
2023-02-17 00:52:28.306463 I | op-k8sutil: CSI_CEPHFS_PLUGIN_RESOURCE="" (default)
2023-02-17 00:52:28.336539 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_TOLERATIONS="" (default)
2023-02-17 00:52:28.336568 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_NODE_AFFINITY="" (default)
2023-02-17 00:52:28.336574 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_RESOURCE="" (default)
2023-02-17 00:52:28.361938 I | ceph-csi: successfully started CSI CephFS driver
2023-02-17 00:52:28.361973 I | op-k8sutil: CSI_RBD_FSGROUPPOLICY="ReadWriteOnceWithFSType" (configmap)
2023-02-17 00:52:28.379756 I | ceph-csi: CSIDriver object created for driver "rook-ceph.rbd.csi.ceph.com"
2023-02-17 00:52:28.379791 I | op-k8sutil: CSI_CEPHFS_FSGROUPPOLICY="ReadWriteOnceWithFSType" (configmap)
2023-02-17 00:52:28.394579 I | ceph-csi: CSIDriver object created for driver "rook-ceph.cephfs.csi.ceph.com"
2023-02-17 00:52:28.394600 I | ceph-csi: CSI NFS driver disabled
2023-02-17 00:52:28.394608 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2023-02-17 00:52:28.399218 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2023-02-17 00:52:28.399254 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2023-02-17 00:52:28.410654 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2023-02-17 00:52:28.410683 I | ceph-csi: successfully removed CSI NFS driver
2023-02-17 00:52:28.998172 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-17 00:52:28.998217 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-17 00:52:28.998228 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-17 00:52:28.998234 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2023-02-17 00:52:29.420117 D | CmdReporter: job rook-ceph-detect-version has returned results
2023-02-17 00:52:29.437500 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-detect-version" cm is handled by another watcher
2023-02-17 00:52:29.438307 I | ceph-spec: detected ceph image version: "16.2.10-0 pacific"
2023-02-17 00:52:29.438318 I | ceph-cluster-controller: validating ceph version from provided image
2023-02-17 00:52:29.442192 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2023-02-17 00:52:29.451043 I | ceph-spec: parsing mon endpoints: a=10.211.55.97:6789
2023-02-17 00:52:29.451098 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc00128e160], assignment=&{Schedule:map[a:0xc000862bc0]}
2023-02-17 00:52:29.456126 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2023-02-17 00:52:29.456765 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2023-02-17 00:52:29.456792 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:29.459002 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:52:29.459028 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:52:29.459037 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:52:29.459046 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:52:29.840590 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:52:29.840644 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:52:29.840685 D | ceph-cluster-controller: both cluster and image spec versions are identical, doing nothing 16.2.10-0 pacific
2023-02-17 00:52:29.840706 I | ceph-cluster-controller: cluster "rook-ceph": version "16.2.10-0 pacific" detected for image "quay.io/ceph/ceph:v16.2.10"
2023-02-17 00:52:29.856935 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring the Ceph cluster"
2023-02-17 00:52:29.867204 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:52:29.867233 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:52:29.872442 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2023-02-17 00:52:29.872498 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mons"
2023-02-17 00:52:29.882201 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:52:29.882240 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:52:29.882246 I | op-mon: start running mons
2023-02-17 00:52:29.882249 D | op-mon: establishing ceph cluster info
2023-02-17 00:52:29.882293 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:52:29.882303 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:52:29.885763 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2023-02-17 00:52:29.920325 I | ceph-spec: parsing mon endpoints: a=10.211.55.97:6789
2023-02-17 00:52:29.920371 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc00128e5e0], assignment=&{Schedule:map[a:0xc0008639c0]}
2023-02-17 00:52:30.724929 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2023-02-17 00:52:30.922770 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.211.55.97:6789"],"namespace":""}] data:a=10.211.55.97:6789 mapping:{"node":{"a":{"Name":"node1","Hostname":"node1","Address":"10.211.55.97"}}} maxMonId:0]
2023-02-17 00:52:31.121491 D | op-config: updating config secret "rook-ceph-config"
2023-02-17 00:52:31.521742 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2023-02-17 00:52:31.521950 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2023-02-17 00:52:31.521963 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2023-02-17 00:52:32.121953 D | op-cfg-keyring: updating secret for rook-ceph-mons-keyring
2023-02-17 00:52:32.520253 D | op-cfg-keyring: updating secret for rook-ceph-admin-keyring
2023-02-17 00:52:33.121360 I | op-mon: targeting the mon count 2
2023-02-17 00:52:33.130537 D | op-mon: mon a already scheduled
2023-02-17 00:52:33.130592 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-b DaemonName:b PublicIP: Port:6789 Zone: DataPathMap:0xc00110ed20}
2023-02-17 00:52:33.138482 I | op-mon: created canary deployment rook-ceph-mon-b-canary
2023-02-17 00:52:33.154900 D | ceph-spec: object "rook-ceph-mon-b-canary" matched on update
2023-02-17 00:52:33.154935 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:33.175822 D | ceph-spec: object "rook-ceph-mon-b-canary" matched on update
2023-02-17 00:52:33.175841 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:33.197433 D | ceph-spec: object "rook-ceph-mon-b-canary" matched on update
2023-02-17 00:52:33.197463 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:33.522890 I | op-mon: canary monitor deployment rook-ceph-mon-b-canary scheduled to node3
2023-02-17 00:52:33.522957 I | op-mon: mon b assigned to node node3
2023-02-17 00:52:33.522964 D | op-mon: using internal IP 10.211.55.99 for node node3
2023-02-17 00:52:33.522974 D | op-mon: mons have been scheduled
2023-02-17 00:52:33.530118 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-b-canary"
2023-02-17 00:52:33.538400 I | op-config: setting "global"="mon allow pool delete"="true" option to the mon configuration database
2023-02-17 00:52:33.538425 D | exec: Running command: ceph config set global mon_allow_pool_delete true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:33.538436 D | ceph-spec: object "rook-ceph-mon-b-canary" matched on update
2023-02-17 00:52:33.538446 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:33.547073 D | ceph-spec: object "rook-ceph-mon-b-canary" matched on update
2023-02-17 00:52:33.547103 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:33.591913 D | ceph-spec: object "rook-ceph-mon-b-canary" matched on update
2023-02-17 00:52:33.591940 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:33.873507 I | op-config: successfully set "global"="mon allow pool delete"="true" option to the mon configuration database
2023-02-17 00:52:33.873539 I | op-config: setting "global"="mon cluster log file"="" option to the mon configuration database
2023-02-17 00:52:33.873560 D | exec: Running command: ceph config set global mon_cluster_log_file  --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:34.174852 I | op-config: successfully set "global"="mon cluster log file"="" option to the mon configuration database
2023-02-17 00:52:34.174868 I | op-config: setting "global"="mon allow pool size one"="true" option to the mon configuration database
2023-02-17 00:52:34.174880 D | exec: Running command: ceph config set global mon_allow_pool_size_one true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:34.487521 I | op-config: successfully set "global"="mon allow pool size one"="true" option to the mon configuration database
2023-02-17 00:52:34.487539 I | op-config: setting "global"="osd scrub auto repair"="true" option to the mon configuration database
2023-02-17 00:52:34.487552 D | exec: Running command: ceph config set global osd_scrub_auto_repair true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:34.789654 I | op-config: successfully set "global"="osd scrub auto repair"="true" option to the mon configuration database
2023-02-17 00:52:34.789682 I | op-config: setting "global"="log to file"="false" option to the mon configuration database
2023-02-17 00:52:34.789696 D | exec: Running command: ceph config set global log_to_file false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:35.121929 I | op-config: successfully set "global"="log to file"="false" option to the mon configuration database
2023-02-17 00:52:35.121997 I | op-config: deleting "log file" option from the mon configuration database
2023-02-17 00:52:35.122011 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:35.412715 I | op-config: successfully deleted "log file" option from the mon configuration database
2023-02-17 00:52:35.412764 I | op-mon: creating mon b
2023-02-17 00:52:35.412770 I | op-mon: setting mon endpoints for hostnetwork mode
2023-02-17 00:52:35.412774 I | op-mon: setting mon endpoints for hostnetwork mode
2023-02-17 00:52:35.424772 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2023-02-17 00:52:35.429125 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-mon-endpoints" cm is handled by another watcher
2023-02-17 00:52:35.429161 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2023-02-17 00:52:35.429191 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:52:35.429246 D | op-mon: mons were added or removed from the endpoints cm
2023-02-17 00:52:35.429253 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2023-02-17 00:52:35.429637 D | op-mon: mons were added or removed from the endpoints cm
2023-02-17 00:52:35.429645 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2023-02-17 00:52:35.429914 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.211.55.99:6789","10.211.55.97:6789"],"namespace":""}] data:a=10.211.55.97:6789,b=10.211.55.99:6789 mapping:{"node":{"a":{"Name":"node1","Hostname":"node1","Address":"10.211.55.97"},"b":{"Name":"node3","Hostname":"node3","Address":"10.211.55.99"}}} maxMonId:0]
2023-02-17 00:52:35.434071 D | op-config: updating config secret "rook-ceph-config"
2023-02-17 00:52:35.439236 D | ceph-spec: object "rook-ceph-config" matched on update
2023-02-17 00:52:35.439295 D | ceph-spec: do not reconcile on "rook-ceph-config" secret changes
2023-02-17 00:52:35.442573 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2023-02-17 00:52:35.442731 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2023-02-17 00:52:35.442756 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2023-02-17 00:52:35.449293 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-csi-config" cm is handled by another watcher
2023-02-17 00:52:35.458375 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.211.55.97 Port:6789 Zone: DataPathMap:0xc00110ecf0}
2023-02-17 00:52:35.463474 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2023-02-17 00:52:35.463492 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2023-02-17 00:52:35.471489 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2023-02-17 00:52:35.471519 I | op-mon: waiting for mon quorum with [a b]
2023-02-17 00:52:35.524593 I | op-mon: mon b is not yet running
2023-02-17 00:52:35.524623 I | op-mon: mons running: [a]
2023-02-17 00:52:35.524643 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:35.879697 I | op-mon: Monitors in quorum: [a]
2023-02-17 00:52:35.879786 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-b DaemonName:b PublicIP:10.211.55.99 Port:6789 Zone: DataPathMap:0xc00110ed20}
2023-02-17 00:52:35.885071 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-b
2023-02-17 00:52:35.885105 D | op-mon: Starting mon: rook-ceph-mon-b
2023-02-17 00:52:35.899969 I | op-mon: updating maxMonID from 0 to 1 after committing mon "b"
2023-02-17 00:52:35.917143 D | ceph-spec: object "rook-ceph-mon-b" matched on update
2023-02-17 00:52:35.917232 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:35.923070 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-mon-endpoints" cm is handled by another watcher
2023-02-17 00:52:35.923119 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2023-02-17 00:52:35.923151 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:52:35.946093 D | ceph-spec: object "rook-ceph-mon-b" matched on update
2023-02-17 00:52:35.946123 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:35.976863 D | ceph-spec: object "rook-ceph-mon-b" matched on update
2023-02-17 00:52:35.976879 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:36.325545 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2023-02-17 00:52:36.522705 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-mon-endpoints" cm is handled by another watcher
2023-02-17 00:52:36.522748 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2023-02-17 00:52:36.522754 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:52:36.522799 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.211.55.97:6789","10.211.55.99:6789"],"namespace":""}] data:a=10.211.55.97:6789,b=10.211.55.99:6789 mapping:{"node":{"a":{"Name":"node1","Hostname":"node1","Address":"10.211.55.97"},"b":{"Name":"node3","Hostname":"node3","Address":"10.211.55.99"}}} maxMonId:1]
2023-02-17 00:52:36.522811 I | op-mon: waiting for mon quorum with [a b]
2023-02-17 00:52:36.926986 I | op-mon: mon b is not yet running
2023-02-17 00:52:36.927016 I | op-mon: mons running: [a]
2023-02-17 00:52:36.927029 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:37.298913 I | op-mon: Monitors in quorum: [a]
2023-02-17 00:52:37.298945 I | op-mon: mons created: 2
2023-02-17 00:52:37.298962 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:37.670007 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:52:37.670037 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":8}}
2023-02-17 00:52:37.670069 I | op-mon: waiting for mon quorum with [a b]
2023-02-17 00:52:37.685217 I | op-mon: mon b is not yet running
2023-02-17 00:52:37.685250 I | op-mon: mons running: [a]
2023-02-17 00:52:38.528210 D | ceph-crashcollector-controller: "rook-ceph-mon-b-74884858f6-mv9gm" is a ceph pod!
2023-02-17 00:52:38.528316 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:52:38.528815 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:52:38.546584 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:52:38.547873 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:52:38.553141 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:52:38.560613 D | ceph-spec: object "rook-ceph-mon-b-canary" matched on update
2023-02-17 00:52:38.560628 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:38.592983 D | ceph-spec: object "rook-ceph-mon-b-canary" did not match on delete
2023-02-17 00:52:38.593000 D | ceph-spec: object "rook-ceph-mon-b-canary" did not match on delete
2023-02-17 00:52:38.593005 D | ceph-spec: object "rook-ceph-mon-b-canary" did not match on delete
2023-02-17 00:52:38.593010 D | ceph-spec: object "rook-ceph-mon-b-canary" did not match on delete
2023-02-17 00:52:38.593018 D | ceph-spec: do not reconcile "rook-ceph-mon-b-canary" on monitor canary deployments
2023-02-17 00:52:38.593022 D | ceph-spec: object "rook-ceph-mon-b-canary" did not match on delete
2023-02-17 00:52:42.705734 I | op-mon: mons running: [a b]
2023-02-17 00:52:42.705769 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:46.074602 I | op-mon: Monitors in quorum: [a b]
2023-02-17 00:52:46.074633 D | op-mon: mon endpoints used are: a=10.211.55.97:6789,b=10.211.55.99:6789
2023-02-17 00:52:46.074638 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2023-02-17 00:52:46.074641 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2023-02-17 00:52:46.074643 D | op-mon: Released lock for mon orchestration
2023-02-17 00:52:46.074647 D | ceph-cluster-controller: monitors are up and running, executing post actions
2023-02-17 00:52:46.074652 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2023-02-17 00:52:46.074662 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:46.458018 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2023-02-17 00:52:46.458062 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:46.835444 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2023-02-17 00:52:46.835485 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r mgr allow rw osd allow rw tag cephfs metadata=* --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:47.218651 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2023-02-17 00:52:47.218691 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:47.665636 D | op-cfg-keyring: updating secret for rook-csi-cephfs-node
2023-02-17 00:52:47.676735 D | op-cfg-keyring: updating secret for rook-csi-rbd-provisioner
2023-02-17 00:52:47.684146 D | op-cfg-keyring: updating secret for rook-csi-rbd-node
2023-02-17 00:52:47.690853 D | op-cfg-keyring: updating secret for rook-csi-cephfs-provisioner
2023-02-17 00:52:47.694783 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2023-02-17 00:52:47.694799 I | cephclient: getting or creating ceph auth key "client.crash"
2023-02-17 00:52:47.694809 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:48.090795 D | op-cfg-keyring: updating secret for rook-ceph-crash-collector-keyring
2023-02-17 00:52:48.094656 I | ceph-crashcollector-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2023-02-17 00:52:48.094684 I | op-config: deleting "mon_mds_skip_sanity" option from the mon configuration database
2023-02-17 00:52:48.094696 D | exec: Running command: ceph config rm mon mon_mds_skip_sanity --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:48.393698 I | op-config: successfully deleted "mon_mds_skip_sanity" option from the mon configuration database
2023-02-17 00:52:48.393733 I | ceph-cluster-controller: setting msgr2 encryption mode to "crc secure"
2023-02-17 00:52:48.393739 I | op-config: setting "global"="ms_cluster_mode"="crc secure" option to the mon configuration database
2023-02-17 00:52:48.393751 D | exec: Running command: ceph config set global ms_cluster_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:48.716201 I | op-config: successfully set "global"="ms_cluster_mode"="crc secure" option to the mon configuration database
2023-02-17 00:52:48.716231 I | op-config: setting "global"="ms_service_mode"="crc secure" option to the mon configuration database
2023-02-17 00:52:48.716246 D | exec: Running command: ceph config set global ms_service_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:49.022645 I | op-config: successfully set "global"="ms_service_mode"="crc secure" option to the mon configuration database
2023-02-17 00:52:49.022677 I | op-config: setting "global"="ms_client_mode"="crc secure" option to the mon configuration database
2023-02-17 00:52:49.022695 D | exec: Running command: ceph config set global ms_client_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:49.386986 I | op-config: successfully set "global"="ms_client_mode"="crc secure" option to the mon configuration database
2023-02-17 00:52:49.387024 W | ceph-cluster-controller: network compression requires Ceph Quincy (v17) or newer, skipping for current ceph "16.2.10-0 pacific"
2023-02-17 00:52:49.387032 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2023-02-17 00:52:49.387035 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2023-02-17 00:52:49.387049 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:49.757449 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2023-02-17 00:52:49.757495 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-rook-ceph" in namespace "rook-ceph"
2023-02-17 00:52:49.757499 D | op-k8sutil: creating secret cluster-peer-token-rook-ceph
2023-02-17 00:52:49.772145 D | ceph-spec: object "cluster-peer-token-rook-ceph" matched on update
2023-02-17 00:52:49.772480 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mgr(s)"
2023-02-17 00:52:49.772738 D | ceph-spec: object "cluster-peer-token-rook-ceph" diff is [redacted for Secrets]
2023-02-17 00:52:49.772770 D | ceph-spec: trimming 'status' field from patch
2023-02-17 00:52:49.772775 D | ceph-spec: trimming 'metadata' field from patch
2023-02-17 00:52:49.772800 I | ceph-spec: controller will reconcile resource "cluster-peer-token-rook-ceph" based on patch: [redacted patch details due to potentially sensitive content]
2023-02-17 00:52:49.782413 I | op-mgr: start running mgr
2023-02-17 00:52:49.782438 I | cephclient: getting or creating ceph auth key "mgr.a"
2023-02-17 00:52:49.782447 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:49.782664 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:52:49.782673 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:52:50.161012 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2023-02-17 00:52:50.164766 D | op-cfg-keyring: updating secret for rook-ceph-mgr-a-keyring
2023-02-17 00:52:50.168180 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc0012debd0}
2023-02-17 00:52:50.178102 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2023-02-17 00:52:50.188476 I | op-k8sutil: updating deployment "rook-ceph-mgr-a" after verifying it is safe to stop
2023-02-17 00:52:50.188492 I | op-mon: checking if we can stop the deployment rook-ceph-mgr-a
2023-02-17 00:52:50.188503 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:52:50.592368 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":9}}
2023-02-17 00:52:50.592398 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":9}}
2023-02-17 00:52:50.592472 D | cephclient: deployment rook-ceph-mgr-a is ok to be updated.
2023-02-17 00:52:50.604677 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-17 00:52:50.604711 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:50.612142 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:1 AvailableReplicas:1 UnavailableReplicas:0 Conditions:[{Type:Available Status:True LastUpdateTime:2023-02-17 00:48:12 +0000 UTC LastTransitionTime:2023-02-17 00:48:12 +0000 UTC Reason:MinimumReplicasAvailable Message:Deployment has minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:48:12 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:NewReplicaSetAvailable Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" has successfully progressed.}] CollisionCount:<nil>}
2023-02-17 00:52:50.629695 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-17 00:52:50.629742 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:50.681682 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-17 00:52:50.681711 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:53.620702 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:0 UpdatedReplicas:0 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:0 Conditions:[{Type:Progressing Status:True LastUpdateTime:2023-02-17 00:48:12 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:NewReplicaSetAvailable Message:ReplicaSet "rook-ceph-mgr-a-8759b445f" has successfully progressed.} {Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.}] CollisionCount:<nil>}
2023-02-17 00:52:54.663565 D | ceph-crashcollector-controller: "rook-ceph-mgr-a-8759b445f-h2hhr" is a ceph pod!
2023-02-17 00:52:54.663655 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:52:54.664168 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:52:54.676559 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:52:54.677565 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:52:54.683017 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:52:54.695139 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-17 00:52:54.695168 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:54.718465 D | ceph-crashcollector-controller: "rook-ceph-mgr-a-84b878fd5b-pglkm" is a ceph pod!
2023-02-17 00:52:54.718516 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:52:54.718862 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:52:54.733553 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-17 00:52:54.733571 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:54.734660 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:52:54.735794 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:52:54.739617 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:52:54.755932 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-17 00:52:54.755949 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:52:56.627824 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:52:59.637630 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:02.647029 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:05.655177 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:08.661370 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:11.669117 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:14.679215 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:17.686871 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:20.696723 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:23.704997 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:26.255854 D | operator: number of goroutines 427
2023-02-17 00:53:26.712514 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:27.137483 D | op-osd: checking osd processes status.
2023-02-17 00:53:27.137543 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:53:27.440809 D | op-osd: validating status of osd.0
2023-02-17 00:53:27.440836 D | op-osd: osd.0 is healthy.
2023-02-17 00:53:27.440841 D | op-osd: validating status of osd.1
2023-02-17 00:53:27.440844 D | op-osd: osd.1 is healthy.
2023-02-17 00:53:27.440846 D | op-osd: validating status of osd.2
2023-02-17 00:53:27.440848 D | op-osd: osd.2 is healthy.
2023-02-17 00:53:27.440850 D | op-osd: validating status of osd.3
2023-02-17 00:53:27.440852 D | op-osd: osd.3 is healthy.
2023-02-17 00:53:27.440854 D | op-osd: validating status of osd.4
2023-02-17 00:53:27.440856 D | op-osd: osd.4 is healthy.
2023-02-17 00:53:27.440857 D | op-osd: validating status of osd.5
2023-02-17 00:53:27.440859 D | op-osd: osd.5 is healthy.
2023-02-17 00:53:27.440871 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:53:28.020539 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:53:28.020592 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:53:28.364966 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30007296 AvailableBytes:1649237434368 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:53:28.371327 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:53:28.717744 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":9}}
2023-02-17 00:53:28.717797 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":9}}
2023-02-17 00:53:28.717894 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30007296 AvailableBytes:1649237434368 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:53:28.717910 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:53:28.729327 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:53:28.729342 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:53:29.718816 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:32.727103 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:35.734283 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:38.743049 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:41.754575 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:44.765468 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:47.772146 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:50.782457 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:53.794579 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:56.800668 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:53:59.810805 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:01.522695 D | ceph-spec: object "rook-ceph-mon-b" matched on update
2023-02-17 00:54:01.522742 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:54:02.818310 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:05.824047 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:08.834467 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:11.846602 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:14.854223 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:17.862112 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:20.872247 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:2 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:52:50 +0000 UTC LastTransitionTime:2023-02-17 00:52:50 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:52:54 +0000 UTC LastTransitionTime:2023-02-17 00:47:06 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-84b878fd5b" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:22.695129 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2023-02-17 00:54:22.695146 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:54:23.881260 I | op-k8sutil: finished waiting for updated deployment "rook-ceph-mgr-a"
2023-02-17 00:54:23.881311 I | op-mon: checking if we can continue the deployment rook-ceph-mgr-a
2023-02-17 00:54:23.881321 I | cephclient: getting or creating ceph auth key "mgr.b"
2023-02-17 00:54:23.881331 D | exec: Running command: ceph auth get-or-create-key mgr.b mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:54:24.257856 D | op-mgr: legacy mgr key "rook-ceph-mgr-b" is already removed
2023-02-17 00:54:24.261753 D | op-cfg-keyring: creating secret for rook-ceph-mgr-b-keyring
2023-02-17 00:54:24.269080 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-b DaemonID:b DataPathMap:0xc000dd51d0}
2023-02-17 00:54:24.277873 I | op-config: setting "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2023-02-17 00:54:24.277911 D | exec: Running command: ceph config set mon auth_allow_insecure_global_id_reclaim false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:54:24.307394 D | ceph-spec: object "rook-ceph-mgr-b" matched on update
2023-02-17 00:54:24.307422 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:54:24.337871 D | ceph-crashcollector-controller: "rook-ceph-mgr-b-64dd9df8fd-cqxq5" is a ceph pod!
2023-02-17 00:54:24.337943 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:54:24.341606 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:54:24.351473 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:54:24.353329 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:54:24.361658 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:54:24.367885 D | ceph-spec: object "rook-ceph-mgr-b" matched on update
2023-02-17 00:54:24.367906 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:54:24.394476 D | ceph-spec: object "rook-ceph-mgr-b" matched on update
2023-02-17 00:54:24.394512 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:54:24.634532 I | op-config: successfully set "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2023-02-17 00:54:24.634549 I | op-config: insecure global ID is now disabled
2023-02-17 00:54:24.643793 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:27.649739 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:27.743077 D | op-osd: checking osd processes status.
2023-02-17 00:54:27.743114 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:54:28.085771 D | op-osd: validating status of osd.0
2023-02-17 00:54:28.085798 D | op-osd: osd.0 is healthy.
2023-02-17 00:54:28.085803 D | op-osd: validating status of osd.1
2023-02-17 00:54:28.085805 D | op-osd: osd.1 is healthy.
2023-02-17 00:54:28.085807 D | op-osd: validating status of osd.2
2023-02-17 00:54:28.085809 D | op-osd: osd.2 is healthy.
2023-02-17 00:54:28.085814 D | op-osd: validating status of osd.3
2023-02-17 00:54:28.085816 D | op-osd: osd.3 is healthy.
2023-02-17 00:54:28.085818 D | op-osd: validating status of osd.4
2023-02-17 00:54:28.085820 D | op-osd: osd.4 is healthy.
2023-02-17 00:54:28.085822 D | op-osd: validating status of osd.5
2023-02-17 00:54:28.085824 D | op-osd: osd.5 is healthy.
2023-02-17 00:54:28.085836 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:54:28.729125 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:54:28.729156 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:54:29.092276 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30007296 AvailableBytes:1649237434368 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:54:29.099686 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:54:29.473326 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":9}}
2023-02-17 00:54:29.473356 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":1},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":9}}
2023-02-17 00:54:29.473432 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30007296 AvailableBytes:1649237434368 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:54:29.473445 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:54:29.485622 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:54:29.485639 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:54:30.656708 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:33.668880 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:36.675754 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:39.681899 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:42.689741 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:45.701012 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:48.707609 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:51.719403 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:54.727522 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:54:57.733601 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:00.743136 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:03.753138 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:06.758754 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:09.768227 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:12.775107 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:15.781908 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:18.790961 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:21.799648 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:24.805785 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:27.813216 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:28.408872 D | op-osd: checking osd processes status.
2023-02-17 00:55:28.408922 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:28.696438 D | op-osd: validating status of osd.0
2023-02-17 00:55:28.696467 D | op-osd: osd.0 is healthy.
2023-02-17 00:55:28.696472 D | op-osd: validating status of osd.1
2023-02-17 00:55:28.696474 D | op-osd: osd.1 is healthy.
2023-02-17 00:55:28.696476 D | op-osd: validating status of osd.2
2023-02-17 00:55:28.696478 D | op-osd: osd.2 is healthy.
2023-02-17 00:55:28.696480 D | op-osd: validating status of osd.3
2023-02-17 00:55:28.696482 D | op-osd: osd.3 is healthy.
2023-02-17 00:55:28.696484 D | op-osd: validating status of osd.4
2023-02-17 00:55:28.696485 D | op-osd: osd.4 is healthy.
2023-02-17 00:55:28.696487 D | op-osd: validating status of osd.5
2023-02-17 00:55:28.696489 D | op-osd: osd.5 is healthy.
2023-02-17 00:55:28.696501 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:29.486154 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:55:29.486189 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:55:29.838713 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30007296 AvailableBytes:1649237434368 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:55:29.845438 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:30.206037 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:55:30.206069 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:55:30.206133 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30007296 AvailableBytes:1649237434368 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:55:30.206151 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:55:30.217935 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:55:30.217967 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:55:30.820702 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:33.829340 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:36.836709 D | op-k8sutil: deployment "rook-ceph-mgr-b" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2023-02-17 00:54:24 +0000 UTC LastTransitionTime:2023-02-17 00:54:24 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-b-64dd9df8fd" is progressing.}] CollisionCount:<nil>}
2023-02-17 00:55:37.513355 D | ceph-spec: object "rook-ceph-mgr-b" matched on update
2023-02-17 00:55:37.513413 D | ceph-spec: do not reconcile deployments updates
2023-02-17 00:55:39.845343 I | op-k8sutil: finished waiting for updated deployment "rook-ceph-mgr-b"
2023-02-17 00:55:39.845378 D | exec: Running command: ceph mgr stat --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:40.193661 D | op-k8sutil: kubernetes version fetched 1.20.4-aliyun.1
2023-02-17 00:55:40.200844 I | op-mgr: setting services to point to mgr "a"
2023-02-17 00:55:40.200882 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2023-02-17 00:55:40.219471 D | op-k8sutil: updating service rook-ceph-mgr-dashboard
2023-02-17 00:55:40.228371 D | op-k8sutil: creating service rook-ceph-mgr
2023-02-17 00:55:40.242173 D | op-k8sutil: updating service rook-ceph-mgr
2023-02-17 00:55:40.254603 I | op-mgr: no need to update service "rook-ceph-mgr"
2023-02-17 00:55:40.254631 I | op-mgr: no need to update service "rook-ceph-mgr-dashboard"
2023-02-17 00:55:40.254690 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph OSDs"
2023-02-17 00:55:40.254714 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:40.254955 D | cephclient: balancer module is already 'on' on pacific, doing nothingbalancer
2023-02-17 00:55:40.254971 I | op-mgr: successful modules: balancer
2023-02-17 00:55:40.254986 D | exec: Running command: ceph mgr module enable pg_autoscaler --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:40.255082 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:40.264448 I | op-osd: start running osds in namespace "rook-ceph"
2023-02-17 00:55:40.264475 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2023-02-17 00:55:40.269209 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:55:40.269246 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:55:40.292720 D | op-osd: 6 of 6 OSD Deployments need updated
2023-02-17 00:55:40.292742 I | op-osd: start provisioning the OSDs on PVCs, if needed
2023-02-17 00:55:40.296723 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2023-02-17 00:55:40.296740 I | op-osd: start provisioning the OSDs on nodes, if needed
2023-02-17 00:55:40.303370 D | op-osd: storage nodes: [{Name:node1 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}} {Name:node2 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}} {Name:node3 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}}]
2023-02-17 00:55:40.310853 I | op-osd: 3 of the 3 storage nodes are valid
2023-02-17 00:55:40.611825 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-node1 to start a new one
2023-02-17 00:55:40.634733 I | op-k8sutil: batch job rook-ceph-osd-prepare-node1 still exists
2023-02-17 00:55:41.190137 I | op-mgr: successful modules: prometheus
2023-02-17 00:55:41.191001 I | op-config: setting "global"="osd_pool_default_pg_autoscale_mode"="on" option to the mon configuration database
2023-02-17 00:55:41.191039 D | exec: Running command: ceph config set global osd_pool_default_pg_autoscale_mode on --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:41.403849 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node1-kdbmm" is a ceph pod!
2023-02-17 00:55:41.403945 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:55:41.404509 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:55:41.413759 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:55:41.414731 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:55:41.418565 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:55:41.512362 I | op-config: successfully set "global"="osd_pool_default_pg_autoscale_mode"="on" option to the mon configuration database
2023-02-17 00:55:41.512393 I | op-config: setting "global"="mon_pg_warn_min_per_osd"="0" option to the mon configuration database
2023-02-17 00:55:41.512417 D | exec: Running command: ceph config set global mon_pg_warn_min_per_osd 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:41.821528 I | op-config: successfully set "global"="mon_pg_warn_min_per_osd"="0" option to the mon configuration database
2023-02-17 00:55:41.821557 I | op-mgr: successful modules: mgr module(s) from the spec
2023-02-17 00:55:43.650383 I | op-k8sutil: batch job rook-ceph-osd-prepare-node1 deleted
2023-02-17 00:55:43.661836 I | op-osd: started OSD provisioning job for node "node1"
2023-02-17 00:55:43.681496 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-node2 to start a new one
2023-02-17 00:55:43.698574 I | op-k8sutil: batch job rook-ceph-osd-prepare-node2 still exists
2023-02-17 00:55:43.706363 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node1-759lj" is a ceph pod!
2023-02-17 00:55:43.706453 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:55:43.706889 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:55:43.726423 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node2-4sc47" is a ceph pod!
2023-02-17 00:55:43.731200 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:55:43.732370 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:55:43.737467 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:55:43.737516 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:55:43.737835 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:55:43.750551 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-17 00:55:43.751680 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:55:43.754774 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:55:45.305555 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node1-status" cm is handled by another watcher
2023-02-17 00:55:45.305626 D | ceph-spec: object "rook-ceph-osd-node1-status" matched on update
2023-02-17 00:55:45.305681 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:55:46.196015 I | op-mgr: the dashboard secret was already generated
2023-02-17 00:55:46.196044 D | exec: Running command: ceph dashboard create-self-signed-cert --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:46.669812 I | op-mgr: setting ceph dashboard "admin" login creds
2023-02-17 00:55:46.669933 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/2000649542 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:46.703672 I | op-k8sutil: batch job rook-ceph-osd-prepare-node2 deleted
2023-02-17 00:55:46.711397 I | op-osd: started OSD provisioning job for node "node2"
2023-02-17 00:55:46.733434 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-node3 to start a new one
2023-02-17 00:55:46.746069 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node2-7bch7" is a ceph pod!
2023-02-17 00:55:46.746139 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:55:46.746780 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:55:46.767729 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-17 00:55:46.768374 I | op-k8sutil: batch job rook-ceph-osd-prepare-node3 still exists
2023-02-17 00:55:46.769334 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:55:46.775032 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:55:46.803693 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node3-l25sz" is a ceph pod!
2023-02-17 00:55:46.803738 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:55:46.807652 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:55:46.820623 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:55:46.821842 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:55:46.826938 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:55:47.085223 I | op-mgr: successfully set ceph dashboard creds
2023-02-17 00:55:47.085383 D | exec: Running command: ceph config get mgr.a mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:47.102554 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node1-status" cm is handled by another watcher
2023-02-17 00:55:47.102586 D | ceph-spec: object "rook-ceph-osd-node1-status" matched on update
2023-02-17 00:55:47.102595 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:55:47.492409 D | exec: Running command: ceph config get mgr.a mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:47.818973 D | exec: Running command: ceph config get mgr.a mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:48.164933 D | exec: Running command: ceph config get mgr.a mgr/dashboard/ssl_server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:48.223664 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node2-status" cm is handled by another watcher
2023-02-17 00:55:48.223696 D | ceph-spec: object "rook-ceph-osd-node2-status" matched on update
2023-02-17 00:55:48.223702 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:55:48.495532 D | exec: Running command: ceph config get mgr.b mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:48.792126 D | exec: Running command: ceph config get mgr.b mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:49.101938 D | exec: Running command: ceph config get mgr.b mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:49.398676 I | op-config: setting "mgr.b"="mgr/dashboard/server_port"="8443" option to the mon configuration database
2023-02-17 00:55:49.398703 D | exec: Running command: ceph config set mgr.b mgr/dashboard/server_port 8443 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:49.709221 I | op-config: successfully set "mgr.b"="mgr/dashboard/server_port"="8443" option to the mon configuration database
2023-02-17 00:55:49.709256 D | exec: Running command: ceph config get mgr.b mgr/dashboard/ssl_server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:49.776946 I | op-k8sutil: batch job rook-ceph-osd-prepare-node3 deleted
2023-02-17 00:55:49.785425 I | op-osd: started OSD provisioning job for node "node3"
2023-02-17 00:55:49.788770 I | op-osd: OSD orchestration status for node node1 is "completed"
2023-02-17 00:55:49.788804 D | op-osd: not creating deployment for OSD 3 which already exists
2023-02-17 00:55:49.788809 D | op-osd: not creating deployment for OSD 0 which already exists
2023-02-17 00:55:49.797846 D | ceph-spec: object "rook-ceph-osd-node1-status" did not match on delete
2023-02-17 00:55:49.797882 D | ceph-spec: do not reconcile on "rook-ceph-osd-node1-status" config map changes
2023-02-17 00:55:49.797892 D | ceph-spec: object "rook-ceph-osd-node1-status" did not match on delete
2023-02-17 00:55:49.797900 D | ceph-spec: object "rook-ceph-osd-node1-status" did not match on delete
2023-02-17 00:55:49.798212 I | op-osd: OSD orchestration status for node node2 is "orchestrating"
2023-02-17 00:55:49.798229 I | op-osd: OSD orchestration status for node node3 is "starting"
2023-02-17 00:55:49.799703 D | op-osd: not processing DELETED event for object "rook-ceph-osd-node1-status"
2023-02-17 00:55:49.819629 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node3-zg6nb" is a ceph pod!
2023-02-17 00:55:49.819676 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:55:49.820680 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:55:49.834421 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:55:49.835664 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:55:49.840592 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:55:49.885871 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node2-status" cm is handled by another watcher
2023-02-17 00:55:49.885900 D | ceph-spec: object "rook-ceph-osd-node2-status" matched on update
2023-02-17 00:55:49.885934 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:55:49.885960 I | op-osd: OSD orchestration status for node node2 is "completed"
2023-02-17 00:55:49.885971 D | op-osd: not creating deployment for OSD 1 which already exists
2023-02-17 00:55:49.885984 D | op-osd: not creating deployment for OSD 4 which already exists
2023-02-17 00:55:49.895220 D | ceph-spec: do not reconcile on "rook-ceph-osd-node2-status" config map changes
2023-02-17 00:55:49.895245 D | ceph-spec: object "rook-ceph-osd-node2-status" did not match on delete
2023-02-17 00:55:49.895251 D | ceph-spec: object "rook-ceph-osd-node2-status" did not match on delete
2023-02-17 00:55:49.895261 D | ceph-spec: object "rook-ceph-osd-node2-status" did not match on delete
2023-02-17 00:55:49.895660 D | op-osd: not processing DELETED event for object "rook-ceph-osd-node2-status"
2023-02-17 00:55:49.996090 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:50.045361 I | op-mgr: dashboard config has changed. restarting the dashboard module
2023-02-17 00:55:50.045388 I | op-mgr: restarting the mgr module
2023-02-17 00:55:50.045405 D | exec: Running command: ceph mgr module disable dashboard --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:50.345577 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:50.664966 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:50.724457 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:51.058432 D | exec: Running command: ceph osd ok-to-stop 0 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:55:51.086480 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node3-status" cm is handled by another watcher
2023-02-17 00:55:51.086507 D | ceph-spec: object "rook-ceph-osd-node3-status" matched on update
2023-02-17 00:55:51.086511 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:55:51.775359 I | op-mgr: successful modules: dashboard
2023-02-17 00:55:52.955034 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node3-status" cm is handled by another watcher
2023-02-17 00:55:52.955060 D | ceph-spec: object "rook-ceph-osd-node3-status" matched on update
2023-02-17 00:55:52.955064 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:56:04.556351 D | op-osd: updating OSDs: [0 1 2 3 4 5]
2023-02-17 00:56:04.566212 I | op-osd: updating OSD 0 on node "node1"
2023-02-17 00:56:04.566365 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:04.566373 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:04.566427 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 0 on node \"node1\""
2023-02-17 00:56:04.577405 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:04.577420 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:04.582491 I | op-osd: updating OSD 1 on node "node2"
2023-02-17 00:56:04.582929 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:04.582955 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:04.583131 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"node2\""
2023-02-17 00:56:04.599569 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:04.599585 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:04.604634 I | op-osd: updating OSD 2 on node "node3"
2023-02-17 00:56:04.604759 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:04.604765 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:04.604816 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 2 on node \"node3\""
2023-02-17 00:56:04.614472 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:04.614487 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:04.618424 I | op-osd: updating OSD 3 on node "node1"
2023-02-17 00:56:04.618555 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:04.618561 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:04.618639 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 3 on node \"node1\""
2023-02-17 00:56:04.631231 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:04.631248 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:04.637415 I | op-osd: updating OSD 4 on node "node2"
2023-02-17 00:56:04.637547 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:04.637553 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:04.637608 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 4 on node \"node2\""
2023-02-17 00:56:04.648876 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:04.648891 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:04.654247 I | op-osd: updating OSD 5 on node "node3"
2023-02-17 00:56:04.654421 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:04.654430 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:04.654484 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 5 on node \"node3\""
2023-02-17 00:56:04.664219 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:04.664250 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:04.670965 D | op-k8sutil: deployment "rook-ceph-osd-0" did not change. nothing to update
2023-02-17 00:56:04.679997 D | op-k8sutil: deployment "rook-ceph-osd-1" did not change. nothing to update
2023-02-17 00:56:04.688417 D | op-k8sutil: deployment "rook-ceph-osd-2" did not change. nothing to update
2023-02-17 00:56:04.697340 D | op-k8sutil: deployment "rook-ceph-osd-3" did not change. nothing to update
2023-02-17 00:56:04.766443 D | op-k8sutil: deployment "rook-ceph-osd-4" did not change. nothing to update
2023-02-17 00:56:04.966349 D | op-k8sutil: deployment "rook-ceph-osd-5" did not change. nothing to update
2023-02-17 00:56:05.171230 I | op-osd: OSD orchestration status for node node3 is "orchestrating"
2023-02-17 00:56:05.171474 I | op-osd: OSD orchestration status for node node3 is "completed"
2023-02-17 00:56:05.171499 D | op-osd: not creating deployment for OSD 2 which already exists
2023-02-17 00:56:05.171504 D | op-osd: not creating deployment for OSD 5 which already exists
2023-02-17 00:56:05.179689 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-17 00:56:05.179729 D | ceph-spec: do not reconcile on "rook-ceph-osd-node3-status" config map changes
2023-02-17 00:56:05.179759 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-17 00:56:05.179765 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-17 00:56:05.183364 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:05.556383 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:56:05.556416 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:56:05.556477 D | exec: Running command: ceph osd require-osd-release pacific --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:05.861931 D | cephclient: 
2023-02-17 00:56:05.861963 I | cephclient: successfully disallowed pre-pacific osds and enabled all new pacific-only functionality
2023-02-17 00:56:05.861971 I | op-osd: finished running OSDs in namespace "rook-ceph"
2023-02-17 00:56:05.861975 I | ceph-cluster-controller: done reconciling ceph cluster in namespace "rook-ceph"
2023-02-17 00:56:05.862018 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:56:05.873769 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "rook-ceph"
2023-02-17 00:56:05.873817 D | ceph-cluster-controller: monitoring routine for "osd" is already running
2023-02-17 00:56:05.873823 D | ceph-cluster-controller: monitoring routine for "status" is already running
2023-02-17 00:56:05.873843 D | ceph-cluster-controller: successfully configured CephCluster "rook-ceph/rook-ceph"
2023-02-17 00:56:05.873871 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:56:05.873890 I | ceph-cluster-controller: reporting cluster telemetry
2023-02-17 00:56:05.873902 D | op-config: setting "rook/version"="v1.9.9" option in the mon config-key store
2023-02-17 00:56:05.873916 D | exec: Running command: ceph config-key set rook/version v1.9.9 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:05.874018 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:05.874028 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:05.874236 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2023-02-17 00:56:05.879643 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2023-02-17 00:56:05.882701 I | ceph-spec: parsing mon endpoints: a=10.211.55.97:6789,b=10.211.55.99:6789
2023-02-17 00:56:05.882755 D | ceph-spec: loaded: maxMonID=1, mons=map[a:0xc0009fefa0 b:0xc0009fefe0], assignment=&{Schedule:map[a:0xc000753780 b:0xc0007537c0]}
2023-02-17 00:56:05.892991 D | ceph-cluster-controller: monitoring routine for "mon" is already running
2023-02-17 00:56:05.893020 D | ceph-cluster-controller: monitoring routine for "osd" is already running
2023-02-17 00:56:05.893027 D | ceph-cluster-controller: monitoring routine for "status" is already running
2023-02-17 00:56:05.898820 D | ceph-cluster-controller: cluster spec successfully validated
2023-02-17 00:56:05.898866 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Detecting Ceph version"
2023-02-17 00:56:05.910574 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v16.2.10...
2023-02-17 00:56:05.910699 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:05.910710 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:05.915072 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2023-02-17 00:56:06.321086 D | telemetry: set telemetry key: rook/version=v1.9.9
2023-02-17 00:56:06.322450 D | op-config: setting "rook/kubernetes/version"="v1.20.4-aliyun.1" option in the mon config-key store
2023-02-17 00:56:06.322484 D | exec: Running command: ceph config-key set rook/kubernetes/version v1.20.4-aliyun.1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:06.739108 D | telemetry: set telemetry key: rook/kubernetes/version=v1.20.4-aliyun.1
2023-02-17 00:56:06.739139 D | op-config: setting "rook/csi/version"="v3.6.2" option in the mon config-key store
2023-02-17 00:56:06.739153 D | exec: Running command: ceph config-key set rook/csi/version v3.6.2 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:07.142669 D | telemetry: set telemetry key: rook/csi/version=v3.6.2
2023-02-17 00:56:07.142711 D | op-config: setting "rook/cluster/mon/max-id"="1" option in the mon config-key store
2023-02-17 00:56:07.142732 D | exec: Running command: ceph config-key set rook/cluster/mon/max-id 1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:07.552513 D | telemetry: set telemetry key: rook/cluster/mon/max-id=1
2023-02-17 00:56:07.552542 D | op-config: setting "rook/cluster/mon/count"="2" option in the mon config-key store
2023-02-17 00:56:07.552556 D | exec: Running command: ceph config-key set rook/cluster/mon/count 2 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:07.751125 D | CmdReporter: job rook-ceph-detect-version has returned results
2023-02-17 00:56:07.777554 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-detect-version" cm is handled by another watcher
2023-02-17 00:56:07.778010 I | ceph-spec: detected ceph image version: "16.2.10-0 pacific"
2023-02-17 00:56:07.778035 I | ceph-cluster-controller: validating ceph version from provided image
2023-02-17 00:56:07.781195 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2023-02-17 00:56:07.784993 I | ceph-spec: parsing mon endpoints: a=10.211.55.97:6789,b=10.211.55.99:6789
2023-02-17 00:56:07.785042 D | ceph-spec: loaded: maxMonID=1, mons=map[a:0xc000968b00 b:0xc000968b40], assignment=&{Schedule:map[a:0xc0006e6480 b:0xc0006e64c0]}
2023-02-17 00:56:07.788103 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2023-02-17 00:56:07.788374 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2023-02-17 00:56:07.788410 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:07.963789 D | telemetry: set telemetry key: rook/cluster/mon/count=2
2023-02-17 00:56:07.963811 D | op-config: setting "rook/cluster/mon/allow-multiple-per-node"="false" option in the mon config-key store
2023-02-17 00:56:07.963825 D | exec: Running command: ceph config-key set rook/cluster/mon/allow-multiple-per-node false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:08.166019 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:56:08.166053 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:56:08.166097 D | ceph-cluster-controller: both cluster and image spec versions are identical, doing nothing 16.2.10-0 pacific
2023-02-17 00:56:08.166104 I | ceph-cluster-controller: cluster "rook-ceph": version "16.2.10-0 pacific" detected for image "quay.io/ceph/ceph:v16.2.10"
2023-02-17 00:56:08.180724 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring the Ceph cluster"
2023-02-17 00:56:08.190693 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:08.190723 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:08.196857 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2023-02-17 00:56:08.196913 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mons"
2023-02-17 00:56:08.206081 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:56:08.206097 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:56:08.206100 I | op-mon: start running mons
2023-02-17 00:56:08.206102 D | op-mon: establishing ceph cluster info
2023-02-17 00:56:08.206288 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:08.206298 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:08.209608 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2023-02-17 00:56:08.212344 I | ceph-spec: parsing mon endpoints: a=10.211.55.97:6789,b=10.211.55.99:6789
2023-02-17 00:56:08.212392 D | ceph-spec: loaded: maxMonID=1, mons=map[a:0xc000a98f40 b:0xc000a98f80], assignment=&{Schedule:map[a:0xc0002f6900 b:0xc0002f6940]}
2023-02-17 00:56:08.230189 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2023-02-17 00:56:08.356187 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.211.55.97:6789","10.211.55.99:6789"],"namespace":""}] data:a=10.211.55.97:6789,b=10.211.55.99:6789 mapping:{"node":{"a":{"Name":"node1","Hostname":"node1","Address":"10.211.55.97"},"b":{"Name":"node3","Hostname":"node3","Address":"10.211.55.99"}}} maxMonId:1]
2023-02-17 00:56:08.390215 D | telemetry: set telemetry key: rook/cluster/mon/allow-multiple-per-node=false
2023-02-17 00:56:08.390245 D | op-config: setting "rook/cluster/mon/pvc/enabled"="false" option in the mon config-key store
2023-02-17 00:56:08.390259 D | exec: Running command: ceph config-key set rook/cluster/mon/pvc/enabled false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:08.539858 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:56:08.539889 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:56:08.539900 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:56:08.539910 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2023-02-17 00:56:08.559349 D | op-config: updating config secret "rook-ceph-config"
2023-02-17 00:56:08.808915 D | telemetry: set telemetry key: rook/cluster/mon/pvc/enabled=false
2023-02-17 00:56:08.808946 D | op-config: setting "rook/cluster/mon/stretch/enabled"="false" option in the mon config-key store
2023-02-17 00:56:08.808969 D | exec: Running command: ceph config-key set rook/cluster/mon/stretch/enabled false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:08.955956 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2023-02-17 00:56:08.956143 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2023-02-17 00:56:08.956166 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2023-02-17 00:56:09.215705 D | telemetry: set telemetry key: rook/cluster/mon/stretch/enabled=false
2023-02-17 00:56:09.215736 D | op-config: setting "rook/cluster/storage/device-set/count/total"="0" option in the mon config-key store
2023-02-17 00:56:09.215751 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/total 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:09.554345 D | op-cfg-keyring: updating secret for rook-ceph-mons-keyring
2023-02-17 00:56:09.608785 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/total=0
2023-02-17 00:56:09.608816 D | op-config: setting "rook/cluster/storage/device-set/count/portable"="0" option in the mon config-key store
2023-02-17 00:56:09.608830 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/portable 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:09.957516 D | op-cfg-keyring: updating secret for rook-ceph-admin-keyring
2023-02-17 00:56:10.005558 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/portable=0
2023-02-17 00:56:10.005576 D | op-config: setting "rook/cluster/storage/device-set/count/non-portable"="0" option in the mon config-key store
2023-02-17 00:56:10.005587 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/non-portable 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:10.400562 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/non-portable=0
2023-02-17 00:56:10.400595 D | op-config: setting "rook/cluster/network/provider"="host" option in the mon config-key store
2023-02-17 00:56:10.400608 D | exec: Running command: ceph config-key set rook/cluster/network/provider host --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:10.556476 I | op-mon: targeting the mon count 2
2023-02-17 00:56:10.563351 D | op-mon: mon a already scheduled
2023-02-17 00:56:10.563380 D | op-mon: mon b already scheduled
2023-02-17 00:56:10.563385 D | op-mon: mons have been scheduled
2023-02-17 00:56:10.568283 I | op-config: setting "global"="mon allow pool delete"="true" option to the mon configuration database
2023-02-17 00:56:10.568316 D | exec: Running command: ceph config set global mon_allow_pool_delete true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:10.800707 D | telemetry: set telemetry key: rook/cluster/network/provider=host
2023-02-17 00:56:10.800764 D | op-config: setting "rook/cluster/external-mode"="false" option in the mon config-key store
2023-02-17 00:56:10.800775 D | exec: Running command: ceph config-key set rook/cluster/external-mode false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:10.873927 I | op-config: successfully set "global"="mon allow pool delete"="true" option to the mon configuration database
2023-02-17 00:56:10.873945 I | op-config: setting "global"="mon cluster log file"="" option to the mon configuration database
2023-02-17 00:56:10.873958 D | exec: Running command: ceph config set global mon_cluster_log_file  --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:11.195581 I | op-config: successfully set "global"="mon cluster log file"="" option to the mon configuration database
2023-02-17 00:56:11.195611 I | op-config: setting "global"="mon allow pool size one"="true" option to the mon configuration database
2023-02-17 00:56:11.195627 D | exec: Running command: ceph config set global mon_allow_pool_size_one true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:11.219021 D | telemetry: set telemetry key: rook/cluster/external-mode=false
2023-02-17 00:56:11.510356 I | op-config: successfully set "global"="mon allow pool size one"="true" option to the mon configuration database
2023-02-17 00:56:11.510374 I | op-config: setting "global"="osd scrub auto repair"="true" option to the mon configuration database
2023-02-17 00:56:11.510389 D | exec: Running command: ceph config set global osd_scrub_auto_repair true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:11.829739 I | op-config: successfully set "global"="osd scrub auto repair"="true" option to the mon configuration database
2023-02-17 00:56:11.829767 I | op-config: setting "global"="log to file"="false" option to the mon configuration database
2023-02-17 00:56:11.829782 D | exec: Running command: ceph config set global log_to_file false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:12.133863 I | op-config: successfully set "global"="log to file"="false" option to the mon configuration database
2023-02-17 00:56:12.133882 I | op-config: deleting "log file" option from the mon configuration database
2023-02-17 00:56:12.134123 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:12.443880 I | op-config: successfully deleted "log file" option from the mon configuration database
2023-02-17 00:56:12.443908 I | op-mon: checking for basic quorum with existing mons
2023-02-17 00:56:12.443915 I | op-mon: setting mon endpoints for hostnetwork mode
2023-02-17 00:56:12.443921 I | op-mon: setting mon endpoints for hostnetwork mode
2023-02-17 00:56:12.457911 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2023-02-17 00:56:12.461378 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.211.55.97:6789","10.211.55.99:6789"],"namespace":""}] data:a=10.211.55.97:6789,b=10.211.55.99:6789 mapping:{"node":{"a":{"Name":"node1","Hostname":"node1","Address":"10.211.55.97"},"b":{"Name":"node3","Hostname":"node3","Address":"10.211.55.99"}}} maxMonId:1]
2023-02-17 00:56:12.463948 D | op-config: updating config secret "rook-ceph-config"
2023-02-17 00:56:12.469036 D | ceph-spec: object "rook-ceph-config" matched on update
2023-02-17 00:56:12.469073 D | ceph-spec: do not reconcile on "rook-ceph-config" secret changes
2023-02-17 00:56:12.473247 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2023-02-17 00:56:12.473487 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2023-02-17 00:56:12.473513 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2023-02-17 00:56:12.493966 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.211.55.97 Port:6789 Zone: DataPathMap:0xc0011ca060}
2023-02-17 00:56:12.499544 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2023-02-17 00:56:12.499576 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2023-02-17 00:56:12.506397 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2023-02-17 00:56:12.506427 I | op-mon: waiting for mon quorum with [a b]
2023-02-17 00:56:12.557761 I | op-mon: mons running: [a b]
2023-02-17 00:56:12.557797 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:12.922454 I | op-mon: Monitors in quorum: [a b]
2023-02-17 00:56:12.922544 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-b DaemonName:b PublicIP:10.211.55.99 Port:6789 Zone: DataPathMap:0xc0011ca090}
2023-02-17 00:56:12.928982 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-b
2023-02-17 00:56:12.929013 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2023-02-17 00:56:12.937835 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2023-02-17 00:56:12.937871 I | op-mon: waiting for mon quorum with [a b]
2023-02-17 00:56:12.960438 I | op-mon: mons running: [a b]
2023-02-17 00:56:12.960473 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:13.332922 I | op-mon: Monitors in quorum: [a b]
2023-02-17 00:56:13.332941 I | op-mon: mons created: 2
2023-02-17 00:56:13.332954 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:13.717826 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:56:13.717844 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:56:13.717908 I | op-mon: waiting for mon quorum with [a b]
2023-02-17 00:56:13.748552 I | op-mon: mons running: [a b]
2023-02-17 00:56:13.748591 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:14.112094 I | op-mon: Monitors in quorum: [a b]
2023-02-17 00:56:14.112126 D | op-mon: mon endpoints used are: a=10.211.55.97:6789,b=10.211.55.99:6789
2023-02-17 00:56:14.112132 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2023-02-17 00:56:14.112134 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2023-02-17 00:56:14.112137 D | op-mon: Released lock for mon orchestration
2023-02-17 00:56:14.112140 D | ceph-cluster-controller: monitors are up and running, executing post actions
2023-02-17 00:56:14.112145 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2023-02-17 00:56:14.112159 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:14.481409 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2023-02-17 00:56:14.481431 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:14.855339 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2023-02-17 00:56:14.855362 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r mgr allow rw osd allow rw tag cephfs metadata=* --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:15.217339 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2023-02-17 00:56:15.217377 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:15.590200 D | op-cfg-keyring: updating secret for rook-csi-rbd-provisioner
2023-02-17 00:56:15.597658 D | op-cfg-keyring: updating secret for rook-csi-rbd-node
2023-02-17 00:56:15.604124 D | op-cfg-keyring: updating secret for rook-csi-cephfs-provisioner
2023-02-17 00:56:15.611419 D | op-cfg-keyring: updating secret for rook-csi-cephfs-node
2023-02-17 00:56:15.614769 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2023-02-17 00:56:15.614801 I | cephclient: getting or creating ceph auth key "client.crash"
2023-02-17 00:56:15.614818 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:15.980204 D | op-cfg-keyring: updating secret for rook-ceph-crash-collector-keyring
2023-02-17 00:56:15.984349 I | ceph-crashcollector-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2023-02-17 00:56:15.984376 I | op-config: deleting "mon_mds_skip_sanity" option from the mon configuration database
2023-02-17 00:56:15.984391 D | exec: Running command: ceph config rm mon mon_mds_skip_sanity --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:16.275902 I | op-config: successfully deleted "mon_mds_skip_sanity" option from the mon configuration database
2023-02-17 00:56:16.275930 I | ceph-cluster-controller: setting msgr2 encryption mode to "crc secure"
2023-02-17 00:56:16.275936 I | op-config: setting "global"="ms_cluster_mode"="crc secure" option to the mon configuration database
2023-02-17 00:56:16.275949 D | exec: Running command: ceph config set global ms_cluster_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:16.584333 I | op-config: successfully set "global"="ms_cluster_mode"="crc secure" option to the mon configuration database
2023-02-17 00:56:16.584360 I | op-config: setting "global"="ms_service_mode"="crc secure" option to the mon configuration database
2023-02-17 00:56:16.584373 D | exec: Running command: ceph config set global ms_service_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:16.899667 I | op-config: successfully set "global"="ms_service_mode"="crc secure" option to the mon configuration database
2023-02-17 00:56:16.899702 I | op-config: setting "global"="ms_client_mode"="crc secure" option to the mon configuration database
2023-02-17 00:56:16.899716 D | exec: Running command: ceph config set global ms_client_mode crc secure --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:17.210345 I | op-config: successfully set "global"="ms_client_mode"="crc secure" option to the mon configuration database
2023-02-17 00:56:17.210376 W | ceph-cluster-controller: network compression requires Ceph Quincy (v17) or newer, skipping for current ceph "16.2.10-0 pacific"
2023-02-17 00:56:17.210384 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2023-02-17 00:56:17.210386 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2023-02-17 00:56:17.210398 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:17.576663 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2023-02-17 00:56:17.576753 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-rook-ceph" in namespace "rook-ceph"
2023-02-17 00:56:17.576763 D | op-k8sutil: creating secret cluster-peer-token-rook-ceph
2023-02-17 00:56:17.587583 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mgr(s)"
2023-02-17 00:56:17.597421 I | op-mgr: start running mgr
2023-02-17 00:56:17.597458 I | cephclient: getting or creating ceph auth key "mgr.a"
2023-02-17 00:56:17.597470 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:17.597625 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:17.597638 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:17.975089 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2023-02-17 00:56:17.978904 D | op-cfg-keyring: updating secret for rook-ceph-mgr-a-keyring
2023-02-17 00:56:17.982945 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc000e0d9e0}
2023-02-17 00:56:17.992766 I | op-mgr: deployment for mgr rook-ceph-mgr-a already exists. updating if needed
2023-02-17 00:56:18.000470 I | op-k8sutil: deployment "rook-ceph-mgr-a" did not change, nothing to update
2023-02-17 00:56:18.000494 I | cephclient: getting or creating ceph auth key "mgr.b"
2023-02-17 00:56:18.000503 D | exec: Running command: ceph auth get-or-create-key mgr.b mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:18.379377 D | op-mgr: legacy mgr key "rook-ceph-mgr-b" is already removed
2023-02-17 00:56:18.382677 D | op-cfg-keyring: updating secret for rook-ceph-mgr-b-keyring
2023-02-17 00:56:18.386667 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-b DaemonID:b DataPathMap:0xc0016678f0}
2023-02-17 00:56:18.400004 I | op-mgr: deployment for mgr rook-ceph-mgr-b already exists. updating if needed
2023-02-17 00:56:18.407959 I | op-k8sutil: deployment "rook-ceph-mgr-b" did not change, nothing to update
2023-02-17 00:56:18.408002 D | exec: Running command: ceph mgr stat --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:18.757450 D | op-k8sutil: kubernetes version fetched 1.20.4-aliyun.1
2023-02-17 00:56:18.757552 I | op-mgr: setting services to point to mgr "a"
2023-02-17 00:56:18.757566 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2023-02-17 00:56:18.773339 D | op-k8sutil: updating service rook-ceph-mgr-dashboard
2023-02-17 00:56:18.782047 D | op-k8sutil: creating service rook-ceph-mgr
2023-02-17 00:56:18.796520 D | op-k8sutil: updating service rook-ceph-mgr
2023-02-17 00:56:18.809322 I | op-mgr: no need to update service "rook-ceph-mgr"
2023-02-17 00:56:18.809351 I | op-mgr: no need to update service "rook-ceph-mgr-dashboard"
2023-02-17 00:56:18.809406 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph OSDs"
2023-02-17 00:56:18.809419 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:18.809520 D | cephclient: balancer module is already 'on' on pacific, doing nothingbalancer
2023-02-17 00:56:18.809542 I | op-mgr: successful modules: balancer
2023-02-17 00:56:18.809560 D | exec: Running command: ceph mgr module enable pg_autoscaler --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:18.809575 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:18.820529 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:18.820546 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:18.821765 I | op-osd: start running osds in namespace "rook-ceph"
2023-02-17 00:56:18.821792 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2023-02-17 00:56:18.846398 D | op-osd: 6 of 6 OSD Deployments need updated
2023-02-17 00:56:18.846414 I | op-osd: start provisioning the OSDs on PVCs, if needed
2023-02-17 00:56:18.850063 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2023-02-17 00:56:18.850083 I | op-osd: start provisioning the OSDs on nodes, if needed
2023-02-17 00:56:18.991311 D | op-osd: storage nodes: [{Name:node1 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}} {Name:node2 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}} {Name:node3 Resources:{Limits:map[] Requests:map[]} Config:map[] Selection:{UseAllDevices:<nil> DeviceFilter: DevicePathFilter: Devices:[] VolumeClaimTemplates:[]}}]
2023-02-17 00:56:19.192123 I | op-osd: 3 of the 3 storage nodes are valid
2023-02-17 00:56:19.547396 I | op-mgr: successful modules: prometheus
2023-02-17 00:56:19.554204 I | op-config: setting "global"="osd_pool_default_pg_autoscale_mode"="on" option to the mon configuration database
2023-02-17 00:56:19.554243 D | exec: Running command: ceph config set global osd_pool_default_pg_autoscale_mode on --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:19.595895 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-node1 to start a new one
2023-02-17 00:56:19.609081 I | op-k8sutil: batch job rook-ceph-osd-prepare-node1 still exists
2023-02-17 00:56:19.629640 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node1-759lj" is a ceph pod!
2023-02-17 00:56:19.629698 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:56:19.630064 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:56:19.640003 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:56:19.641024 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:56:19.644809 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:56:19.871743 I | op-config: successfully set "global"="osd_pool_default_pg_autoscale_mode"="on" option to the mon configuration database
2023-02-17 00:56:19.871762 I | op-config: setting "global"="mon_pg_warn_min_per_osd"="0" option to the mon configuration database
2023-02-17 00:56:19.871776 D | exec: Running command: ceph config set global mon_pg_warn_min_per_osd 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:20.172166 I | op-config: successfully set "global"="mon_pg_warn_min_per_osd"="0" option to the mon configuration database
2023-02-17 00:56:20.172198 I | op-mgr: successful modules: mgr module(s) from the spec
2023-02-17 00:56:22.616132 I | op-k8sutil: batch job rook-ceph-osd-prepare-node1 deleted
2023-02-17 00:56:22.624132 I | op-osd: started OSD provisioning job for node "node1"
2023-02-17 00:56:22.647135 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-node2 to start a new one
2023-02-17 00:56:22.670689 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node1-wht67" is a ceph pod!
2023-02-17 00:56:22.670764 D | ceph-crashcollector-controller: reconciling node: "node1"
2023-02-17 00:56:22.671480 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:56:22.673788 I | op-k8sutil: batch job rook-ceph-osd-prepare-node2 still exists
2023-02-17 00:56:22.680404 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node1". operation: "updated"
2023-02-17 00:56:22.684692 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:56:22.689248 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:56:22.705460 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node2-7bch7" is a ceph pod!
2023-02-17 00:56:22.705522 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:56:22.705989 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:56:22.716093 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-17 00:56:22.717466 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:56:22.721585 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:56:23.928076 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node1-status" cm is handled by another watcher
2023-02-17 00:56:23.928123 D | ceph-spec: object "rook-ceph-osd-node1-status" matched on update
2023-02-17 00:56:23.928149 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:56:24.553254 I | op-mgr: the dashboard secret was already generated
2023-02-17 00:56:24.553325 D | exec: Running command: ceph dashboard create-self-signed-cert --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:24.956663 I | op-mgr: setting ceph dashboard "admin" login creds
2023-02-17 00:56:24.956825 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/2475383630 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:25.278549 I | op-mgr: successfully set ceph dashboard creds
2023-02-17 00:56:25.278653 D | exec: Running command: ceph config get mgr.a mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:25.581473 D | exec: Running command: ceph config get mgr.a mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:25.585532 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node1-status" cm is handled by another watcher
2023-02-17 00:56:25.585595 D | ceph-spec: object "rook-ceph-osd-node1-status" matched on update
2023-02-17 00:56:25.585604 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:56:25.680119 I | op-k8sutil: batch job rook-ceph-osd-prepare-node2 deleted
2023-02-17 00:56:25.689191 I | op-osd: started OSD provisioning job for node "node2"
2023-02-17 00:56:25.705806 I | op-k8sutil: Removing previous job rook-ceph-osd-prepare-node3 to start a new one
2023-02-17 00:56:25.724548 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node2-swb58" is a ceph pod!
2023-02-17 00:56:25.724590 D | ceph-crashcollector-controller: reconciling node: "node2"
2023-02-17 00:56:25.728442 I | op-k8sutil: batch job rook-ceph-osd-prepare-node3 still exists
2023-02-17 00:56:25.729127 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:56:25.747032 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node2". operation: "updated"
2023-02-17 00:56:25.748480 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:56:25.753335 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:56:25.930633 D | exec: Running command: ceph config get mgr.a mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:26.237416 D | exec: Running command: ceph config get mgr.a mgr/dashboard/ssl_server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:26.498501 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node3-zg6nb" is a ceph pod!
2023-02-17 00:56:26.498571 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:56:26.498952 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:56:26.509362 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:56:26.510422 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:56:26.512969 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:56:26.555154 D | exec: Running command: ceph config get mgr.b mgr/dashboard/url_prefix --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:26.873122 D | exec: Running command: ceph config get mgr.b mgr/dashboard/ssl --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:26.935558 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node2-status" cm is handled by another watcher
2023-02-17 00:56:26.935599 D | ceph-spec: object "rook-ceph-osd-node2-status" matched on update
2023-02-17 00:56:26.935626 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:56:27.196908 D | exec: Running command: ceph config get mgr.b mgr/dashboard/server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:27.503930 D | exec: Running command: ceph config get mgr.b mgr/dashboard/ssl_server_port --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:27.805007 I | op-mgr: successful modules: dashboard
2023-02-17 00:56:28.588450 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node2-status" cm is handled by another watcher
2023-02-17 00:56:28.588540 D | ceph-spec: object "rook-ceph-osd-node2-status" matched on update
2023-02-17 00:56:28.588553 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:56:28.735736 I | op-k8sutil: batch job rook-ceph-osd-prepare-node3 deleted
2023-02-17 00:56:28.742586 I | op-osd: started OSD provisioning job for node "node3"
2023-02-17 00:56:28.746717 I | op-osd: OSD orchestration status for node node1 is "completed"
2023-02-17 00:56:28.746748 D | op-osd: not creating deployment for OSD 3 which already exists
2023-02-17 00:56:28.746753 D | op-osd: not creating deployment for OSD 0 which already exists
2023-02-17 00:56:28.755625 D | ceph-spec: object "rook-ceph-osd-node1-status" did not match on delete
2023-02-17 00:56:28.755644 D | ceph-spec: object "rook-ceph-osd-node1-status" did not match on delete
2023-02-17 00:56:28.755657 D | ceph-spec: object "rook-ceph-osd-node1-status" did not match on delete
2023-02-17 00:56:28.755662 D | ceph-spec: do not reconcile on "rook-ceph-osd-node1-status" config map changes
2023-02-17 00:56:28.756094 I | op-osd: OSD orchestration status for node node2 is "completed"
2023-02-17 00:56:28.756126 D | op-osd: not creating deployment for OSD 1 which already exists
2023-02-17 00:56:28.756170 D | op-osd: not creating deployment for OSD 4 which already exists
2023-02-17 00:56:28.765623 D | ceph-spec: object "rook-ceph-osd-node2-status" did not match on delete
2023-02-17 00:56:28.765654 D | ceph-spec: do not reconcile on "rook-ceph-osd-node2-status" config map changes
2023-02-17 00:56:28.765664 D | ceph-spec: object "rook-ceph-osd-node2-status" did not match on delete
2023-02-17 00:56:28.765677 D | ceph-spec: object "rook-ceph-osd-node2-status" did not match on delete
2023-02-17 00:56:28.767571 I | op-osd: OSD orchestration status for node node3 is "starting"
2023-02-17 00:56:28.769484 D | op-osd: not processing DELETED event for object "rook-ceph-osd-node1-status"
2023-02-17 00:56:28.769517 D | op-osd: not processing DELETED event for object "rook-ceph-osd-node2-status"
2023-02-17 00:56:28.769831 D | ceph-crashcollector-controller: "rook-ceph-osd-prepare-node3-5pzwn" is a ceph pod!
2023-02-17 00:56:28.769856 D | ceph-crashcollector-controller: reconciling node: "node3"
2023-02-17 00:56:28.770172 D | ceph-spec: ceph version found "16.2.10-0"
2023-02-17 00:56:28.782092 D | ceph-crashcollector-controller: deployment successfully reconciled for node "node3". operation: "updated"
2023-02-17 00:56:28.783105 D | ceph-crashcollector-controller: deleting cronjob if it exists...
2023-02-17 00:56:28.786367 D | ceph-crashcollector-controller: cronJob resource not found. Ignoring since object must be deleted.
2023-02-17 00:56:28.869692 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:28.999208 D | op-osd: checking osd processes status.
2023-02-17 00:56:28.999244 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:29.235882 D | exec: Running command: ceph osd tree --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:29.379610 D | op-osd: validating status of osd.0
2023-02-17 00:56:29.379638 D | op-osd: osd.0 is healthy.
2023-02-17 00:56:29.379643 D | op-osd: validating status of osd.1
2023-02-17 00:56:29.379645 D | op-osd: osd.1 is healthy.
2023-02-17 00:56:29.379647 D | op-osd: validating status of osd.2
2023-02-17 00:56:29.379649 D | op-osd: osd.2 is healthy.
2023-02-17 00:56:29.379651 D | op-osd: validating status of osd.3
2023-02-17 00:56:29.379652 D | op-osd: osd.3 is healthy.
2023-02-17 00:56:29.379654 D | op-osd: validating status of osd.4
2023-02-17 00:56:29.379656 D | op-osd: osd.4 is healthy.
2023-02-17 00:56:29.379658 D | op-osd: validating status of osd.5
2023-02-17 00:56:29.379660 D | op-osd: osd.5 is healthy.
2023-02-17 00:56:29.379672 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:29.567417 D | exec: Running command: ceph osd ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:29.873500 D | exec: Running command: ceph osd ok-to-stop 0 --max=20 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:30.166946 D | op-osd: updating OSDs: [0 1 2 3 4 5]
2023-02-17 00:56:30.173881 I | op-osd: updating OSD 0 on node "node1"
2023-02-17 00:56:30.174009 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:30.174032 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:30.174090 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 0 on node \"node1\""
2023-02-17 00:56:30.184213 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:30.184229 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:30.188023 I | op-osd: updating OSD 1 on node "node2"
2023-02-17 00:56:30.188153 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:30.188161 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:30.188209 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 1 on node \"node2\""
2023-02-17 00:56:30.198837 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:30.198854 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:30.204447 I | op-osd: updating OSD 2 on node "node3"
2023-02-17 00:56:30.204585 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:30.204607 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:30.204664 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 2 on node \"node3\""
2023-02-17 00:56:30.213968 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:30.213995 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:30.218429 I | op-osd: updating OSD 3 on node "node1"
2023-02-17 00:56:30.218552 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:56:30.218581 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:56:30.218749 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:30.218775 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:30.218858 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 3 on node \"node1\""
2023-02-17 00:56:30.228845 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:30.228861 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:30.232534 I | op-osd: updating OSD 4 on node "node2"
2023-02-17 00:56:30.232659 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:30.232665 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:30.232718 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 4 on node \"node2\""
2023-02-17 00:56:30.242556 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:30.242584 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:30.247908 I | op-osd: updating OSD 5 on node "node3"
2023-02-17 00:56:30.248034 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2023-02-17 00:56:30.248040 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2023-02-17 00:56:30.248097 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 5 on node \"node3\""
2023-02-17 00:56:30.257985 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:30.258003 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:30.266320 D | op-k8sutil: deployment "rook-ceph-osd-0" did not change. nothing to update
2023-02-17 00:56:30.275461 D | op-k8sutil: deployment "rook-ceph-osd-1" did not change. nothing to update
2023-02-17 00:56:30.284019 D | op-k8sutil: deployment "rook-ceph-osd-2" did not change. nothing to update
2023-02-17 00:56:30.292536 D | op-k8sutil: deployment "rook-ceph-osd-3" did not change. nothing to update
2023-02-17 00:56:30.380547 D | op-k8sutil: deployment "rook-ceph-osd-4" did not change. nothing to update
2023-02-17 00:56:30.522939 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node3-status" cm is handled by another watcher
2023-02-17 00:56:30.522971 D | ceph-spec: object "rook-ceph-osd-node3-status" matched on update
2023-02-17 00:56:30.523041 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:56:30.576614 D | op-k8sutil: deployment "rook-ceph-osd-5" did not change. nothing to update
2023-02-17 00:56:30.629985 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:56:30.636520 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:30.783686 I | op-osd: OSD orchestration status for node node3 is "orchestrating"
2023-02-17 00:56:31.001620 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:56:31.001638 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:56:31.001701 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:56:31.001711 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:56:31.012954 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:31.012985 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:32.164570 D | ceph-cluster-controller: hot-plug cm watcher: only reconcile on hot plug cm changes, this "rook-ceph-osd-node3-status" cm is handled by another watcher
2023-02-17 00:56:32.164625 D | ceph-spec: object "rook-ceph-osd-node3-status" matched on update
2023-02-17 00:56:32.164692 D | ceph-spec: do not reconcile on configmap that is not "rook-config-override"
2023-02-17 00:56:32.164764 I | op-osd: OSD orchestration status for node node3 is "completed"
2023-02-17 00:56:32.164776 D | op-osd: not creating deployment for OSD 5 which already exists
2023-02-17 00:56:32.164779 D | op-osd: not creating deployment for OSD 2 which already exists
2023-02-17 00:56:32.175525 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-17 00:56:32.175548 D | ceph-spec: do not reconcile on "rook-ceph-osd-node3-status" config map changes
2023-02-17 00:56:32.175555 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-17 00:56:32.175561 D | ceph-spec: object "rook-ceph-osd-node3-status" did not match on delete
2023-02-17 00:56:32.180147 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:32.560177 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:56:32.560207 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:56:32.560293 D | exec: Running command: ceph osd require-osd-release pacific --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:32.864620 D | cephclient: 
2023-02-17 00:56:32.864637 I | cephclient: successfully disallowed pre-pacific osds and enabled all new pacific-only functionality
2023-02-17 00:56:32.864644 I | op-osd: finished running OSDs in namespace "rook-ceph"
2023-02-17 00:56:32.864648 I | ceph-cluster-controller: done reconciling ceph cluster in namespace "rook-ceph"
2023-02-17 00:56:32.864698 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:56:32.875161 D | ceph-cluster-controller: monitoring routine for "mon" is already running
2023-02-17 00:56:32.875203 D | ceph-cluster-controller: monitoring routine for "osd" is already running
2023-02-17 00:56:32.875209 D | ceph-cluster-controller: monitoring routine for "status" is already running
2023-02-17 00:56:32.875245 D | ceph-cluster-controller: successfully configured CephCluster "rook-ceph/rook-ceph"
2023-02-17 00:56:32.875296 I | ceph-cluster-controller: reporting cluster telemetry
2023-02-17 00:56:32.875335 D | op-config: setting "rook/version"="v1.9.9" option in the mon config-key store
2023-02-17 00:56:32.875348 D | exec: Running command: ceph config-key set rook/version v1.9.9 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:32.877234 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:56:32.877258 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:56:33.288128 D | telemetry: set telemetry key: rook/version=v1.9.9
2023-02-17 00:56:33.290335 D | op-config: setting "rook/kubernetes/version"="v1.20.4-aliyun.1" option in the mon config-key store
2023-02-17 00:56:33.290368 D | exec: Running command: ceph config-key set rook/kubernetes/version v1.20.4-aliyun.1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:33.683725 D | telemetry: set telemetry key: rook/kubernetes/version=v1.20.4-aliyun.1
2023-02-17 00:56:33.683756 D | op-config: setting "rook/csi/version"="v3.6.2" option in the mon config-key store
2023-02-17 00:56:33.683770 D | exec: Running command: ceph config-key set rook/csi/version v3.6.2 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:34.088549 D | telemetry: set telemetry key: rook/csi/version=v3.6.2
2023-02-17 00:56:34.088569 D | op-config: setting "rook/cluster/mon/max-id"="1" option in the mon config-key store
2023-02-17 00:56:34.088581 D | exec: Running command: ceph config-key set rook/cluster/mon/max-id 1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:34.496284 D | telemetry: set telemetry key: rook/cluster/mon/max-id=1
2023-02-17 00:56:34.496302 D | op-config: setting "rook/cluster/mon/count"="2" option in the mon config-key store
2023-02-17 00:56:34.496314 D | exec: Running command: ceph config-key set rook/cluster/mon/count 2 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:34.888246 D | telemetry: set telemetry key: rook/cluster/mon/count=2
2023-02-17 00:56:34.888292 D | op-config: setting "rook/cluster/mon/allow-multiple-per-node"="false" option in the mon config-key store
2023-02-17 00:56:34.888315 D | exec: Running command: ceph config-key set rook/cluster/mon/allow-multiple-per-node false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:35.277016 D | telemetry: set telemetry key: rook/cluster/mon/allow-multiple-per-node=false
2023-02-17 00:56:35.277034 D | op-config: setting "rook/cluster/mon/pvc/enabled"="false" option in the mon config-key store
2023-02-17 00:56:35.277047 D | exec: Running command: ceph config-key set rook/cluster/mon/pvc/enabled false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:35.664491 D | telemetry: set telemetry key: rook/cluster/mon/pvc/enabled=false
2023-02-17 00:56:35.664514 D | op-config: setting "rook/cluster/mon/stretch/enabled"="false" option in the mon config-key store
2023-02-17 00:56:35.664525 D | exec: Running command: ceph config-key set rook/cluster/mon/stretch/enabled false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:36.058243 D | telemetry: set telemetry key: rook/cluster/mon/stretch/enabled=false
2023-02-17 00:56:36.058287 D | op-config: setting "rook/cluster/storage/device-set/count/total"="0" option in the mon config-key store
2023-02-17 00:56:36.058309 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/total 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:36.443394 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/total=0
2023-02-17 00:56:36.443427 D | op-config: setting "rook/cluster/storage/device-set/count/portable"="0" option in the mon config-key store
2023-02-17 00:56:36.443441 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/portable 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:36.841573 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/portable=0
2023-02-17 00:56:36.841592 D | op-config: setting "rook/cluster/storage/device-set/count/non-portable"="0" option in the mon config-key store
2023-02-17 00:56:36.841604 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/non-portable 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:37.242885 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/non-portable=0
2023-02-17 00:56:37.242912 D | op-config: setting "rook/cluster/network/provider"="host" option in the mon config-key store
2023-02-17 00:56:37.242926 D | exec: Running command: ceph config-key set rook/cluster/network/provider host --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:37.640997 D | telemetry: set telemetry key: rook/cluster/network/provider=host
2023-02-17 00:56:37.641027 D | op-config: setting "rook/cluster/external-mode"="false" option in the mon config-key store
2023-02-17 00:56:37.641040 D | exec: Running command: ceph config-key set rook/cluster/external-mode false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:38.057194 D | telemetry: set telemetry key: rook/cluster/external-mode=false
2023-02-17 00:56:50.874164 D | op-mon: checking health of mons
2023-02-17 00:56:50.874204 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:56:50.874227 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:56:50.885415 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 00:56:50.885451 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:56:51.238583 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 00:56:51.238608 D | op-mon: targeting the mon count 2
2023-02-17 00:56:51.238615 D | op-mon: mon "a" found in quorum
2023-02-17 00:56:51.238618 D | op-mon: mon "b" found in quorum
2023-02-17 00:56:51.238620 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 00:56:51.246776 D | op-mon: Released lock for mon orchestration
2023-02-17 00:56:51.246808 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:57:29.714223 D | op-osd: checking osd processes status.
2023-02-17 00:57:29.714286 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:57:30.014852 D | op-osd: validating status of osd.0
2023-02-17 00:57:30.014869 D | op-osd: osd.0 is healthy.
2023-02-17 00:57:30.014871 D | op-osd: validating status of osd.1
2023-02-17 00:57:30.014874 D | op-osd: osd.1 is healthy.
2023-02-17 00:57:30.014877 D | op-osd: validating status of osd.2
2023-02-17 00:57:30.014879 D | op-osd: osd.2 is healthy.
2023-02-17 00:57:30.014881 D | op-osd: validating status of osd.3
2023-02-17 00:57:30.014883 D | op-osd: osd.3 is healthy.
2023-02-17 00:57:30.014885 D | op-osd: validating status of osd.4
2023-02-17 00:57:30.014887 D | op-osd: osd.4 is healthy.
2023-02-17 00:57:30.014889 D | op-osd: validating status of osd.5
2023-02-17 00:57:30.014891 D | op-osd: osd.5 is healthy.
2023-02-17 00:57:30.014903 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:57:31.014333 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:57:31.014365 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:57:31.364713 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:57:31.371773 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:57:31.729757 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:57:31.729788 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:57:31.729849 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:57:31.729857 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:57:31.741722 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:57:31.741751 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:57:36.247209 D | op-mon: checking health of mons
2023-02-17 00:57:36.247230 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:57:36.247233 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:57:36.260195 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 00:57:36.260236 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:57:36.612009 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 00:57:36.612037 D | op-mon: targeting the mon count 2
2023-02-17 00:57:36.612044 D | op-mon: mon "a" found in quorum
2023-02-17 00:57:36.612047 D | op-mon: mon "b" found in quorum
2023-02-17 00:57:36.612049 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 00:57:36.617626 D | op-mon: Released lock for mon orchestration
2023-02-17 00:57:36.617659 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:58:21.617751 D | op-mon: checking health of mons
2023-02-17 00:58:21.617784 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:58:21.617789 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:58:21.626293 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 00:58:21.626329 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:58:21.999921 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 00:58:21.999956 D | op-mon: targeting the mon count 2
2023-02-17 00:58:21.999965 D | op-mon: mon "a" found in quorum
2023-02-17 00:58:21.999967 D | op-mon: mon "b" found in quorum
2023-02-17 00:58:21.999969 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 00:58:22.007572 D | op-mon: Released lock for mon orchestration
2023-02-17 00:58:22.007606 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:58:30.316434 D | op-osd: checking osd processes status.
2023-02-17 00:58:30.316481 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:58:30.601492 D | op-osd: validating status of osd.0
2023-02-17 00:58:30.601521 D | op-osd: osd.0 is healthy.
2023-02-17 00:58:30.601526 D | op-osd: validating status of osd.1
2023-02-17 00:58:30.601528 D | op-osd: osd.1 is healthy.
2023-02-17 00:58:30.601556 D | op-osd: validating status of osd.2
2023-02-17 00:58:30.601560 D | op-osd: osd.2 is healthy.
2023-02-17 00:58:30.601561 D | op-osd: validating status of osd.3
2023-02-17 00:58:30.601563 D | op-osd: osd.3 is healthy.
2023-02-17 00:58:30.601565 D | op-osd: validating status of osd.4
2023-02-17 00:58:30.601567 D | op-osd: osd.4 is healthy.
2023-02-17 00:58:30.601569 D | op-osd: validating status of osd.5
2023-02-17 00:58:30.601571 D | op-osd: osd.5 is healthy.
2023-02-17 00:58:30.601581 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:58:31.741400 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:58:31.741445 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:58:32.104477 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:58:32.110293 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:58:32.475174 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:58:32.475213 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:58:32.475299 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:58:32.475310 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:58:32.486093 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:58:32.486122 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:59:07.008552 D | op-mon: checking health of mons
2023-02-17 00:59:07.008584 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:59:07.008589 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:59:07.016213 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 00:59:07.016249 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:59:07.379073 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 00:59:07.379098 D | op-mon: targeting the mon count 2
2023-02-17 00:59:07.379106 D | op-mon: mon "a" found in quorum
2023-02-17 00:59:07.379109 D | op-mon: mon "b" found in quorum
2023-02-17 00:59:07.379111 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 00:59:07.387007 D | op-mon: Released lock for mon orchestration
2023-02-17 00:59:07.387066 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 00:59:30.912653 D | op-osd: checking osd processes status.
2023-02-17 00:59:30.912698 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:59:31.201131 D | op-osd: validating status of osd.0
2023-02-17 00:59:31.201160 D | op-osd: osd.0 is healthy.
2023-02-17 00:59:31.201165 D | op-osd: validating status of osd.1
2023-02-17 00:59:31.201167 D | op-osd: osd.1 is healthy.
2023-02-17 00:59:31.201170 D | op-osd: validating status of osd.2
2023-02-17 00:59:31.201171 D | op-osd: osd.2 is healthy.
2023-02-17 00:59:31.201173 D | op-osd: validating status of osd.3
2023-02-17 00:59:31.201175 D | op-osd: osd.3 is healthy.
2023-02-17 00:59:31.201177 D | op-osd: validating status of osd.4
2023-02-17 00:59:31.201179 D | op-osd: osd.4 is healthy.
2023-02-17 00:59:31.201181 D | op-osd: validating status of osd.5
2023-02-17 00:59:31.201183 D | op-osd: osd.5 is healthy.
2023-02-17 00:59:31.201196 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:59:32.485663 D | ceph-cluster-controller: checking health of cluster
2023-02-17 00:59:32.485693 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 00:59:32.837981 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 00:59:32.847657 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:59:33.239332 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:59:33.239349 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 00:59:33.239407 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 00:59:33.239416 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 00:59:33.250849 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 00:59:33.250877 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 00:59:52.388061 D | op-mon: checking health of mons
2023-02-17 00:59:52.388095 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 00:59:52.388100 D | op-mon: Acquired lock for mon orchestration
2023-02-17 00:59:52.397296 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 00:59:52.397333 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 00:59:52.745364 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 00:59:52.745408 D | op-mon: targeting the mon count 2
2023-02-17 00:59:52.745413 D | op-mon: mon "a" found in quorum
2023-02-17 00:59:52.745416 D | op-mon: mon "b" found in quorum
2023-02-17 00:59:52.745417 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 00:59:52.752979 D | op-mon: Released lock for mon orchestration
2023-02-17 00:59:52.753012 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:00:31.507460 D | op-osd: checking osd processes status.
2023-02-17 01:00:31.507504 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:00:31.803485 D | op-osd: validating status of osd.0
2023-02-17 01:00:31.803517 D | op-osd: osd.0 is healthy.
2023-02-17 01:00:31.803523 D | op-osd: validating status of osd.1
2023-02-17 01:00:31.803525 D | op-osd: osd.1 is healthy.
2023-02-17 01:00:31.803527 D | op-osd: validating status of osd.2
2023-02-17 01:00:31.803529 D | op-osd: osd.2 is healthy.
2023-02-17 01:00:31.803530 D | op-osd: validating status of osd.3
2023-02-17 01:00:31.803532 D | op-osd: osd.3 is healthy.
2023-02-17 01:00:31.803534 D | op-osd: validating status of osd.4
2023-02-17 01:00:31.803536 D | op-osd: osd.4 is healthy.
2023-02-17 01:00:31.803538 D | op-osd: validating status of osd.5
2023-02-17 01:00:31.803540 D | op-osd: osd.5 is healthy.
2023-02-17 01:00:31.803554 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:00:33.251482 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:00:33.251513 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:00:33.603881 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:00:33.611310 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:00:33.972725 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:00:33.972751 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:00:33.972809 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:00:33.972815 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:00:33.983216 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:00:33.983246 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:00:37.754184 D | op-mon: checking health of mons
2023-02-17 01:00:37.754217 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:00:37.754224 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:00:37.761592 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:00:37.761628 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:00:38.122226 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:00:38.122240 D | op-mon: targeting the mon count 2
2023-02-17 01:00:38.122247 D | op-mon: mon "a" found in quorum
2023-02-17 01:00:38.122249 D | op-mon: mon "b" found in quorum
2023-02-17 01:00:38.122251 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:00:38.129736 D | op-mon: Released lock for mon orchestration
2023-02-17 01:00:38.129769 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:01:23.129827 D | op-mon: checking health of mons
2023-02-17 01:01:23.129871 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:01:23.129875 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:01:23.138118 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:01:23.138157 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:01:23.506663 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:01:23.506694 D | op-mon: targeting the mon count 2
2023-02-17 01:01:23.506703 D | op-mon: mon "a" found in quorum
2023-02-17 01:01:23.506705 D | op-mon: mon "b" found in quorum
2023-02-17 01:01:23.506707 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:01:23.513639 D | op-mon: Released lock for mon orchestration
2023-02-17 01:01:23.513673 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:01:32.111558 D | op-osd: checking osd processes status.
2023-02-17 01:01:32.111614 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:01:32.407349 D | op-osd: validating status of osd.0
2023-02-17 01:01:32.407378 D | op-osd: osd.0 is healthy.
2023-02-17 01:01:32.407383 D | op-osd: validating status of osd.1
2023-02-17 01:01:32.407386 D | op-osd: osd.1 is healthy.
2023-02-17 01:01:32.407388 D | op-osd: validating status of osd.2
2023-02-17 01:01:32.407389 D | op-osd: osd.2 is healthy.
2023-02-17 01:01:32.407391 D | op-osd: validating status of osd.3
2023-02-17 01:01:32.407393 D | op-osd: osd.3 is healthy.
2023-02-17 01:01:32.407395 D | op-osd: validating status of osd.4
2023-02-17 01:01:32.407398 D | op-osd: osd.4 is healthy.
2023-02-17 01:01:32.407400 D | op-osd: validating status of osd.5
2023-02-17 01:01:32.407401 D | op-osd: osd.5 is healthy.
2023-02-17 01:01:32.407412 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:01:33.983578 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:01:33.983618 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:01:34.341282 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:01:34.348938 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:01:34.708366 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:01:34.708381 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:01:34.708459 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:01:34.708470 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:01:34.719111 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:01:34.719139 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:02:08.514721 D | op-mon: checking health of mons
2023-02-17 01:02:08.514754 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:02:08.514759 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:02:08.522043 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:02:08.522082 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:02:08.868186 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:02:08.868213 D | op-mon: targeting the mon count 2
2023-02-17 01:02:08.868221 D | op-mon: mon "a" found in quorum
2023-02-17 01:02:08.868224 D | op-mon: mon "b" found in quorum
2023-02-17 01:02:08.868226 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:02:08.873794 D | op-mon: Released lock for mon orchestration
2023-02-17 01:02:08.873826 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:02:32.721388 D | op-osd: checking osd processes status.
2023-02-17 01:02:32.721467 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:02:33.020470 D | op-osd: validating status of osd.0
2023-02-17 01:02:33.020499 D | op-osd: osd.0 is healthy.
2023-02-17 01:02:33.020504 D | op-osd: validating status of osd.1
2023-02-17 01:02:33.020507 D | op-osd: osd.1 is healthy.
2023-02-17 01:02:33.020509 D | op-osd: validating status of osd.2
2023-02-17 01:02:33.020511 D | op-osd: osd.2 is healthy.
2023-02-17 01:02:33.020512 D | op-osd: validating status of osd.3
2023-02-17 01:02:33.020514 D | op-osd: osd.3 is healthy.
2023-02-17 01:02:33.020516 D | op-osd: validating status of osd.4
2023-02-17 01:02:33.020518 D | op-osd: osd.4 is healthy.
2023-02-17 01:02:33.020520 D | op-osd: validating status of osd.5
2023-02-17 01:02:33.020522 D | op-osd: osd.5 is healthy.
2023-02-17 01:02:33.020533 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:02:34.719432 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:02:34.719474 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:02:35.072911 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:02:35.078321 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:02:35.421160 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:02:35.421188 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:02:35.421256 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:02:35.421295 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:02:35.435172 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:02:35.435188 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:02:53.874515 D | op-mon: checking health of mons
2023-02-17 01:02:53.874548 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:02:53.874553 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:02:53.883316 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:02:53.883351 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:02:54.238363 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:02:54.238393 D | op-mon: targeting the mon count 2
2023-02-17 01:02:54.238401 D | op-mon: mon "a" found in quorum
2023-02-17 01:02:54.238403 D | op-mon: mon "b" found in quorum
2023-02-17 01:02:54.238405 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:02:54.245586 D | op-mon: Released lock for mon orchestration
2023-02-17 01:02:54.245618 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:03:33.335446 D | op-osd: checking osd processes status.
2023-02-17 01:03:33.335517 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:03:33.638723 D | op-osd: validating status of osd.0
2023-02-17 01:03:33.638755 D | op-osd: osd.0 is healthy.
2023-02-17 01:03:33.638790 D | op-osd: validating status of osd.1
2023-02-17 01:03:33.638795 D | op-osd: osd.1 is healthy.
2023-02-17 01:03:33.638797 D | op-osd: validating status of osd.2
2023-02-17 01:03:33.638799 D | op-osd: osd.2 is healthy.
2023-02-17 01:03:33.638800 D | op-osd: validating status of osd.3
2023-02-17 01:03:33.638802 D | op-osd: osd.3 is healthy.
2023-02-17 01:03:33.638804 D | op-osd: validating status of osd.4
2023-02-17 01:03:33.638806 D | op-osd: osd.4 is healthy.
2023-02-17 01:03:33.638808 D | op-osd: validating status of osd.5
2023-02-17 01:03:33.638810 D | op-osd: osd.5 is healthy.
2023-02-17 01:03:33.638823 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:03:35.435139 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:03:35.435177 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:03:35.791029 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:03:35.799746 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:03:36.147243 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:03:36.147258 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:03:36.147357 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:03:36.147366 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:03:36.159579 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:03:36.159609 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:03:39.246762 D | op-mon: checking health of mons
2023-02-17 01:03:39.246799 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:03:39.246804 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:03:39.258363 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:03:39.258399 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:03:39.604123 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:03:39.604169 D | op-mon: targeting the mon count 2
2023-02-17 01:03:39.604177 D | op-mon: mon "a" found in quorum
2023-02-17 01:03:39.604179 D | op-mon: mon "b" found in quorum
2023-02-17 01:03:39.604181 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:03:39.611593 D | op-mon: Released lock for mon orchestration
2023-02-17 01:03:39.611626 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:04:24.612440 D | op-mon: checking health of mons
2023-02-17 01:04:24.612461 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:04:24.612464 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:04:24.624380 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:04:24.624419 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:04:24.986304 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:04:24.986344 D | op-mon: targeting the mon count 2
2023-02-17 01:04:24.986352 D | op-mon: mon "a" found in quorum
2023-02-17 01:04:24.986355 D | op-mon: mon "b" found in quorum
2023-02-17 01:04:24.986356 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:04:24.991455 D | op-mon: Released lock for mon orchestration
2023-02-17 01:04:24.991486 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:04:33.953667 D | op-osd: checking osd processes status.
2023-02-17 01:04:33.953721 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:04:34.239773 D | op-osd: validating status of osd.0
2023-02-17 01:04:34.239801 D | op-osd: osd.0 is healthy.
2023-02-17 01:04:34.239807 D | op-osd: validating status of osd.1
2023-02-17 01:04:34.239809 D | op-osd: osd.1 is healthy.
2023-02-17 01:04:34.239811 D | op-osd: validating status of osd.2
2023-02-17 01:04:34.239813 D | op-osd: osd.2 is healthy.
2023-02-17 01:04:34.239815 D | op-osd: validating status of osd.3
2023-02-17 01:04:34.239817 D | op-osd: osd.3 is healthy.
2023-02-17 01:04:34.239819 D | op-osd: validating status of osd.4
2023-02-17 01:04:34.239820 D | op-osd: osd.4 is healthy.
2023-02-17 01:04:34.239822 D | op-osd: validating status of osd.5
2023-02-17 01:04:34.239824 D | op-osd: osd.5 is healthy.
2023-02-17 01:04:34.239837 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:04:36.159729 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:04:36.159770 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:04:36.513496 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:04:36.521249 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:04:36.879856 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:04:36.879889 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:04:36.879952 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:04:36.879960 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:04:36.893397 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:04:36.893413 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:05:09.991796 D | op-mon: checking health of mons
2023-02-17 01:05:09.991831 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:05:09.991837 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:05:10.002203 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:05:10.002256 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:05:10.356024 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:05:10.356050 D | op-mon: targeting the mon count 2
2023-02-17 01:05:10.356058 D | op-mon: mon "a" found in quorum
2023-02-17 01:05:10.356060 D | op-mon: mon "b" found in quorum
2023-02-17 01:05:10.356062 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:05:10.363637 D | op-mon: Released lock for mon orchestration
2023-02-17 01:05:10.363671 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:05:34.557677 D | op-osd: checking osd processes status.
2023-02-17 01:05:34.557722 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:05:34.845610 D | op-osd: validating status of osd.0
2023-02-17 01:05:34.845638 D | op-osd: osd.0 is healthy.
2023-02-17 01:05:34.845644 D | op-osd: validating status of osd.1
2023-02-17 01:05:34.845646 D | op-osd: osd.1 is healthy.
2023-02-17 01:05:34.845648 D | op-osd: validating status of osd.2
2023-02-17 01:05:34.845650 D | op-osd: osd.2 is healthy.
2023-02-17 01:05:34.845652 D | op-osd: validating status of osd.3
2023-02-17 01:05:34.845654 D | op-osd: osd.3 is healthy.
2023-02-17 01:05:34.845656 D | op-osd: validating status of osd.4
2023-02-17 01:05:34.845658 D | op-osd: osd.4 is healthy.
2023-02-17 01:05:34.845660 D | op-osd: validating status of osd.5
2023-02-17 01:05:34.845662 D | op-osd: osd.5 is healthy.
2023-02-17 01:05:34.845673 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:05:36.892383 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:05:36.892433 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:05:37.252963 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:05:37.261559 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:05:37.628851 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:05:37.628880 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:05:37.628943 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:05:37.628961 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:05:37.638749 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:05:37.638766 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:05:55.364584 D | op-mon: checking health of mons
2023-02-17 01:05:55.364616 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:05:55.364622 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:05:55.371573 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:05:55.371611 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:05:55.734682 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:05:55.734711 D | op-mon: targeting the mon count 2
2023-02-17 01:05:55.734719 D | op-mon: mon "a" found in quorum
2023-02-17 01:05:55.734723 D | op-mon: mon "b" found in quorum
2023-02-17 01:05:55.734724 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:05:55.743475 D | op-mon: Released lock for mon orchestration
2023-02-17 01:05:55.743498 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:06:35.160863 D | op-osd: checking osd processes status.
2023-02-17 01:06:35.160908 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:06:35.464880 D | op-osd: validating status of osd.0
2023-02-17 01:06:35.464911 D | op-osd: osd.0 is healthy.
2023-02-17 01:06:35.464916 D | op-osd: validating status of osd.1
2023-02-17 01:06:35.464918 D | op-osd: osd.1 is healthy.
2023-02-17 01:06:35.464920 D | op-osd: validating status of osd.2
2023-02-17 01:06:35.464922 D | op-osd: osd.2 is healthy.
2023-02-17 01:06:35.464924 D | op-osd: validating status of osd.3
2023-02-17 01:06:35.464926 D | op-osd: osd.3 is healthy.
2023-02-17 01:06:35.464927 D | op-osd: validating status of osd.4
2023-02-17 01:06:35.464929 D | op-osd: osd.4 is healthy.
2023-02-17 01:06:35.464931 D | op-osd: validating status of osd.5
2023-02-17 01:06:35.464933 D | op-osd: osd.5 is healthy.
2023-02-17 01:06:35.464947 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:06:37.638788 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:06:37.638864 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:06:38.025024 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:06:38.032924 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:06:38.389526 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:06:38.389562 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:06:38.389645 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:06:38.389683 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:06:38.402737 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:06:38.402753 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:06:40.744399 D | op-mon: checking health of mons
2023-02-17 01:06:40.744430 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:06:40.744435 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:06:40.753899 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:06:40.753935 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:06:41.118816 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:06:41.118843 D | op-mon: targeting the mon count 2
2023-02-17 01:06:41.118851 D | op-mon: mon "a" found in quorum
2023-02-17 01:06:41.118854 D | op-mon: mon "b" found in quorum
2023-02-17 01:06:41.118856 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:06:41.124704 D | op-mon: Released lock for mon orchestration
2023-02-17 01:06:41.124728 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:07:26.125039 D | op-mon: checking health of mons
2023-02-17 01:07:26.125095 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:07:26.125101 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:07:26.133626 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:07:26.133660 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:07:26.498751 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:07:26.498780 D | op-mon: targeting the mon count 2
2023-02-17 01:07:26.498789 D | op-mon: mon "a" found in quorum
2023-02-17 01:07:26.498791 D | op-mon: mon "b" found in quorum
2023-02-17 01:07:26.498793 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:07:26.505927 D | op-mon: Released lock for mon orchestration
2023-02-17 01:07:26.505960 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:07:35.792663 D | op-osd: checking osd processes status.
2023-02-17 01:07:35.792712 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:07:36.093980 D | op-osd: validating status of osd.0
2023-02-17 01:07:36.094008 D | op-osd: osd.0 is healthy.
2023-02-17 01:07:36.094013 D | op-osd: validating status of osd.1
2023-02-17 01:07:36.094015 D | op-osd: osd.1 is healthy.
2023-02-17 01:07:36.094017 D | op-osd: validating status of osd.2
2023-02-17 01:07:36.094019 D | op-osd: osd.2 is healthy.
2023-02-17 01:07:36.094021 D | op-osd: validating status of osd.3
2023-02-17 01:07:36.094023 D | op-osd: osd.3 is healthy.
2023-02-17 01:07:36.094025 D | op-osd: validating status of osd.4
2023-02-17 01:07:36.094027 D | op-osd: osd.4 is healthy.
2023-02-17 01:07:36.094029 D | op-osd: validating status of osd.5
2023-02-17 01:07:36.094031 D | op-osd: osd.5 is healthy.
2023-02-17 01:07:36.094045 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:07:38.401136 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:07:38.401175 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:07:38.774319 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:07:38.779752 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:07:39.153648 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:07:39.153679 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:07:39.153744 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:07:39.153752 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:07:39.164763 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:07:39.164794 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:08:11.506639 D | op-mon: checking health of mons
2023-02-17 01:08:11.506671 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:08:11.506676 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:08:11.514519 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:08:11.514560 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:08:11.866953 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:08:11.866979 D | op-mon: targeting the mon count 2
2023-02-17 01:08:11.866987 D | op-mon: mon "a" found in quorum
2023-02-17 01:08:11.866990 D | op-mon: mon "b" found in quorum
2023-02-17 01:08:11.866991 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:08:11.875816 D | op-mon: Released lock for mon orchestration
2023-02-17 01:08:11.875853 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:08:36.415521 D | op-osd: checking osd processes status.
2023-02-17 01:08:36.415582 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:08:36.718387 D | op-osd: validating status of osd.0
2023-02-17 01:08:36.718417 D | op-osd: osd.0 is healthy.
2023-02-17 01:08:36.718422 D | op-osd: validating status of osd.1
2023-02-17 01:08:36.718425 D | op-osd: osd.1 is healthy.
2023-02-17 01:08:36.718427 D | op-osd: validating status of osd.2
2023-02-17 01:08:36.718429 D | op-osd: osd.2 is healthy.
2023-02-17 01:08:36.718431 D | op-osd: validating status of osd.3
2023-02-17 01:08:36.718433 D | op-osd: osd.3 is healthy.
2023-02-17 01:08:36.718434 D | op-osd: validating status of osd.4
2023-02-17 01:08:36.718437 D | op-osd: osd.4 is healthy.
2023-02-17 01:08:36.718438 D | op-osd: validating status of osd.5
2023-02-17 01:08:36.718440 D | op-osd: osd.5 is healthy.
2023-02-17 01:08:36.718454 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:08:39.165323 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:08:39.165380 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:08:39.516709 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:08:39.522753 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:08:39.881299 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:08:39.881332 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:08:39.881394 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:08:39.881402 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:08:39.892843 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:08:39.892872 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:08:56.876065 D | op-mon: checking health of mons
2023-02-17 01:08:56.876112 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:08:56.876119 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:08:56.892547 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:08:56.892584 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:08:57.270001 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:08:57.270031 D | op-mon: targeting the mon count 2
2023-02-17 01:08:57.270039 D | op-mon: mon "a" found in quorum
2023-02-17 01:08:57.270042 D | op-mon: mon "b" found in quorum
2023-02-17 01:08:57.270044 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:08:57.275795 D | op-mon: Released lock for mon orchestration
2023-02-17 01:08:57.275827 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:09:37.046140 D | op-osd: checking osd processes status.
2023-02-17 01:09:37.046183 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:09:37.346910 D | op-osd: validating status of osd.0
2023-02-17 01:09:37.346939 D | op-osd: osd.0 is healthy.
2023-02-17 01:09:37.346945 D | op-osd: validating status of osd.1
2023-02-17 01:09:37.346947 D | op-osd: osd.1 is healthy.
2023-02-17 01:09:37.346949 D | op-osd: validating status of osd.2
2023-02-17 01:09:37.346951 D | op-osd: osd.2 is healthy.
2023-02-17 01:09:37.346953 D | op-osd: validating status of osd.3
2023-02-17 01:09:37.346955 D | op-osd: osd.3 is healthy.
2023-02-17 01:09:37.346957 D | op-osd: validating status of osd.4
2023-02-17 01:09:37.346959 D | op-osd: osd.4 is healthy.
2023-02-17 01:09:37.346961 D | op-osd: validating status of osd.5
2023-02-17 01:09:37.346963 D | op-osd: osd.5 is healthy.
2023-02-17 01:09:37.346974 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:09:39.895226 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:09:39.895293 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:09:40.258247 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:09:40.265906 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:09:40.622873 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:09:40.622923 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:09:40.622986 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:09:40.623008 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:09:40.638674 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:09:40.638701 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:09:42.276241 D | op-mon: checking health of mons
2023-02-17 01:09:42.276307 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:09:42.276312 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:09:42.285962 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:09:42.285997 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:09:42.641595 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:09:42.641621 D | op-mon: targeting the mon count 2
2023-02-17 01:09:42.641629 D | op-mon: mon "a" found in quorum
2023-02-17 01:09:42.641631 D | op-mon: mon "b" found in quorum
2023-02-17 01:09:42.641633 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:09:42.649191 D | op-mon: Released lock for mon orchestration
2023-02-17 01:09:42.649223 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:10:27.649577 D | op-mon: checking health of mons
2023-02-17 01:10:27.649638 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:10:27.649657 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:10:27.660206 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:10:27.660229 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:10:28.039675 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:10:28.039690 D | op-mon: targeting the mon count 2
2023-02-17 01:10:28.039696 D | op-mon: mon "a" found in quorum
2023-02-17 01:10:28.039699 D | op-mon: mon "b" found in quorum
2023-02-17 01:10:28.039701 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:10:28.047670 D | op-mon: Released lock for mon orchestration
2023-02-17 01:10:28.047703 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:10:37.663841 D | op-osd: checking osd processes status.
2023-02-17 01:10:37.663894 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:10:37.994328 D | op-osd: validating status of osd.0
2023-02-17 01:10:37.994350 D | op-osd: osd.0 is healthy.
2023-02-17 01:10:37.994354 D | op-osd: validating status of osd.1
2023-02-17 01:10:37.994356 D | op-osd: osd.1 is healthy.
2023-02-17 01:10:37.994358 D | op-osd: validating status of osd.2
2023-02-17 01:10:37.994360 D | op-osd: osd.2 is healthy.
2023-02-17 01:10:37.994363 D | op-osd: validating status of osd.3
2023-02-17 01:10:37.994365 D | op-osd: osd.3 is healthy.
2023-02-17 01:10:37.994367 D | op-osd: validating status of osd.4
2023-02-17 01:10:37.994369 D | op-osd: osd.4 is healthy.
2023-02-17 01:10:37.994371 D | op-osd: validating status of osd.5
2023-02-17 01:10:37.994373 D | op-osd: osd.5 is healthy.
2023-02-17 01:10:37.994416 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:10:40.637934 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:10:40.637967 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:10:40.997343 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:10:41.005356 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:10:41.363012 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:10:41.363039 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:10:41.363102 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:10:41.363123 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:10:41.376055 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:10:41.376086 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:11:13.048015 D | op-mon: checking health of mons
2023-02-17 01:11:13.048060 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:11:13.048066 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:11:13.059408 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:11:13.059451 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:11:13.434001 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:11:13.434029 D | op-mon: targeting the mon count 2
2023-02-17 01:11:13.434037 D | op-mon: mon "a" found in quorum
2023-02-17 01:11:13.434040 D | op-mon: mon "b" found in quorum
2023-02-17 01:11:13.434042 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:11:13.439696 D | op-mon: Released lock for mon orchestration
2023-02-17 01:11:13.439730 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:11:38.323606 D | op-osd: checking osd processes status.
2023-02-17 01:11:38.323671 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:11:38.615240 D | op-osd: validating status of osd.0
2023-02-17 01:11:38.615256 D | op-osd: osd.0 is healthy.
2023-02-17 01:11:38.615259 D | op-osd: validating status of osd.1
2023-02-17 01:11:38.615261 D | op-osd: osd.1 is healthy.
2023-02-17 01:11:38.615262 D | op-osd: validating status of osd.2
2023-02-17 01:11:38.615264 D | op-osd: osd.2 is healthy.
2023-02-17 01:11:38.615266 D | op-osd: validating status of osd.3
2023-02-17 01:11:38.615268 D | op-osd: osd.3 is healthy.
2023-02-17 01:11:38.615270 D | op-osd: validating status of osd.4
2023-02-17 01:11:38.615272 D | op-osd: osd.4 is healthy.
2023-02-17 01:11:38.615273 D | op-osd: validating status of osd.5
2023-02-17 01:11:38.615275 D | op-osd: osd.5 is healthy.
2023-02-17 01:11:38.615290 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:11:41.375566 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:11:41.375628 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:11:41.729455 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:11:41.737894 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:11:42.100359 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:11:42.100407 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:11:42.100471 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:11:42.100480 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:11:42.110774 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:11:42.110800 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:11:58.440670 D | op-mon: checking health of mons
2023-02-17 01:11:58.440704 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:11:58.440709 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:11:58.448992 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:11:58.449026 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:11:58.801869 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:11:58.801897 D | op-mon: targeting the mon count 2
2023-02-17 01:11:58.801906 D | op-mon: mon "a" found in quorum
2023-02-17 01:11:58.801909 D | op-mon: mon "b" found in quorum
2023-02-17 01:11:58.801910 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:11:58.809213 D | op-mon: Released lock for mon orchestration
2023-02-17 01:11:58.809242 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:12:38.944407 D | op-osd: checking osd processes status.
2023-02-17 01:12:38.944456 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:12:39.252384 D | op-osd: validating status of osd.0
2023-02-17 01:12:39.252411 D | op-osd: osd.0 is healthy.
2023-02-17 01:12:39.252416 D | op-osd: validating status of osd.1
2023-02-17 01:12:39.252418 D | op-osd: osd.1 is healthy.
2023-02-17 01:12:39.252420 D | op-osd: validating status of osd.2
2023-02-17 01:12:39.252422 D | op-osd: osd.2 is healthy.
2023-02-17 01:12:39.252424 D | op-osd: validating status of osd.3
2023-02-17 01:12:39.252426 D | op-osd: osd.3 is healthy.
2023-02-17 01:12:39.252428 D | op-osd: validating status of osd.4
2023-02-17 01:12:39.252430 D | op-osd: osd.4 is healthy.
2023-02-17 01:12:39.252431 D | op-osd: validating status of osd.5
2023-02-17 01:12:39.252434 D | op-osd: osd.5 is healthy.
2023-02-17 01:12:39.252445 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:12:42.110378 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:12:42.110409 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:12:42.476609 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:12:42.485479 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:12:42.842214 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:12:42.842241 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:12:42.842327 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:12:42.842352 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:12:42.854306 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:12:42.854322 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:12:43.809975 D | op-mon: checking health of mons
2023-02-17 01:12:43.810014 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:12:43.810021 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:12:43.817249 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:12:43.817311 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:12:44.176499 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:12:44.176527 D | op-mon: targeting the mon count 2
2023-02-17 01:12:44.176535 D | op-mon: mon "a" found in quorum
2023-02-17 01:12:44.176537 D | op-mon: mon "b" found in quorum
2023-02-17 01:12:44.176539 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:12:44.184506 D | op-mon: Released lock for mon orchestration
2023-02-17 01:12:44.184541 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:13:29.184645 D | op-mon: checking health of mons
2023-02-17 01:13:29.184669 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:13:29.184672 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:13:29.196484 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:13:29.196507 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:13:29.563556 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:13:29.563584 D | op-mon: targeting the mon count 2
2023-02-17 01:13:29.563591 D | op-mon: mon "a" found in quorum
2023-02-17 01:13:29.563594 D | op-mon: mon "b" found in quorum
2023-02-17 01:13:29.563596 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:13:29.569819 D | op-mon: Released lock for mon orchestration
2023-02-17 01:13:29.569852 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:13:39.562855 D | op-osd: checking osd processes status.
2023-02-17 01:13:39.562902 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:13:39.862943 D | op-osd: validating status of osd.0
2023-02-17 01:13:39.862977 D | op-osd: osd.0 is healthy.
2023-02-17 01:13:39.862983 D | op-osd: validating status of osd.1
2023-02-17 01:13:39.862985 D | op-osd: osd.1 is healthy.
2023-02-17 01:13:39.862987 D | op-osd: validating status of osd.2
2023-02-17 01:13:39.862989 D | op-osd: osd.2 is healthy.
2023-02-17 01:13:39.862991 D | op-osd: validating status of osd.3
2023-02-17 01:13:39.862993 D | op-osd: osd.3 is healthy.
2023-02-17 01:13:39.862995 D | op-osd: validating status of osd.4
2023-02-17 01:13:39.862997 D | op-osd: osd.4 is healthy.
2023-02-17 01:13:39.862999 D | op-osd: validating status of osd.5
2023-02-17 01:13:39.863001 D | op-osd: osd.5 is healthy.
2023-02-17 01:13:39.863013 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:13:42.854706 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:13:42.854765 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:13:43.240685 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:13:43.250215 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:13:43.616310 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:13:43.616372 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:13:43.616447 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:13:43.616455 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:13:43.627729 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:13:43.627757 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:14:14.570857 D | op-mon: checking health of mons
2023-02-17 01:14:14.570891 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:14:14.570897 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:14:14.583182 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:14:14.583223 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:14:14.954033 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:14:14.954063 D | op-mon: targeting the mon count 2
2023-02-17 01:14:14.954072 D | op-mon: mon "a" found in quorum
2023-02-17 01:14:14.954075 D | op-mon: mon "b" found in quorum
2023-02-17 01:14:14.954077 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:14:14.961582 D | op-mon: Released lock for mon orchestration
2023-02-17 01:14:14.961615 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:14:40.212595 D | op-osd: checking osd processes status.
2023-02-17 01:14:40.212651 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:14:40.504944 D | op-osd: validating status of osd.0
2023-02-17 01:14:40.505000 D | op-osd: osd.0 is healthy.
2023-02-17 01:14:40.505005 D | op-osd: validating status of osd.1
2023-02-17 01:14:40.505008 D | op-osd: osd.1 is healthy.
2023-02-17 01:14:40.505010 D | op-osd: validating status of osd.2
2023-02-17 01:14:40.505011 D | op-osd: osd.2 is healthy.
2023-02-17 01:14:40.505013 D | op-osd: validating status of osd.3
2023-02-17 01:14:40.505015 D | op-osd: osd.3 is healthy.
2023-02-17 01:14:40.505017 D | op-osd: validating status of osd.4
2023-02-17 01:14:40.505019 D | op-osd: osd.4 is healthy.
2023-02-17 01:14:40.505021 D | op-osd: validating status of osd.5
2023-02-17 01:14:40.505023 D | op-osd: osd.5 is healthy.
2023-02-17 01:14:40.505037 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:14:43.627541 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:14:43.627587 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:14:43.985593 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:14:43.995230 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:14:44.352149 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:14:44.352164 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:14:44.352219 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:14:44.352225 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:14:44.363390 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:14:44.363419 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:14:59.962669 D | op-mon: checking health of mons
2023-02-17 01:14:59.962701 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:14:59.962706 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:14:59.971985 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:14:59.972006 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:15:00.333010 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:15:00.333037 D | op-mon: targeting the mon count 2
2023-02-17 01:15:00.333045 D | op-mon: mon "a" found in quorum
2023-02-17 01:15:00.333048 D | op-mon: mon "b" found in quorum
2023-02-17 01:15:00.333050 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:15:00.339954 D | op-mon: Released lock for mon orchestration
2023-02-17 01:15:00.339985 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:15:40.808464 D | op-osd: checking osd processes status.
2023-02-17 01:15:40.808525 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:15:41.119066 D | op-osd: validating status of osd.0
2023-02-17 01:15:41.119096 D | op-osd: osd.0 is healthy.
2023-02-17 01:15:41.119101 D | op-osd: validating status of osd.1
2023-02-17 01:15:41.119103 D | op-osd: osd.1 is healthy.
2023-02-17 01:15:41.119105 D | op-osd: validating status of osd.2
2023-02-17 01:15:41.119107 D | op-osd: osd.2 is healthy.
2023-02-17 01:15:41.119109 D | op-osd: validating status of osd.3
2023-02-17 01:15:41.119111 D | op-osd: osd.3 is healthy.
2023-02-17 01:15:41.119113 D | op-osd: validating status of osd.4
2023-02-17 01:15:41.119115 D | op-osd: osd.4 is healthy.
2023-02-17 01:15:41.119116 D | op-osd: validating status of osd.5
2023-02-17 01:15:41.119118 D | op-osd: osd.5 is healthy.
2023-02-17 01:15:41.119129 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:15:44.363919 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:15:44.363961 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:15:44.720846 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:15:44.728626 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:15:45.082255 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:15:45.082307 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:15:45.082370 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:15:45.082392 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:15:45.094316 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:15:45.094344 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:15:45.340307 D | op-mon: checking health of mons
2023-02-17 01:15:45.340352 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:15:45.340357 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:15:45.348127 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:15:45.348163 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:15:45.702491 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:15:45.702519 D | op-mon: targeting the mon count 2
2023-02-17 01:15:45.702527 D | op-mon: mon "a" found in quorum
2023-02-17 01:15:45.702529 D | op-mon: mon "b" found in quorum
2023-02-17 01:15:45.702531 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:15:45.707939 D | op-mon: Released lock for mon orchestration
2023-02-17 01:15:45.707968 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:16:30.708738 D | op-mon: checking health of mons
2023-02-17 01:16:30.708780 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:16:30.708786 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:16:30.716544 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:16:30.716583 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:16:31.062863 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:16:31.062897 D | op-mon: targeting the mon count 2
2023-02-17 01:16:31.062904 D | op-mon: mon "a" found in quorum
2023-02-17 01:16:31.062906 D | op-mon: mon "b" found in quorum
2023-02-17 01:16:31.062908 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:16:31.070417 D | op-mon: Released lock for mon orchestration
2023-02-17 01:16:31.070450 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:16:41.422004 D | op-osd: checking osd processes status.
2023-02-17 01:16:41.422062 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:16:41.717081 D | op-osd: validating status of osd.0
2023-02-17 01:16:41.717109 D | op-osd: osd.0 is healthy.
2023-02-17 01:16:41.717113 D | op-osd: validating status of osd.1
2023-02-17 01:16:41.717115 D | op-osd: osd.1 is healthy.
2023-02-17 01:16:41.717117 D | op-osd: validating status of osd.2
2023-02-17 01:16:41.717119 D | op-osd: osd.2 is healthy.
2023-02-17 01:16:41.717121 D | op-osd: validating status of osd.3
2023-02-17 01:16:41.717123 D | op-osd: osd.3 is healthy.
2023-02-17 01:16:41.717124 D | op-osd: validating status of osd.4
2023-02-17 01:16:41.717126 D | op-osd: osd.4 is healthy.
2023-02-17 01:16:41.717128 D | op-osd: validating status of osd.5
2023-02-17 01:16:41.717129 D | op-osd: osd.5 is healthy.
2023-02-17 01:16:41.717143 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:16:45.093257 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:16:45.093346 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:16:45.447097 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:16:45.456613 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:16:45.822180 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:16:45.822211 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:16:45.822302 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:16:45.822317 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:16:45.833078 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:16:45.833105 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:17:16.071309 D | op-mon: checking health of mons
2023-02-17 01:17:16.071346 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:17:16.071351 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:17:16.077798 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:17:16.077839 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:17:16.442150 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:17:16.442176 D | op-mon: targeting the mon count 2
2023-02-17 01:17:16.442185 D | op-mon: mon "a" found in quorum
2023-02-17 01:17:16.442187 D | op-mon: mon "b" found in quorum
2023-02-17 01:17:16.442189 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:17:16.449584 D | op-mon: Released lock for mon orchestration
2023-02-17 01:17:16.449617 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:17:42.040422 D | op-osd: checking osd processes status.
2023-02-17 01:17:42.040471 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:17:42.327761 D | op-osd: validating status of osd.0
2023-02-17 01:17:42.327789 D | op-osd: osd.0 is healthy.
2023-02-17 01:17:42.327794 D | op-osd: validating status of osd.1
2023-02-17 01:17:42.327796 D | op-osd: osd.1 is healthy.
2023-02-17 01:17:42.327798 D | op-osd: validating status of osd.2
2023-02-17 01:17:42.327800 D | op-osd: osd.2 is healthy.
2023-02-17 01:17:42.327802 D | op-osd: validating status of osd.3
2023-02-17 01:17:42.327804 D | op-osd: osd.3 is healthy.
2023-02-17 01:17:42.327806 D | op-osd: validating status of osd.4
2023-02-17 01:17:42.327808 D | op-osd: osd.4 is healthy.
2023-02-17 01:17:42.327810 D | op-osd: validating status of osd.5
2023-02-17 01:17:42.327812 D | op-osd: osd.5 is healthy.
2023-02-17 01:17:42.327826 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:17:45.832828 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:17:45.832869 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:17:46.192055 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:17:46.202042 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:17:46.574992 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:17:46.575026 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:17:46.575093 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:17:46.575101 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:17:46.586865 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:17:46.586880 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:18:01.450455 D | op-mon: checking health of mons
2023-02-17 01:18:01.450488 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:18:01.450493 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:18:01.458065 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:18:01.458101 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:18:01.808208 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:18:01.808237 D | op-mon: targeting the mon count 2
2023-02-17 01:18:01.808245 D | op-mon: mon "a" found in quorum
2023-02-17 01:18:01.808247 D | op-mon: mon "b" found in quorum
2023-02-17 01:18:01.808249 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:18:01.813909 D | op-mon: Released lock for mon orchestration
2023-02-17 01:18:01.813941 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:18:42.641221 D | op-osd: checking osd processes status.
2023-02-17 01:18:42.641322 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:18:42.940515 D | op-osd: validating status of osd.0
2023-02-17 01:18:42.940545 D | op-osd: osd.0 is healthy.
2023-02-17 01:18:42.940551 D | op-osd: validating status of osd.1
2023-02-17 01:18:42.940553 D | op-osd: osd.1 is healthy.
2023-02-17 01:18:42.940555 D | op-osd: validating status of osd.2
2023-02-17 01:18:42.940557 D | op-osd: osd.2 is healthy.
2023-02-17 01:18:42.940559 D | op-osd: validating status of osd.3
2023-02-17 01:18:42.940561 D | op-osd: osd.3 is healthy.
2023-02-17 01:18:42.940563 D | op-osd: validating status of osd.4
2023-02-17 01:18:42.940565 D | op-osd: osd.4 is healthy.
2023-02-17 01:18:42.940567 D | op-osd: validating status of osd.5
2023-02-17 01:18:42.940569 D | op-osd: osd.5 is healthy.
2023-02-17 01:18:42.940582 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:18:46.586447 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:18:46.586488 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:18:46.813982 D | op-mon: checking health of mons
2023-02-17 01:18:46.814017 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:18:46.814023 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:18:46.825431 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:18:46.825494 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:18:46.944142 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:18:46.950144 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:18:47.203350 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:18:47.203379 D | op-mon: targeting the mon count 2
2023-02-17 01:18:47.203388 D | op-mon: mon "a" found in quorum
2023-02-17 01:18:47.203391 D | op-mon: mon "b" found in quorum
2023-02-17 01:18:47.203393 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:18:47.210989 D | op-mon: Released lock for mon orchestration
2023-02-17 01:18:47.211022 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:18:47.324558 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:18:47.324611 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:18:47.324672 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:18:47.324693 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:18:47.336163 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:18:47.336177 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:19:32.211640 D | op-mon: checking health of mons
2023-02-17 01:19:32.211694 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:19:32.211699 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:19:32.219297 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:19:32.219337 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:19:32.575364 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:19:32.575392 D | op-mon: targeting the mon count 2
2023-02-17 01:19:32.575400 D | op-mon: mon "a" found in quorum
2023-02-17 01:19:32.575403 D | op-mon: mon "b" found in quorum
2023-02-17 01:19:32.575405 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:19:32.582653 D | op-mon: Released lock for mon orchestration
2023-02-17 01:19:32.582686 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:19:43.260054 D | op-osd: checking osd processes status.
2023-02-17 01:19:43.260098 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:19:43.557380 D | op-osd: validating status of osd.0
2023-02-17 01:19:43.557408 D | op-osd: osd.0 is healthy.
2023-02-17 01:19:43.557413 D | op-osd: validating status of osd.1
2023-02-17 01:19:43.557415 D | op-osd: osd.1 is healthy.
2023-02-17 01:19:43.557417 D | op-osd: validating status of osd.2
2023-02-17 01:19:43.557419 D | op-osd: osd.2 is healthy.
2023-02-17 01:19:43.557421 D | op-osd: validating status of osd.3
2023-02-17 01:19:43.557423 D | op-osd: osd.3 is healthy.
2023-02-17 01:19:43.557425 D | op-osd: validating status of osd.4
2023-02-17 01:19:43.557427 D | op-osd: osd.4 is healthy.
2023-02-17 01:19:43.557429 D | op-osd: validating status of osd.5
2023-02-17 01:19:43.557431 D | op-osd: osd.5 is healthy.
2023-02-17 01:19:43.557446 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:19:47.335996 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:19:47.336036 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:19:47.697109 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:19:47.704541 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:19:48.064580 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:19:48.064610 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:19:48.064675 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:19:48.064682 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:19:48.075532 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:19:48.075549 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:20:17.583824 D | op-mon: checking health of mons
2023-02-17 01:20:17.583856 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:20:17.583862 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:20:17.591883 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:20:17.591919 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:20:17.941783 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:20:17.941816 D | op-mon: targeting the mon count 2
2023-02-17 01:20:17.941826 D | op-mon: mon "a" found in quorum
2023-02-17 01:20:17.941852 D | op-mon: mon "b" found in quorum
2023-02-17 01:20:17.941856 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:20:17.947311 D | op-mon: Released lock for mon orchestration
2023-02-17 01:20:17.947344 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:20:43.872211 D | op-osd: checking osd processes status.
2023-02-17 01:20:43.872250 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:20:44.159075 D | op-osd: validating status of osd.0
2023-02-17 01:20:44.159104 D | op-osd: osd.0 is healthy.
2023-02-17 01:20:44.159109 D | op-osd: validating status of osd.1
2023-02-17 01:20:44.159111 D | op-osd: osd.1 is healthy.
2023-02-17 01:20:44.159113 D | op-osd: validating status of osd.2
2023-02-17 01:20:44.159115 D | op-osd: osd.2 is healthy.
2023-02-17 01:20:44.159117 D | op-osd: validating status of osd.3
2023-02-17 01:20:44.159118 D | op-osd: osd.3 is healthy.
2023-02-17 01:20:44.159120 D | op-osd: validating status of osd.4
2023-02-17 01:20:44.159122 D | op-osd: osd.4 is healthy.
2023-02-17 01:20:44.159124 D | op-osd: validating status of osd.5
2023-02-17 01:20:44.159126 D | op-osd: osd.5 is healthy.
2023-02-17 01:20:44.159138 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:20:48.076148 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:20:48.076179 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:20:48.430296 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:20:48.436306 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:20:48.801776 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:20:48.801810 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:20:48.801878 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:20:48.801898 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:20:48.812619 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:20:48.812637 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:21:02.948110 D | op-mon: checking health of mons
2023-02-17 01:21:02.948181 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:21:02.948186 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:21:02.958089 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:21:02.958150 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:21:03.329888 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:21:03.329918 D | op-mon: targeting the mon count 2
2023-02-17 01:21:03.329926 D | op-mon: mon "a" found in quorum
2023-02-17 01:21:03.329929 D | op-mon: mon "b" found in quorum
2023-02-17 01:21:03.329930 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:21:03.337038 D | op-mon: Released lock for mon orchestration
2023-02-17 01:21:03.337073 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:21:44.473308 D | op-osd: checking osd processes status.
2023-02-17 01:21:44.473355 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:21:44.772255 D | op-osd: validating status of osd.0
2023-02-17 01:21:44.772301 D | op-osd: osd.0 is healthy.
2023-02-17 01:21:44.772306 D | op-osd: validating status of osd.1
2023-02-17 01:21:44.772308 D | op-osd: osd.1 is healthy.
2023-02-17 01:21:44.772310 D | op-osd: validating status of osd.2
2023-02-17 01:21:44.772312 D | op-osd: osd.2 is healthy.
2023-02-17 01:21:44.772314 D | op-osd: validating status of osd.3
2023-02-17 01:21:44.772316 D | op-osd: osd.3 is healthy.
2023-02-17 01:21:44.772318 D | op-osd: validating status of osd.4
2023-02-17 01:21:44.772320 D | op-osd: osd.4 is healthy.
2023-02-17 01:21:44.772322 D | op-osd: validating status of osd.5
2023-02-17 01:21:44.772324 D | op-osd: osd.5 is healthy.
2023-02-17 01:21:44.772334 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:21:48.337419 D | op-mon: checking health of mons
2023-02-17 01:21:48.337452 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:21:48.337481 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:21:48.345523 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:21:48.345560 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:21:48.697079 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:21:48.697105 D | op-mon: targeting the mon count 2
2023-02-17 01:21:48.697112 D | op-mon: mon "a" found in quorum
2023-02-17 01:21:48.697115 D | op-mon: mon "b" found in quorum
2023-02-17 01:21:48.697117 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:21:48.704528 D | op-mon: Released lock for mon orchestration
2023-02-17 01:21:48.704562 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:21:48.811665 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:21:48.811712 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:21:49.189298 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:21:49.195998 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:21:49.551478 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:21:49.551509 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:21:49.551572 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:21:49.551579 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:21:49.563417 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:21:49.563433 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:22:33.705083 D | op-mon: checking health of mons
2023-02-17 01:22:33.705187 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:22:33.705196 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:22:33.714373 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:22:33.714410 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:22:34.077563 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:22:34.077593 D | op-mon: targeting the mon count 2
2023-02-17 01:22:34.077602 D | op-mon: mon "a" found in quorum
2023-02-17 01:22:34.077604 D | op-mon: mon "b" found in quorum
2023-02-17 01:22:34.077607 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:22:34.082883 D | op-mon: Released lock for mon orchestration
2023-02-17 01:22:34.082912 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:22:45.075796 D | op-osd: checking osd processes status.
2023-02-17 01:22:45.075854 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:22:45.373705 D | op-osd: validating status of osd.0
2023-02-17 01:22:45.373755 D | op-osd: osd.0 is healthy.
2023-02-17 01:22:45.373759 D | op-osd: validating status of osd.1
2023-02-17 01:22:45.373761 D | op-osd: osd.1 is healthy.
2023-02-17 01:22:45.373764 D | op-osd: validating status of osd.2
2023-02-17 01:22:45.373766 D | op-osd: osd.2 is healthy.
2023-02-17 01:22:45.373767 D | op-osd: validating status of osd.3
2023-02-17 01:22:45.373769 D | op-osd: osd.3 is healthy.
2023-02-17 01:22:45.373771 D | op-osd: validating status of osd.4
2023-02-17 01:22:45.373773 D | op-osd: osd.4 is healthy.
2023-02-17 01:22:45.373798 D | op-osd: validating status of osd.5
2023-02-17 01:22:45.373800 D | op-osd: osd.5 is healthy.
2023-02-17 01:22:45.373813 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:22:49.561909 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:22:49.561939 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:22:49.932433 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:22:49.945224 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:22:50.313231 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:22:50.313288 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:22:50.313350 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:22:50.313373 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:22:50.324575 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:22:50.324592 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:23:19.084031 D | op-mon: checking health of mons
2023-02-17 01:23:19.084098 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:23:19.084104 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:23:19.096068 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:23:19.096105 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:23:19.474213 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:23:19.474242 D | op-mon: targeting the mon count 2
2023-02-17 01:23:19.474250 D | op-mon: mon "a" found in quorum
2023-02-17 01:23:19.474253 D | op-mon: mon "b" found in quorum
2023-02-17 01:23:19.474255 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:23:19.481246 D | op-mon: Released lock for mon orchestration
2023-02-17 01:23:19.481308 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:23:45.700471 D | op-osd: checking osd processes status.
2023-02-17 01:23:45.700520 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:23:46.012391 D | op-osd: validating status of osd.0
2023-02-17 01:23:46.012421 D | op-osd: osd.0 is healthy.
2023-02-17 01:23:46.012426 D | op-osd: validating status of osd.1
2023-02-17 01:23:46.012429 D | op-osd: osd.1 is healthy.
2023-02-17 01:23:46.012431 D | op-osd: validating status of osd.2
2023-02-17 01:23:46.012433 D | op-osd: osd.2 is healthy.
2023-02-17 01:23:46.012435 D | op-osd: validating status of osd.3
2023-02-17 01:23:46.012437 D | op-osd: osd.3 is healthy.
2023-02-17 01:23:46.012439 D | op-osd: validating status of osd.4
2023-02-17 01:23:46.012441 D | op-osd: osd.4 is healthy.
2023-02-17 01:23:46.012442 D | op-osd: validating status of osd.5
2023-02-17 01:23:46.012444 D | op-osd: osd.5 is healthy.
2023-02-17 01:23:46.012459 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:23:50.323979 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:23:50.324039 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:23:50.697660 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:23:50.708743 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:23:51.106051 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:23:51.106114 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:23:51.106176 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:23:51.106200 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:23:51.122447 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:23:51.122464 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:24:04.481365 D | op-mon: checking health of mons
2023-02-17 01:24:04.481391 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:24:04.481394 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:24:04.490131 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:24:04.490165 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:24:04.871635 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:24:04.871666 D | op-mon: targeting the mon count 2
2023-02-17 01:24:04.871675 D | op-mon: mon "a" found in quorum
2023-02-17 01:24:04.871678 D | op-mon: mon "b" found in quorum
2023-02-17 01:24:04.871680 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:24:04.879056 D | op-mon: Released lock for mon orchestration
2023-02-17 01:24:04.879089 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:24:46.341077 D | op-osd: checking osd processes status.
2023-02-17 01:24:46.341139 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:24:46.662914 D | op-osd: validating status of osd.0
2023-02-17 01:24:46.662943 D | op-osd: osd.0 is healthy.
2023-02-17 01:24:46.662947 D | op-osd: validating status of osd.1
2023-02-17 01:24:46.662950 D | op-osd: osd.1 is healthy.
2023-02-17 01:24:46.662952 D | op-osd: validating status of osd.2
2023-02-17 01:24:46.662953 D | op-osd: osd.2 is healthy.
2023-02-17 01:24:46.662955 D | op-osd: validating status of osd.3
2023-02-17 01:24:46.662957 D | op-osd: osd.3 is healthy.
2023-02-17 01:24:46.662959 D | op-osd: validating status of osd.4
2023-02-17 01:24:46.662961 D | op-osd: osd.4 is healthy.
2023-02-17 01:24:46.662963 D | op-osd: validating status of osd.5
2023-02-17 01:24:46.662965 D | op-osd: osd.5 is healthy.
2023-02-17 01:24:46.662977 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:24:49.879429 D | op-mon: checking health of mons
2023-02-17 01:24:49.879465 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:24:49.879472 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:24:49.889411 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:24:49.889448 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:24:50.275768 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:24:50.275798 D | op-mon: targeting the mon count 2
2023-02-17 01:24:50.275806 D | op-mon: mon "a" found in quorum
2023-02-17 01:24:50.275809 D | op-mon: mon "b" found in quorum
2023-02-17 01:24:50.275811 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:24:50.281863 D | op-mon: Released lock for mon orchestration
2023-02-17 01:24:50.281897 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:24:51.122458 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:24:51.122512 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:24:51.501928 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:24:51.510571 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:24:51.918749 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:24:51.918780 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:24:51.918840 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:24:51.918863 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:24:51.931315 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:24:51.931344 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:25:35.282063 D | op-mon: checking health of mons
2023-02-17 01:25:35.282095 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:25:35.282101 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:25:35.289030 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:25:35.289068 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:25:35.656463 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:25:35.656494 D | op-mon: targeting the mon count 2
2023-02-17 01:25:35.656502 D | op-mon: mon "a" found in quorum
2023-02-17 01:25:35.656505 D | op-mon: mon "b" found in quorum
2023-02-17 01:25:35.656506 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:25:35.665021 D | op-mon: Released lock for mon orchestration
2023-02-17 01:25:35.665056 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:25:46.993537 D | op-osd: checking osd processes status.
2023-02-17 01:25:46.993581 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:25:47.304122 D | op-osd: validating status of osd.0
2023-02-17 01:25:47.304151 D | op-osd: osd.0 is healthy.
2023-02-17 01:25:47.304156 D | op-osd: validating status of osd.1
2023-02-17 01:25:47.304158 D | op-osd: osd.1 is healthy.
2023-02-17 01:25:47.304160 D | op-osd: validating status of osd.2
2023-02-17 01:25:47.304162 D | op-osd: osd.2 is healthy.
2023-02-17 01:25:47.304164 D | op-osd: validating status of osd.3
2023-02-17 01:25:47.304166 D | op-osd: osd.3 is healthy.
2023-02-17 01:25:47.304168 D | op-osd: validating status of osd.4
2023-02-17 01:25:47.304170 D | op-osd: osd.4 is healthy.
2023-02-17 01:25:47.304172 D | op-osd: validating status of osd.5
2023-02-17 01:25:47.304173 D | op-osd: osd.5 is healthy.
2023-02-17 01:25:47.304184 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:25:51.934528 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:25:51.934575 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:25:52.361892 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:25:52.400763 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:25:52.777255 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:25:52.777323 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:25:52.777388 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:25:52.777396 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:25:52.789977 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:25:52.790007 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:26:20.665807 D | op-mon: checking health of mons
2023-02-17 01:26:20.665838 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:26:20.665843 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:26:20.675794 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:26:20.675835 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:26:21.035491 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:26:21.035522 D | op-mon: targeting the mon count 2
2023-02-17 01:26:21.035531 D | op-mon: mon "a" found in quorum
2023-02-17 01:26:21.035534 D | op-mon: mon "b" found in quorum
2023-02-17 01:26:21.035536 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:26:21.043012 D | op-mon: Released lock for mon orchestration
2023-02-17 01:26:21.043045 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:26:47.631357 D | op-osd: checking osd processes status.
2023-02-17 01:26:47.631407 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:26:47.936312 D | op-osd: validating status of osd.0
2023-02-17 01:26:47.936340 D | op-osd: osd.0 is healthy.
2023-02-17 01:26:47.936344 D | op-osd: validating status of osd.1
2023-02-17 01:26:47.936346 D | op-osd: osd.1 is healthy.
2023-02-17 01:26:47.936348 D | op-osd: validating status of osd.2
2023-02-17 01:26:47.936350 D | op-osd: osd.2 is healthy.
2023-02-17 01:26:47.936352 D | op-osd: validating status of osd.3
2023-02-17 01:26:47.936354 D | op-osd: osd.3 is healthy.
2023-02-17 01:26:47.936356 D | op-osd: validating status of osd.4
2023-02-17 01:26:47.936358 D | op-osd: osd.4 is healthy.
2023-02-17 01:26:47.936360 D | op-osd: validating status of osd.5
2023-02-17 01:26:47.936362 D | op-osd: osd.5 is healthy.
2023-02-17 01:26:47.936373 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:26:52.790288 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:26:52.790330 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:26:53.147243 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:26:53.153818 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:26:53.540312 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:26:53.540341 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:26:53.540403 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:26:53.540411 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:26:53.551046 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:26:53.551061 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:27:06.043705 D | op-mon: checking health of mons
2023-02-17 01:27:06.043738 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:27:06.043743 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:27:06.055574 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:27:06.055610 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:27:06.427023 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:27:06.427051 D | op-mon: targeting the mon count 2
2023-02-17 01:27:06.427059 D | op-mon: mon "a" found in quorum
2023-02-17 01:27:06.427062 D | op-mon: mon "b" found in quorum
2023-02-17 01:27:06.427064 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:27:06.432543 D | op-mon: Released lock for mon orchestration
2023-02-17 01:27:06.432578 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:27:48.265985 D | op-osd: checking osd processes status.
2023-02-17 01:27:48.266039 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:27:48.565639 D | op-osd: validating status of osd.0
2023-02-17 01:27:48.565668 D | op-osd: osd.0 is healthy.
2023-02-17 01:27:48.565673 D | op-osd: validating status of osd.1
2023-02-17 01:27:48.565676 D | op-osd: osd.1 is healthy.
2023-02-17 01:27:48.565677 D | op-osd: validating status of osd.2
2023-02-17 01:27:48.565679 D | op-osd: osd.2 is healthy.
2023-02-17 01:27:48.565681 D | op-osd: validating status of osd.3
2023-02-17 01:27:48.565683 D | op-osd: osd.3 is healthy.
2023-02-17 01:27:48.565685 D | op-osd: validating status of osd.4
2023-02-17 01:27:48.565687 D | op-osd: osd.4 is healthy.
2023-02-17 01:27:48.565689 D | op-osd: validating status of osd.5
2023-02-17 01:27:48.565691 D | op-osd: osd.5 is healthy.
2023-02-17 01:27:48.565705 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:27:51.433460 D | op-mon: checking health of mons
2023-02-17 01:27:51.433495 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:27:51.433500 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:27:51.442505 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:27:51.442540 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:27:51.833705 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:27:51.833737 D | op-mon: targeting the mon count 2
2023-02-17 01:27:51.833746 D | op-mon: mon "a" found in quorum
2023-02-17 01:27:51.833749 D | op-mon: mon "b" found in quorum
2023-02-17 01:27:51.833751 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:27:51.841559 D | op-mon: Released lock for mon orchestration
2023-02-17 01:27:51.841592 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:27:53.550665 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:27:53.550709 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:27:54.029372 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:27:54.038577 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:27:54.515294 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:27:54.515314 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:27:54.515377 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:27:54.515384 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:27:54.529340 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:27:54.529356 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:28:36.841918 D | op-mon: checking health of mons
2023-02-17 01:28:36.841940 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:28:36.841943 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:28:36.852522 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:28:36.852559 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:28:37.284121 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:28:37.284150 D | op-mon: targeting the mon count 2
2023-02-17 01:28:37.284159 D | op-mon: mon "a" found in quorum
2023-02-17 01:28:37.284162 D | op-mon: mon "b" found in quorum
2023-02-17 01:28:37.284164 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:28:37.293130 D | op-mon: Released lock for mon orchestration
2023-02-17 01:28:37.293176 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:28:48.878510 D | op-osd: checking osd processes status.
2023-02-17 01:28:48.878558 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:28:49.173982 D | op-osd: validating status of osd.0
2023-02-17 01:28:49.174013 D | op-osd: osd.0 is healthy.
2023-02-17 01:28:49.174018 D | op-osd: validating status of osd.1
2023-02-17 01:28:49.174020 D | op-osd: osd.1 is healthy.
2023-02-17 01:28:49.174022 D | op-osd: validating status of osd.2
2023-02-17 01:28:49.174024 D | op-osd: osd.2 is healthy.
2023-02-17 01:28:49.174026 D | op-osd: validating status of osd.3
2023-02-17 01:28:49.174028 D | op-osd: osd.3 is healthy.
2023-02-17 01:28:49.174030 D | op-osd: validating status of osd.4
2023-02-17 01:28:49.174032 D | op-osd: osd.4 is healthy.
2023-02-17 01:28:49.174033 D | op-osd: validating status of osd.5
2023-02-17 01:28:49.174035 D | op-osd: osd.5 is healthy.
2023-02-17 01:28:49.174048 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:28:54.527741 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:28:54.527796 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:28:54.906991 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:28:54.914903 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:28:55.277575 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:28:55.277632 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:28:55.277701 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:28:55.277721 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:28:55.290214 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:28:55.290231 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:29:22.293578 D | op-mon: checking health of mons
2023-02-17 01:29:22.293612 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:29:22.293617 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:29:22.307553 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:29:22.307590 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:29:22.677988 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:29:22.678022 D | op-mon: targeting the mon count 2
2023-02-17 01:29:22.678030 D | op-mon: mon "a" found in quorum
2023-02-17 01:29:22.678033 D | op-mon: mon "b" found in quorum
2023-02-17 01:29:22.678035 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:29:22.683997 D | op-mon: Released lock for mon orchestration
2023-02-17 01:29:22.684034 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:29:49.498340 D | op-osd: checking osd processes status.
2023-02-17 01:29:49.498384 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:29:49.801578 D | op-osd: validating status of osd.0
2023-02-17 01:29:49.801607 D | op-osd: osd.0 is healthy.
2023-02-17 01:29:49.801612 D | op-osd: validating status of osd.1
2023-02-17 01:29:49.801614 D | op-osd: osd.1 is healthy.
2023-02-17 01:29:49.801616 D | op-osd: validating status of osd.2
2023-02-17 01:29:49.801618 D | op-osd: osd.2 is healthy.
2023-02-17 01:29:49.801620 D | op-osd: validating status of osd.3
2023-02-17 01:29:49.801622 D | op-osd: osd.3 is healthy.
2023-02-17 01:29:49.801623 D | op-osd: validating status of osd.4
2023-02-17 01:29:49.801625 D | op-osd: osd.4 is healthy.
2023-02-17 01:29:49.801627 D | op-osd: validating status of osd.5
2023-02-17 01:29:49.801629 D | op-osd: osd.5 is healthy.
2023-02-17 01:29:49.801639 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:29:55.289876 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:29:55.289920 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:29:55.654355 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:29:55.660467 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:29:56.020401 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:29:56.020430 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:29:56.020488 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:29:56.020506 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:29:56.030955 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:29:56.030972 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:30:07.684530 D | op-mon: checking health of mons
2023-02-17 01:30:07.684566 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:30:07.684571 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:30:07.701895 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:30:07.701932 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:30:08.109937 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:30:08.109964 D | op-mon: targeting the mon count 2
2023-02-17 01:30:08.109972 D | op-mon: mon "a" found in quorum
2023-02-17 01:30:08.109975 D | op-mon: mon "b" found in quorum
2023-02-17 01:30:08.109977 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:30:08.118486 D | op-mon: Released lock for mon orchestration
2023-02-17 01:30:08.118523 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:30:50.112559 D | op-osd: checking osd processes status.
2023-02-17 01:30:50.112617 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:30:50.405420 D | op-osd: validating status of osd.0
2023-02-17 01:30:50.405449 D | op-osd: osd.0 is healthy.
2023-02-17 01:30:50.405455 D | op-osd: validating status of osd.1
2023-02-17 01:30:50.405457 D | op-osd: osd.1 is healthy.
2023-02-17 01:30:50.405459 D | op-osd: validating status of osd.2
2023-02-17 01:30:50.405461 D | op-osd: osd.2 is healthy.
2023-02-17 01:30:50.405463 D | op-osd: validating status of osd.3
2023-02-17 01:30:50.405465 D | op-osd: osd.3 is healthy.
2023-02-17 01:30:50.405467 D | op-osd: validating status of osd.4
2023-02-17 01:30:50.405469 D | op-osd: osd.4 is healthy.
2023-02-17 01:30:50.405471 D | op-osd: validating status of osd.5
2023-02-17 01:30:50.405473 D | op-osd: osd.5 is healthy.
2023-02-17 01:30:50.405484 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:30:53.118671 D | op-mon: checking health of mons
2023-02-17 01:30:53.118703 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:30:53.118708 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:30:53.125218 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:30:53.125253 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:30:53.491585 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:30:53.491618 D | op-mon: targeting the mon count 2
2023-02-17 01:30:53.491626 D | op-mon: mon "a" found in quorum
2023-02-17 01:30:53.491629 D | op-mon: mon "b" found in quorum
2023-02-17 01:30:53.491631 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:30:53.499462 D | op-mon: Released lock for mon orchestration
2023-02-17 01:30:53.499495 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:30:56.031601 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:30:56.031668 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:30:56.374587 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:30:56.384629 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:30:56.741719 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:30:56.741750 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:30:56.741828 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:30:56.741849 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:30:56.754777 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:30:56.754802 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:31:38.499739 D | op-mon: checking health of mons
2023-02-17 01:31:38.499774 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:31:38.499779 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:31:38.518068 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:31:38.518103 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:31:38.865342 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:31:38.865369 D | op-mon: targeting the mon count 2
2023-02-17 01:31:38.865377 D | op-mon: mon "a" found in quorum
2023-02-17 01:31:38.865379 D | op-mon: mon "b" found in quorum
2023-02-17 01:31:38.865381 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:31:38.870814 D | op-mon: Released lock for mon orchestration
2023-02-17 01:31:38.870845 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:31:50.717463 D | op-osd: checking osd processes status.
2023-02-17 01:31:50.717494 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:31:51.005602 D | op-osd: validating status of osd.0
2023-02-17 01:31:51.005628 D | op-osd: osd.0 is healthy.
2023-02-17 01:31:51.005632 D | op-osd: validating status of osd.1
2023-02-17 01:31:51.005634 D | op-osd: osd.1 is healthy.
2023-02-17 01:31:51.005636 D | op-osd: validating status of osd.2
2023-02-17 01:31:51.005637 D | op-osd: osd.2 is healthy.
2023-02-17 01:31:51.005639 D | op-osd: validating status of osd.3
2023-02-17 01:31:51.005641 D | op-osd: osd.3 is healthy.
2023-02-17 01:31:51.005642 D | op-osd: validating status of osd.4
2023-02-17 01:31:51.005644 D | op-osd: osd.4 is healthy.
2023-02-17 01:31:51.005646 D | op-osd: validating status of osd.5
2023-02-17 01:31:51.005647 D | op-osd: osd.5 is healthy.
2023-02-17 01:31:51.005661 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:31:56.755406 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:31:56.755456 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:31:57.108978 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:31:57.115467 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:31:57.476308 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:31:57.476326 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:31:57.476384 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:31:57.476391 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:31:57.487324 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:31:57.487348 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:32:23.871002 D | op-mon: checking health of mons
2023-02-17 01:32:23.871034 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:32:23.871039 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:32:23.878632 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:32:23.878670 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:32:24.254960 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:32:24.254994 D | op-mon: targeting the mon count 2
2023-02-17 01:32:24.255003 D | op-mon: mon "a" found in quorum
2023-02-17 01:32:24.255006 D | op-mon: mon "b" found in quorum
2023-02-17 01:32:24.255008 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:32:24.262583 D | op-mon: Released lock for mon orchestration
2023-02-17 01:32:24.262617 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:32:51.315935 D | op-osd: checking osd processes status.
2023-02-17 01:32:51.316004 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:32:51.626187 D | op-osd: validating status of osd.0
2023-02-17 01:32:51.626216 D | op-osd: osd.0 is healthy.
2023-02-17 01:32:51.626222 D | op-osd: validating status of osd.1
2023-02-17 01:32:51.626224 D | op-osd: osd.1 is healthy.
2023-02-17 01:32:51.626226 D | op-osd: validating status of osd.2
2023-02-17 01:32:51.626278 D | op-osd: osd.2 is healthy.
2023-02-17 01:32:51.626283 D | op-osd: validating status of osd.3
2023-02-17 01:32:51.626285 D | op-osd: osd.3 is healthy.
2023-02-17 01:32:51.626287 D | op-osd: validating status of osd.4
2023-02-17 01:32:51.626289 D | op-osd: osd.4 is healthy.
2023-02-17 01:32:51.626291 D | op-osd: validating status of osd.5
2023-02-17 01:32:51.626293 D | op-osd: osd.5 is healthy.
2023-02-17 01:32:51.626306 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:32:57.488549 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:32:57.488594 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:32:57.859303 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:32:57.866002 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:32:58.252546 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:32:58.252602 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:32:58.252664 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:32:58.252687 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:32:58.264232 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:32:58.264247 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:33:09.263390 D | op-mon: checking health of mons
2023-02-17 01:33:09.263413 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:33:09.263416 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:33:09.273565 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:33:09.273598 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:33:09.649696 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:33:09.649727 D | op-mon: targeting the mon count 2
2023-02-17 01:33:09.649736 D | op-mon: mon "a" found in quorum
2023-02-17 01:33:09.649739 D | op-mon: mon "b" found in quorum
2023-02-17 01:33:09.649741 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:33:09.658328 D | op-mon: Released lock for mon orchestration
2023-02-17 01:33:09.658367 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:33:51.971851 D | op-osd: checking osd processes status.
2023-02-17 01:33:51.971905 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:33:52.283748 D | op-osd: validating status of osd.0
2023-02-17 01:33:52.283773 D | op-osd: osd.0 is healthy.
2023-02-17 01:33:52.283778 D | op-osd: validating status of osd.1
2023-02-17 01:33:52.283781 D | op-osd: osd.1 is healthy.
2023-02-17 01:33:52.283783 D | op-osd: validating status of osd.2
2023-02-17 01:33:52.283785 D | op-osd: osd.2 is healthy.
2023-02-17 01:33:52.283787 D | op-osd: validating status of osd.3
2023-02-17 01:33:52.283789 D | op-osd: osd.3 is healthy.
2023-02-17 01:33:52.283790 D | op-osd: validating status of osd.4
2023-02-17 01:33:52.283792 D | op-osd: osd.4 is healthy.
2023-02-17 01:33:52.283794 D | op-osd: validating status of osd.5
2023-02-17 01:33:52.283796 D | op-osd: osd.5 is healthy.
2023-02-17 01:33:52.283807 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:33:54.659463 D | op-mon: checking health of mons
2023-02-17 01:33:54.659498 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:33:54.659502 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:33:54.666565 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:33:54.666603 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:33:55.039147 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:33:55.039203 D | op-mon: targeting the mon count 2
2023-02-17 01:33:55.039211 D | op-mon: mon "a" found in quorum
2023-02-17 01:33:55.039214 D | op-mon: mon "b" found in quorum
2023-02-17 01:33:55.039216 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:33:55.045454 D | op-mon: Released lock for mon orchestration
2023-02-17 01:33:55.045489 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:33:58.264466 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:33:58.264512 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:33:58.680433 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:33:58.689115 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:33:59.138172 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:33:59.138201 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:33:59.138286 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:33:59.138306 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:33:59.149838 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:33:59.149865 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:34:40.046153 D | op-mon: checking health of mons
2023-02-17 01:34:40.046208 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:34:40.046214 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:34:40.057153 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:34:40.057189 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:34:40.433336 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:34:40.433364 D | op-mon: targeting the mon count 2
2023-02-17 01:34:40.433372 D | op-mon: mon "a" found in quorum
2023-02-17 01:34:40.433375 D | op-mon: mon "b" found in quorum
2023-02-17 01:34:40.433377 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:34:40.441113 D | op-mon: Released lock for mon orchestration
2023-02-17 01:34:40.441145 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:34:52.614040 D | op-osd: checking osd processes status.
2023-02-17 01:34:52.614087 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:34:52.920299 D | op-osd: validating status of osd.0
2023-02-17 01:34:52.920326 D | op-osd: osd.0 is healthy.
2023-02-17 01:34:52.920330 D | op-osd: validating status of osd.1
2023-02-17 01:34:52.920333 D | op-osd: osd.1 is healthy.
2023-02-17 01:34:52.920334 D | op-osd: validating status of osd.2
2023-02-17 01:34:52.920336 D | op-osd: osd.2 is healthy.
2023-02-17 01:34:52.920338 D | op-osd: validating status of osd.3
2023-02-17 01:34:52.920340 D | op-osd: osd.3 is healthy.
2023-02-17 01:34:52.920342 D | op-osd: validating status of osd.4
2023-02-17 01:34:52.920344 D | op-osd: osd.4 is healthy.
2023-02-17 01:34:52.920346 D | op-osd: validating status of osd.5
2023-02-17 01:34:52.920348 D | op-osd: osd.5 is healthy.
2023-02-17 01:34:52.920363 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:34:59.150012 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:34:59.150080 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:34:59.523173 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:34:59.532308 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:34:59.904779 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:34:59.904835 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:34:59.904960 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:34:59.904992 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:34:59.916915 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:34:59.916942 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:35:25.441481 D | op-mon: checking health of mons
2023-02-17 01:35:25.441614 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:35:25.441620 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:35:25.448258 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:35:25.448315 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:35:25.813137 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:35:25.813171 D | op-mon: targeting the mon count 2
2023-02-17 01:35:25.813179 D | op-mon: mon "a" found in quorum
2023-02-17 01:35:25.813182 D | op-mon: mon "b" found in quorum
2023-02-17 01:35:25.813184 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:35:25.820526 D | op-mon: Released lock for mon orchestration
2023-02-17 01:35:25.820559 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:35:53.271558 D | op-osd: checking osd processes status.
2023-02-17 01:35:53.271609 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:35:53.578696 D | op-osd: validating status of osd.0
2023-02-17 01:35:53.578724 D | op-osd: osd.0 is healthy.
2023-02-17 01:35:53.578730 D | op-osd: validating status of osd.1
2023-02-17 01:35:53.578732 D | op-osd: osd.1 is healthy.
2023-02-17 01:35:53.578734 D | op-osd: validating status of osd.2
2023-02-17 01:35:53.578736 D | op-osd: osd.2 is healthy.
2023-02-17 01:35:53.578738 D | op-osd: validating status of osd.3
2023-02-17 01:35:53.578740 D | op-osd: osd.3 is healthy.
2023-02-17 01:35:53.578742 D | op-osd: validating status of osd.4
2023-02-17 01:35:53.578744 D | op-osd: osd.4 is healthy.
2023-02-17 01:35:53.578745 D | op-osd: validating status of osd.5
2023-02-17 01:35:53.578748 D | op-osd: osd.5 is healthy.
2023-02-17 01:35:53.578758 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:35:59.916604 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:35:59.916648 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:36:00.279502 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:36:00.286360 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:36:00.654187 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:36:00.654221 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:36:00.654317 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:36:00.654335 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:36:00.666729 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:36:00.666756 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:36:10.821540 D | op-mon: checking health of mons
2023-02-17 01:36:10.821571 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:36:10.821577 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:36:10.830128 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:36:10.830162 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:36:11.206128 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:36:11.206156 D | op-mon: targeting the mon count 2
2023-02-17 01:36:11.206164 D | op-mon: mon "a" found in quorum
2023-02-17 01:36:11.206167 D | op-mon: mon "b" found in quorum
2023-02-17 01:36:11.206169 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:36:11.212343 D | op-mon: Released lock for mon orchestration
2023-02-17 01:36:11.212376 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:36:53.908152 D | op-osd: checking osd processes status.
2023-02-17 01:36:53.908197 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:36:54.218349 D | op-osd: validating status of osd.0
2023-02-17 01:36:54.218385 D | op-osd: osd.0 is healthy.
2023-02-17 01:36:54.218390 D | op-osd: validating status of osd.1
2023-02-17 01:36:54.218393 D | op-osd: osd.1 is healthy.
2023-02-17 01:36:54.218395 D | op-osd: validating status of osd.2
2023-02-17 01:36:54.218396 D | op-osd: osd.2 is healthy.
2023-02-17 01:36:54.218398 D | op-osd: validating status of osd.3
2023-02-17 01:36:54.218400 D | op-osd: osd.3 is healthy.
2023-02-17 01:36:54.218402 D | op-osd: validating status of osd.4
2023-02-17 01:36:54.218404 D | op-osd: osd.4 is healthy.
2023-02-17 01:36:54.218407 D | op-osd: validating status of osd.5
2023-02-17 01:36:54.218409 D | op-osd: osd.5 is healthy.
2023-02-17 01:36:54.218422 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:36:56.213425 D | op-mon: checking health of mons
2023-02-17 01:36:56.213458 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:36:56.213464 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:36:56.223013 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:36:56.223049 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:36:56.591092 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:36:56.591123 D | op-mon: targeting the mon count 2
2023-02-17 01:36:56.591132 D | op-mon: mon "a" found in quorum
2023-02-17 01:36:56.591134 D | op-mon: mon "b" found in quorum
2023-02-17 01:36:56.591136 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:36:56.597961 D | op-mon: Released lock for mon orchestration
2023-02-17 01:36:56.597979 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:37:00.665898 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:37:00.665949 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:37:01.033907 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:37:01.045488 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:37:01.421705 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:37:01.421740 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:37:01.421806 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:37:01.421830 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:37:01.432934 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:37:01.432951 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:37:41.598835 D | op-mon: checking health of mons
2023-02-17 01:37:41.598862 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:37:41.598868 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:37:41.605128 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:37:41.605167 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:37:41.972297 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:37:41.972311 D | op-mon: targeting the mon count 2
2023-02-17 01:37:41.972317 D | op-mon: mon "a" found in quorum
2023-02-17 01:37:41.972320 D | op-mon: mon "b" found in quorum
2023-02-17 01:37:41.972322 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:37:41.980233 D | op-mon: Released lock for mon orchestration
2023-02-17 01:37:41.980286 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:37:54.594945 D | op-osd: checking osd processes status.
2023-02-17 01:37:54.594976 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:37:54.910613 D | op-osd: validating status of osd.0
2023-02-17 01:37:54.910642 D | op-osd: osd.0 is healthy.
2023-02-17 01:37:54.910647 D | op-osd: validating status of osd.1
2023-02-17 01:37:54.910649 D | op-osd: osd.1 is healthy.
2023-02-17 01:37:54.910651 D | op-osd: validating status of osd.2
2023-02-17 01:37:54.910653 D | op-osd: osd.2 is healthy.
2023-02-17 01:37:54.910654 D | op-osd: validating status of osd.3
2023-02-17 01:37:54.910657 D | op-osd: osd.3 is healthy.
2023-02-17 01:37:54.910658 D | op-osd: validating status of osd.4
2023-02-17 01:37:54.910661 D | op-osd: osd.4 is healthy.
2023-02-17 01:37:54.910663 D | op-osd: validating status of osd.5
2023-02-17 01:37:54.910665 D | op-osd: osd.5 is healthy.
2023-02-17 01:37:54.910678 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:38:01.433600 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:38:01.433644 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:38:01.802017 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:38:01.813719 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:38:02.193646 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:38:02.193673 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:38:02.193732 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:38:02.193740 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:38:02.205418 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:38:02.205450 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:38:26.980341 D | op-mon: checking health of mons
2023-02-17 01:38:26.980386 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:38:26.980392 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:38:26.990779 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:38:26.990807 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:38:27.402954 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:38:27.402986 D | op-mon: targeting the mon count 2
2023-02-17 01:38:27.402994 D | op-mon: mon "a" found in quorum
2023-02-17 01:38:27.402997 D | op-mon: mon "b" found in quorum
2023-02-17 01:38:27.402999 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:38:27.409050 D | op-mon: Released lock for mon orchestration
2023-02-17 01:38:27.409084 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:38:55.227365 D | op-osd: checking osd processes status.
2023-02-17 01:38:55.227398 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:38:55.547639 D | op-osd: validating status of osd.0
2023-02-17 01:38:55.547668 D | op-osd: osd.0 is healthy.
2023-02-17 01:38:55.547673 D | op-osd: validating status of osd.1
2023-02-17 01:38:55.547676 D | op-osd: osd.1 is healthy.
2023-02-17 01:38:55.547677 D | op-osd: validating status of osd.2
2023-02-17 01:38:55.547679 D | op-osd: osd.2 is healthy.
2023-02-17 01:38:55.547681 D | op-osd: validating status of osd.3
2023-02-17 01:38:55.547683 D | op-osd: osd.3 is healthy.
2023-02-17 01:38:55.547685 D | op-osd: validating status of osd.4
2023-02-17 01:38:55.547687 D | op-osd: osd.4 is healthy.
2023-02-17 01:38:55.547688 D | op-osd: validating status of osd.5
2023-02-17 01:38:55.547690 D | op-osd: osd.5 is healthy.
2023-02-17 01:38:55.547704 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:39:02.205498 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:39:02.205552 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:39:02.578113 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:39:02.585354 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:39:02.958935 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:39:02.958966 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:39:02.959029 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:39:02.959036 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:39:02.972623 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:39:02.972641 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:39:12.409608 D | op-mon: checking health of mons
2023-02-17 01:39:12.409641 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:39:12.409646 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:39:12.417784 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:39:12.417821 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:39:12.781547 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:39:12.781577 D | op-mon: targeting the mon count 2
2023-02-17 01:39:12.781586 D | op-mon: mon "a" found in quorum
2023-02-17 01:39:12.781588 D | op-mon: mon "b" found in quorum
2023-02-17 01:39:12.781590 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:39:12.789429 D | op-mon: Released lock for mon orchestration
2023-02-17 01:39:12.789459 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:39:55.888351 D | op-osd: checking osd processes status.
2023-02-17 01:39:55.888401 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:39:56.195997 D | op-osd: validating status of osd.0
2023-02-17 01:39:56.196029 D | op-osd: osd.0 is healthy.
2023-02-17 01:39:56.196035 D | op-osd: validating status of osd.1
2023-02-17 01:39:56.196037 D | op-osd: osd.1 is healthy.
2023-02-17 01:39:56.196039 D | op-osd: validating status of osd.2
2023-02-17 01:39:56.196041 D | op-osd: osd.2 is healthy.
2023-02-17 01:39:56.196043 D | op-osd: validating status of osd.3
2023-02-17 01:39:56.196045 D | op-osd: osd.3 is healthy.
2023-02-17 01:39:56.196047 D | op-osd: validating status of osd.4
2023-02-17 01:39:56.196049 D | op-osd: osd.4 is healthy.
2023-02-17 01:39:56.196051 D | op-osd: validating status of osd.5
2023-02-17 01:39:56.196053 D | op-osd: osd.5 is healthy.
2023-02-17 01:39:56.196064 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:39:57.790224 D | op-mon: checking health of mons
2023-02-17 01:39:57.790257 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:39:57.790282 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:39:57.796194 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:39:57.796235 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:39:58.169957 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:39:58.169983 D | op-mon: targeting the mon count 2
2023-02-17 01:39:58.169992 D | op-mon: mon "a" found in quorum
2023-02-17 01:39:58.169994 D | op-mon: mon "b" found in quorum
2023-02-17 01:39:58.169996 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:39:58.177474 D | op-mon: Released lock for mon orchestration
2023-02-17 01:39:58.177509 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:40:02.973842 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:40:02.973897 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:40:03.359299 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:40:03.370137 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:40:03.756828 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:40:03.756846 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:40:03.756905 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:40:03.756912 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:40:03.768641 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:40:03.768660 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:40:43.178197 D | op-mon: checking health of mons
2023-02-17 01:40:43.178237 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:40:43.178243 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:40:43.186479 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:40:43.186517 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:40:43.565818 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:40:43.565848 D | op-mon: targeting the mon count 2
2023-02-17 01:40:43.565856 D | op-mon: mon "a" found in quorum
2023-02-17 01:40:43.565859 D | op-mon: mon "b" found in quorum
2023-02-17 01:40:43.565861 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:40:43.571910 D | op-mon: Released lock for mon orchestration
2023-02-17 01:40:43.571941 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:40:56.512405 D | op-osd: checking osd processes status.
2023-02-17 01:40:56.512464 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:40:56.837769 D | op-osd: validating status of osd.0
2023-02-17 01:40:56.837800 D | op-osd: osd.0 is healthy.
2023-02-17 01:40:56.837805 D | op-osd: validating status of osd.1
2023-02-17 01:40:56.837807 D | op-osd: osd.1 is healthy.
2023-02-17 01:40:56.837809 D | op-osd: validating status of osd.2
2023-02-17 01:40:56.837811 D | op-osd: osd.2 is healthy.
2023-02-17 01:40:56.837813 D | op-osd: validating status of osd.3
2023-02-17 01:40:56.837815 D | op-osd: osd.3 is healthy.
2023-02-17 01:40:56.837817 D | op-osd: validating status of osd.4
2023-02-17 01:40:56.837819 D | op-osd: osd.4 is healthy.
2023-02-17 01:40:56.837821 D | op-osd: validating status of osd.5
2023-02-17 01:40:56.837823 D | op-osd: osd.5 is healthy.
2023-02-17 01:40:56.837834 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:41:03.769394 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:41:03.769439 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:41:04.158534 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:41:04.169117 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:41:04.574576 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:41:04.574610 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:41:04.574680 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:41:04.574723 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:41:04.587405 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:41:04.587441 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:41:28.572847 D | op-mon: checking health of mons
2023-02-17 01:41:28.572879 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:41:28.572884 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:41:28.582681 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:41:28.582718 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:41:28.947269 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:41:28.947342 D | op-mon: targeting the mon count 2
2023-02-17 01:41:28.947351 D | op-mon: mon "a" found in quorum
2023-02-17 01:41:28.947354 D | op-mon: mon "b" found in quorum
2023-02-17 01:41:28.947356 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:41:28.954527 D | op-mon: Released lock for mon orchestration
2023-02-17 01:41:28.954561 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:41:57.192021 D | op-osd: checking osd processes status.
2023-02-17 01:41:57.192072 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:41:57.500681 D | op-osd: validating status of osd.0
2023-02-17 01:41:57.500711 D | op-osd: osd.0 is healthy.
2023-02-17 01:41:57.500716 D | op-osd: validating status of osd.1
2023-02-17 01:41:57.500718 D | op-osd: osd.1 is healthy.
2023-02-17 01:41:57.500720 D | op-osd: validating status of osd.2
2023-02-17 01:41:57.500722 D | op-osd: osd.2 is healthy.
2023-02-17 01:41:57.500724 D | op-osd: validating status of osd.3
2023-02-17 01:41:57.500726 D | op-osd: osd.3 is healthy.
2023-02-17 01:41:57.500728 D | op-osd: validating status of osd.4
2023-02-17 01:41:57.500730 D | op-osd: osd.4 is healthy.
2023-02-17 01:41:57.500732 D | op-osd: validating status of osd.5
2023-02-17 01:41:57.500734 D | op-osd: osd.5 is healthy.
2023-02-17 01:41:57.500747 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:42:04.586921 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:42:04.586988 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:42:04.964294 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:42:04.971023 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:42:05.339459 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:42:05.339478 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:42:05.339549 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:42:05.339556 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:42:05.351540 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:42:05.351555 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:42:13.955525 D | op-mon: checking health of mons
2023-02-17 01:42:13.955559 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:42:13.955564 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:42:13.964389 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:42:13.964424 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:42:14.331654 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:42:14.331696 D | op-mon: targeting the mon count 2
2023-02-17 01:42:14.331705 D | op-mon: mon "a" found in quorum
2023-02-17 01:42:14.331707 D | op-mon: mon "b" found in quorum
2023-02-17 01:42:14.331710 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:42:14.339083 D | op-mon: Released lock for mon orchestration
2023-02-17 01:42:14.339116 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:42:57.823120 D | op-osd: checking osd processes status.
2023-02-17 01:42:57.823166 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:42:58.134243 D | op-osd: validating status of osd.0
2023-02-17 01:42:58.134258 D | op-osd: osd.0 is healthy.
2023-02-17 01:42:58.134282 D | op-osd: validating status of osd.1
2023-02-17 01:42:58.134287 D | op-osd: osd.1 is healthy.
2023-02-17 01:42:58.134290 D | op-osd: validating status of osd.2
2023-02-17 01:42:58.134292 D | op-osd: osd.2 is healthy.
2023-02-17 01:42:58.134294 D | op-osd: validating status of osd.3
2023-02-17 01:42:58.134296 D | op-osd: osd.3 is healthy.
2023-02-17 01:42:58.134298 D | op-osd: validating status of osd.4
2023-02-17 01:42:58.134300 D | op-osd: osd.4 is healthy.
2023-02-17 01:42:58.134302 D | op-osd: validating status of osd.5
2023-02-17 01:42:58.134304 D | op-osd: osd.5 is healthy.
2023-02-17 01:42:58.134315 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:42:59.339370 D | op-mon: checking health of mons
2023-02-17 01:42:59.339391 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:42:59.339394 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:42:59.348586 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:42:59.348637 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:42:59.724579 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:42:59.724610 D | op-mon: targeting the mon count 2
2023-02-17 01:42:59.724618 D | op-mon: mon "a" found in quorum
2023-02-17 01:42:59.724621 D | op-mon: mon "b" found in quorum
2023-02-17 01:42:59.724623 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:42:59.730788 D | op-mon: Released lock for mon orchestration
2023-02-17 01:42:59.730823 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:43:05.350571 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:43:05.350617 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:43:05.744590 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:43:05.759206 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:43:06.149062 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:43:06.149094 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:43:06.149159 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:43:06.149166 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:43:06.161755 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:43:06.161771 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:43:44.732146 D | op-mon: checking health of mons
2023-02-17 01:43:44.732175 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:43:44.732181 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:43:44.750129 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:43:44.750190 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:43:45.176730 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:43:45.176762 D | op-mon: targeting the mon count 2
2023-02-17 01:43:45.176770 D | op-mon: mon "a" found in quorum
2023-02-17 01:43:45.176773 D | op-mon: mon "b" found in quorum
2023-02-17 01:43:45.176775 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:43:45.184471 D | op-mon: Released lock for mon orchestration
2023-02-17 01:43:45.184504 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:43:58.472809 D | op-osd: checking osd processes status.
2023-02-17 01:43:58.472859 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:43:58.793756 D | op-osd: validating status of osd.0
2023-02-17 01:43:58.793785 D | op-osd: osd.0 is healthy.
2023-02-17 01:43:58.793790 D | op-osd: validating status of osd.1
2023-02-17 01:43:58.793792 D | op-osd: osd.1 is healthy.
2023-02-17 01:43:58.793794 D | op-osd: validating status of osd.2
2023-02-17 01:43:58.793796 D | op-osd: osd.2 is healthy.
2023-02-17 01:43:58.793798 D | op-osd: validating status of osd.3
2023-02-17 01:43:58.793800 D | op-osd: osd.3 is healthy.
2023-02-17 01:43:58.793801 D | op-osd: validating status of osd.4
2023-02-17 01:43:58.793803 D | op-osd: osd.4 is healthy.
2023-02-17 01:43:58.793805 D | op-osd: validating status of osd.5
2023-02-17 01:43:58.793807 D | op-osd: osd.5 is healthy.
2023-02-17 01:43:58.793818 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:44:06.159964 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:44:06.160017 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:44:06.554172 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:44:06.560306 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:44:06.951779 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:44:06.951807 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:44:06.951870 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:44:06.951877 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:44:06.964411 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:44:06.964442 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:44:30.185407 D | op-mon: checking health of mons
2023-02-17 01:44:30.185440 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:44:30.185445 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:44:30.191497 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:44:30.191534 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:44:30.564103 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:44:30.564119 D | op-mon: targeting the mon count 2
2023-02-17 01:44:30.564125 D | op-mon: mon "a" found in quorum
2023-02-17 01:44:30.564127 D | op-mon: mon "b" found in quorum
2023-02-17 01:44:30.564129 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:44:30.571446 D | op-mon: Released lock for mon orchestration
2023-02-17 01:44:30.571478 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:44:59.159032 D | op-osd: checking osd processes status.
2023-02-17 01:44:59.159110 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:44:59.459120 D | op-osd: validating status of osd.0
2023-02-17 01:44:59.459152 D | op-osd: osd.0 is healthy.
2023-02-17 01:44:59.459157 D | op-osd: validating status of osd.1
2023-02-17 01:44:59.459159 D | op-osd: osd.1 is healthy.
2023-02-17 01:44:59.459161 D | op-osd: validating status of osd.2
2023-02-17 01:44:59.459163 D | op-osd: osd.2 is healthy.
2023-02-17 01:44:59.459165 D | op-osd: validating status of osd.3
2023-02-17 01:44:59.459167 D | op-osd: osd.3 is healthy.
2023-02-17 01:44:59.459169 D | op-osd: validating status of osd.4
2023-02-17 01:44:59.459171 D | op-osd: osd.4 is healthy.
2023-02-17 01:44:59.459173 D | op-osd: validating status of osd.5
2023-02-17 01:44:59.459175 D | op-osd: osd.5 is healthy.
2023-02-17 01:44:59.459186 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:45:06.964472 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:45:06.964523 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:45:07.357362 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:45:07.363910 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:45:07.739656 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:45:07.739711 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:45:07.739778 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:45:07.739800 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:45:07.752009 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:45:07.752036 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:45:15.571732 D | op-mon: checking health of mons
2023-02-17 01:45:15.571753 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:45:15.571756 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:45:15.584268 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:45:15.584339 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:45:15.969408 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:45:15.969436 D | op-mon: targeting the mon count 2
2023-02-17 01:45:15.969636 D | op-mon: mon "a" found in quorum
2023-02-17 01:45:15.969641 D | op-mon: mon "b" found in quorum
2023-02-17 01:45:15.969643 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:45:15.974948 D | op-mon: Released lock for mon orchestration
2023-02-17 01:45:15.974982 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:45:59.779679 D | op-osd: checking osd processes status.
2023-02-17 01:45:59.779737 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:46:00.087470 D | op-osd: validating status of osd.0
2023-02-17 01:46:00.087500 D | op-osd: osd.0 is healthy.
2023-02-17 01:46:00.087506 D | op-osd: validating status of osd.1
2023-02-17 01:46:00.087508 D | op-osd: osd.1 is healthy.
2023-02-17 01:46:00.087510 D | op-osd: validating status of osd.2
2023-02-17 01:46:00.087512 D | op-osd: osd.2 is healthy.
2023-02-17 01:46:00.087514 D | op-osd: validating status of osd.3
2023-02-17 01:46:00.087517 D | op-osd: osd.3 is healthy.
2023-02-17 01:46:00.087519 D | op-osd: validating status of osd.4
2023-02-17 01:46:00.087521 D | op-osd: osd.4 is healthy.
2023-02-17 01:46:00.087523 D | op-osd: validating status of osd.5
2023-02-17 01:46:00.087524 D | op-osd: osd.5 is healthy.
2023-02-17 01:46:00.087537 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:46:00.976926 D | op-mon: checking health of mons
2023-02-17 01:46:00.976988 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:46:00.976993 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:46:00.985631 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:46:00.985668 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:46:01.357454 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:46:01.357522 D | op-mon: targeting the mon count 2
2023-02-17 01:46:01.357530 D | op-mon: mon "a" found in quorum
2023-02-17 01:46:01.357533 D | op-mon: mon "b" found in quorum
2023-02-17 01:46:01.357535 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:46:01.366168 D | op-mon: Released lock for mon orchestration
2023-02-17 01:46:01.366201 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:46:07.752040 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:46:07.752083 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:46:08.125213 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:46:08.132898 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:46:08.507505 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:46:08.507537 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:46:08.507598 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:46:08.507622 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:46:08.518767 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:46:08.518796 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:46:46.367008 D | op-mon: checking health of mons
2023-02-17 01:46:46.367069 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:46:46.367075 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:46:46.381053 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:46:46.381090 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:46:46.833938 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:46:46.833966 D | op-mon: targeting the mon count 2
2023-02-17 01:46:46.833974 D | op-mon: mon "a" found in quorum
2023-02-17 01:46:46.833976 D | op-mon: mon "b" found in quorum
2023-02-17 01:46:46.833978 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:46:46.841600 D | op-mon: Released lock for mon orchestration
2023-02-17 01:46:46.841674 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:47:00.419564 D | op-osd: checking osd processes status.
2023-02-17 01:47:00.419610 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:47:00.725406 D | op-osd: validating status of osd.0
2023-02-17 01:47:00.725434 D | op-osd: osd.0 is healthy.
2023-02-17 01:47:00.725440 D | op-osd: validating status of osd.1
2023-02-17 01:47:00.725442 D | op-osd: osd.1 is healthy.
2023-02-17 01:47:00.725444 D | op-osd: validating status of osd.2
2023-02-17 01:47:00.725446 D | op-osd: osd.2 is healthy.
2023-02-17 01:47:00.725448 D | op-osd: validating status of osd.3
2023-02-17 01:47:00.725450 D | op-osd: osd.3 is healthy.
2023-02-17 01:47:00.725451 D | op-osd: validating status of osd.4
2023-02-17 01:47:00.725453 D | op-osd: osd.4 is healthy.
2023-02-17 01:47:00.725455 D | op-osd: validating status of osd.5
2023-02-17 01:47:00.725457 D | op-osd: osd.5 is healthy.
2023-02-17 01:47:00.725467 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:47:08.518355 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:47:08.518397 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:47:08.891012 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:47:08.899701 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:47:09.316964 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:47:09.316994 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:47:09.317057 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:47:09.317064 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:47:09.328226 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:47:09.328254 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:47:31.842015 D | op-mon: checking health of mons
2023-02-17 01:47:31.842058 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:47:31.842063 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:47:31.855867 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:47:31.855909 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:47:32.230680 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:47:32.230710 D | op-mon: targeting the mon count 2
2023-02-17 01:47:32.230719 D | op-mon: mon "a" found in quorum
2023-02-17 01:47:32.230722 D | op-mon: mon "b" found in quorum
2023-02-17 01:47:32.230724 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:47:32.236683 D | op-mon: Released lock for mon orchestration
2023-02-17 01:47:32.236715 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:48:01.057517 D | op-osd: checking osd processes status.
2023-02-17 01:48:01.057560 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:48:01.364064 D | op-osd: validating status of osd.0
2023-02-17 01:48:01.364095 D | op-osd: osd.0 is healthy.
2023-02-17 01:48:01.364100 D | op-osd: validating status of osd.1
2023-02-17 01:48:01.364103 D | op-osd: osd.1 is healthy.
2023-02-17 01:48:01.364104 D | op-osd: validating status of osd.2
2023-02-17 01:48:01.364106 D | op-osd: osd.2 is healthy.
2023-02-17 01:48:01.364108 D | op-osd: validating status of osd.3
2023-02-17 01:48:01.364110 D | op-osd: osd.3 is healthy.
2023-02-17 01:48:01.364112 D | op-osd: validating status of osd.4
2023-02-17 01:48:01.364114 D | op-osd: osd.4 is healthy.
2023-02-17 01:48:01.364116 D | op-osd: validating status of osd.5
2023-02-17 01:48:01.364118 D | op-osd: osd.5 is healthy.
2023-02-17 01:48:01.364128 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:48:09.328486 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:48:09.328545 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:48:09.697322 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:48:09.705438 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:48:10.073854 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:48:10.073886 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:48:10.073965 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:48:10.074004 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:48:10.084701 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:48:10.084729 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:48:17.237741 D | op-mon: checking health of mons
2023-02-17 01:48:17.237775 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:48:17.237781 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:48:17.245327 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:48:17.245362 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:48:17.616694 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:48:17.616723 D | op-mon: targeting the mon count 2
2023-02-17 01:48:17.616731 D | op-mon: mon "a" found in quorum
2023-02-17 01:48:17.616733 D | op-mon: mon "b" found in quorum
2023-02-17 01:48:17.616735 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:48:17.624781 D | op-mon: Released lock for mon orchestration
2023-02-17 01:48:17.624818 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:49:01.683607 D | op-osd: checking osd processes status.
2023-02-17 01:49:01.683656 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:49:01.994718 D | op-osd: validating status of osd.0
2023-02-17 01:49:01.994746 D | op-osd: osd.0 is healthy.
2023-02-17 01:49:01.994751 D | op-osd: validating status of osd.1
2023-02-17 01:49:01.994753 D | op-osd: osd.1 is healthy.
2023-02-17 01:49:01.994755 D | op-osd: validating status of osd.2
2023-02-17 01:49:01.994757 D | op-osd: osd.2 is healthy.
2023-02-17 01:49:01.994759 D | op-osd: validating status of osd.3
2023-02-17 01:49:01.994761 D | op-osd: osd.3 is healthy.
2023-02-17 01:49:01.994763 D | op-osd: validating status of osd.4
2023-02-17 01:49:01.994765 D | op-osd: osd.4 is healthy.
2023-02-17 01:49:01.994766 D | op-osd: validating status of osd.5
2023-02-17 01:49:01.994768 D | op-osd: osd.5 is healthy.
2023-02-17 01:49:01.994780 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:49:02.625358 D | op-mon: checking health of mons
2023-02-17 01:49:02.625391 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:49:02.625397 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:49:02.630800 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:49:02.630834 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:49:03.008472 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:49:03.008512 D | op-mon: targeting the mon count 2
2023-02-17 01:49:03.008523 D | op-mon: mon "a" found in quorum
2023-02-17 01:49:03.008528 D | op-mon: mon "b" found in quorum
2023-02-17 01:49:03.008531 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:49:03.018959 D | op-mon: Released lock for mon orchestration
2023-02-17 01:49:03.018998 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:49:10.085143 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:49:10.085188 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:49:10.467075 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:49:10.476791 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:49:10.858250 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:49:10.858308 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:49:10.858376 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:49:10.858384 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:49:10.870237 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:49:10.870286 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:49:48.019131 D | op-mon: checking health of mons
2023-02-17 01:49:48.019165 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:49:48.019170 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:49:48.028076 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:49:48.028112 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:49:48.419983 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:49:48.420013 D | op-mon: targeting the mon count 2
2023-02-17 01:49:48.420021 D | op-mon: mon "a" found in quorum
2023-02-17 01:49:48.420025 D | op-mon: mon "b" found in quorum
2023-02-17 01:49:48.420026 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:49:48.426373 D | op-mon: Released lock for mon orchestration
2023-02-17 01:49:48.426475 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:50:02.324471 D | op-osd: checking osd processes status.
2023-02-17 01:50:02.324518 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:50:02.632166 D | op-osd: validating status of osd.0
2023-02-17 01:50:02.632200 D | op-osd: osd.0 is healthy.
2023-02-17 01:50:02.632205 D | op-osd: validating status of osd.1
2023-02-17 01:50:02.632207 D | op-osd: osd.1 is healthy.
2023-02-17 01:50:02.632209 D | op-osd: validating status of osd.2
2023-02-17 01:50:02.632211 D | op-osd: osd.2 is healthy.
2023-02-17 01:50:02.632213 D | op-osd: validating status of osd.3
2023-02-17 01:50:02.632215 D | op-osd: osd.3 is healthy.
2023-02-17 01:50:02.632217 D | op-osd: validating status of osd.4
2023-02-17 01:50:02.632219 D | op-osd: osd.4 is healthy.
2023-02-17 01:50:02.632221 D | op-osd: validating status of osd.5
2023-02-17 01:50:02.632222 D | op-osd: osd.5 is healthy.
2023-02-17 01:50:02.632237 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:50:10.869443 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:50:10.869700 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:50:11.237751 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:50:11.249730 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:50:11.609080 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:50:11.609113 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:50:11.609180 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:50:11.609191 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:50:11.620569 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:50:11.620598 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:50:33.427483 D | op-mon: checking health of mons
2023-02-17 01:50:33.427514 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:50:33.427519 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:50:33.438682 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:50:33.438717 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:50:33.809631 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:50:33.809659 D | op-mon: targeting the mon count 2
2023-02-17 01:50:33.809667 D | op-mon: mon "a" found in quorum
2023-02-17 01:50:33.809670 D | op-mon: mon "b" found in quorum
2023-02-17 01:50:33.809672 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:50:33.817196 D | op-mon: Released lock for mon orchestration
2023-02-17 01:50:33.817229 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:51:02.968615 D | op-osd: checking osd processes status.
2023-02-17 01:51:02.968680 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:51:03.278662 D | op-osd: validating status of osd.0
2023-02-17 01:51:03.278709 D | op-osd: osd.0 is healthy.
2023-02-17 01:51:03.278716 D | op-osd: validating status of osd.1
2023-02-17 01:51:03.278718 D | op-osd: osd.1 is healthy.
2023-02-17 01:51:03.278720 D | op-osd: validating status of osd.2
2023-02-17 01:51:03.278722 D | op-osd: osd.2 is healthy.
2023-02-17 01:51:03.278724 D | op-osd: validating status of osd.3
2023-02-17 01:51:03.278726 D | op-osd: osd.3 is healthy.
2023-02-17 01:51:03.278728 D | op-osd: validating status of osd.4
2023-02-17 01:51:03.278730 D | op-osd: osd.4 is healthy.
2023-02-17 01:51:03.278732 D | op-osd: validating status of osd.5
2023-02-17 01:51:03.278734 D | op-osd: osd.5 is healthy.
2023-02-17 01:51:03.278745 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:51:11.620339 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:51:11.620381 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:51:11.992147 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:51:12.001344 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:51:12.374170 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:51:12.374215 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:51:12.374304 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:51:12.374316 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:51:12.385008 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:51:12.385023 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:51:18.817351 D | op-mon: checking health of mons
2023-02-17 01:51:18.817383 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:51:18.817388 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:51:18.824745 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:51:18.824781 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:51:19.203928 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:51:19.203945 D | op-mon: targeting the mon count 2
2023-02-17 01:51:19.203951 D | op-mon: mon "a" found in quorum
2023-02-17 01:51:19.203954 D | op-mon: mon "b" found in quorum
2023-02-17 01:51:19.203956 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:51:19.212890 D | op-mon: Released lock for mon orchestration
2023-02-17 01:51:19.212923 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:52:03.605928 D | op-osd: checking osd processes status.
2023-02-17 01:52:03.605992 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:52:03.915408 D | op-osd: validating status of osd.0
2023-02-17 01:52:03.915437 D | op-osd: osd.0 is healthy.
2023-02-17 01:52:03.915443 D | op-osd: validating status of osd.1
2023-02-17 01:52:03.915445 D | op-osd: osd.1 is healthy.
2023-02-17 01:52:03.915447 D | op-osd: validating status of osd.2
2023-02-17 01:52:03.915449 D | op-osd: osd.2 is healthy.
2023-02-17 01:52:03.915451 D | op-osd: validating status of osd.3
2023-02-17 01:52:03.915453 D | op-osd: osd.3 is healthy.
2023-02-17 01:52:03.915455 D | op-osd: validating status of osd.4
2023-02-17 01:52:03.915457 D | op-osd: osd.4 is healthy.
2023-02-17 01:52:03.915459 D | op-osd: validating status of osd.5
2023-02-17 01:52:03.915461 D | op-osd: osd.5 is healthy.
2023-02-17 01:52:03.915474 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:52:04.213152 D | op-mon: checking health of mons
2023-02-17 01:52:04.213187 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:52:04.213194 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:52:04.222734 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:52:04.222774 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:52:04.607684 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:52:04.607716 D | op-mon: targeting the mon count 2
2023-02-17 01:52:04.607724 D | op-mon: mon "a" found in quorum
2023-02-17 01:52:04.607727 D | op-mon: mon "b" found in quorum
2023-02-17 01:52:04.607728 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:52:04.618035 D | op-mon: Released lock for mon orchestration
2023-02-17 01:52:04.618062 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:52:12.384554 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:52:12.384610 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:52:12.760206 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:52:12.772129 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:52:13.159524 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:52:13.159558 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:52:13.159761 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:52:13.159774 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:52:13.170232 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:52:13.170249 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:52:49.618492 D | op-mon: checking health of mons
2023-02-17 01:52:49.618550 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:52:49.618556 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:52:49.627583 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:52:49.627621 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:52:49.992635 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:52:49.992662 D | op-mon: targeting the mon count 2
2023-02-17 01:52:49.992687 D | op-mon: mon "a" found in quorum
2023-02-17 01:52:49.992689 D | op-mon: mon "b" found in quorum
2023-02-17 01:52:49.992714 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:52:50.000730 D | op-mon: Released lock for mon orchestration
2023-02-17 01:52:50.000764 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:53:04.236479 D | op-osd: checking osd processes status.
2023-02-17 01:53:04.236513 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:53:04.537938 D | op-osd: validating status of osd.0
2023-02-17 01:53:04.537970 D | op-osd: osd.0 is healthy.
2023-02-17 01:53:04.537975 D | op-osd: validating status of osd.1
2023-02-17 01:53:04.537977 D | op-osd: osd.1 is healthy.
2023-02-17 01:53:04.537979 D | op-osd: validating status of osd.2
2023-02-17 01:53:04.537981 D | op-osd: osd.2 is healthy.
2023-02-17 01:53:04.537983 D | op-osd: validating status of osd.3
2023-02-17 01:53:04.537985 D | op-osd: osd.3 is healthy.
2023-02-17 01:53:04.537987 D | op-osd: validating status of osd.4
2023-02-17 01:53:04.537989 D | op-osd: osd.4 is healthy.
2023-02-17 01:53:04.537991 D | op-osd: validating status of osd.5
2023-02-17 01:53:04.537993 D | op-osd: osd.5 is healthy.
2023-02-17 01:53:04.538006 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:53:13.170153 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:53:13.170214 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:53:13.559484 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:53:13.567649 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:53:13.955960 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:53:13.955995 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:53:13.956061 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:53:13.956068 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:53:13.968369 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:53:13.968386 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:53:35.000989 D | op-mon: checking health of mons
2023-02-17 01:53:35.001026 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:53:35.001031 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:53:35.007515 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:53:35.007550 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:53:35.378395 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:53:35.378425 D | op-mon: targeting the mon count 2
2023-02-17 01:53:35.378434 D | op-mon: mon "a" found in quorum
2023-02-17 01:53:35.378437 D | op-mon: mon "b" found in quorum
2023-02-17 01:53:35.378438 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:53:35.385928 D | op-mon: Released lock for mon orchestration
2023-02-17 01:53:35.385963 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
2023-02-17 01:54:04.866926 D | op-osd: checking osd processes status.
2023-02-17 01:54:04.866959 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:54:05.183739 D | op-osd: validating status of osd.0
2023-02-17 01:54:05.183770 D | op-osd: osd.0 is healthy.
2023-02-17 01:54:05.183775 D | op-osd: validating status of osd.1
2023-02-17 01:54:05.183777 D | op-osd: osd.1 is healthy.
2023-02-17 01:54:05.183779 D | op-osd: validating status of osd.2
2023-02-17 01:54:05.183781 D | op-osd: osd.2 is healthy.
2023-02-17 01:54:05.183783 D | op-osd: validating status of osd.3
2023-02-17 01:54:05.183786 D | op-osd: osd.3 is healthy.
2023-02-17 01:54:05.183787 D | op-osd: validating status of osd.4
2023-02-17 01:54:05.183789 D | op-osd: osd.4 is healthy.
2023-02-17 01:54:05.183791 D | op-osd: validating status of osd.5
2023-02-17 01:54:05.183793 D | op-osd: osd.5 is healthy.
2023-02-17 01:54:05.183804 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:54:13.967961 D | ceph-cluster-controller: checking health of cluster
2023-02-17 01:54:13.968002 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2023-02-17 01:54:14.332024 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2023-02-17 01:54:14.338245 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:54:14.723846 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:54:14.723882 D | cephclient: {"mon":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"mgr":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":2},"osd":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":6},"mds":{},"overall":{"ceph version 16.2.10 (45fa1a083152e41a408d15505f594ec5f1b4fe17) pacific (stable)":10}}
2023-02-17 01:54:14.723964 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:7f72a05c-d4eb-4365-9ddc-908f83a1eb8b ElectionEpoch:8 Quorum:[0 1] QuorumNames:[a b] MonMap:{Epoch:2 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{OsdMap:{Epoch:0 NumOsd:0 NumUpOsd:0 NumInOsd:0 Full:false NearFull:false NumRemappedPgs:0}} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:30547968 AvailableBytes:1649236893696 TotalBytes:1649267441664 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2023-02-17 01:54:14.723990 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2023-02-17 01:54:14.737051 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2023-02-17 01:54:14.737078 D | ceph-cluster-controller: update event on CephCluster CR
2023-02-17 01:54:20.386366 D | op-mon: checking health of mons
2023-02-17 01:54:20.386400 D | op-mon: Acquiring lock for mon orchestration
2023-02-17 01:54:20.386405 D | op-mon: Acquired lock for mon orchestration
2023-02-17 01:54:20.394890 D | op-mon: Checking health for mons in cluster "rook-ceph"
2023-02-17 01:54:20.394927 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2023-02-17 01:54:20.762301 D | op-mon: Mon quorum status: {Quorum:[0 1] MonMap:{Mons:[{Name:a Rank:0 Address:10.211.55.97:6789/0 PublicAddr:10.211.55.97:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.97:3300 Nonce:0} {Type:v1 Addr:10.211.55.97:6789 Nonce:0}]}} {Name:b Rank:1 Address:10.211.55.99:6789/0 PublicAddr:10.211.55.99:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.211.55.99:3300 Nonce:0} {Type:v1 Addr:10.211.55.99:6789 Nonce:0}]}}]}}
2023-02-17 01:54:20.762329 D | op-mon: targeting the mon count 2
2023-02-17 01:54:20.762337 D | op-mon: mon "a" found in quorum
2023-02-17 01:54:20.762340 D | op-mon: mon "b" found in quorum
2023-02-17 01:54:20.762342 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2023-02-17 01:54:20.768378 D | op-mon: Released lock for mon orchestration
2023-02-17 01:54:20.768412 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "45s"
